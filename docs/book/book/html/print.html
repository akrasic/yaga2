<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Yaga2 Anomaly Detection Guide</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="Comprehensive guide to anomaly detection, SLO evaluation, and incident management">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon-de23e50b.svg">
        <link rel="shortcut icon" href="favicon-8114d1fc.png">
        <link rel="stylesheet" href="css/variables-8adf115d.css">
        <link rel="stylesheet" href="css/general-2459343d.css">
        <link rel="stylesheet" href="css/chrome-ae938929.css">
        <link rel="stylesheet" href="css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "ayu";
            window.path_to_searchindex_js = "searchindex-5019ba8d.js";
        </script>

        <!-- Custom JS scripts for mdbook-pdf PDF generation -->
        <script type='text/javascript'>
            let markAllContentHasLoadedForPrinting = () =>
                window.setTimeout(
                    () => {
                        let p = document.createElement('div');
                        p.setAttribute('id', 'content-has-all-loaded-for-mdbook-pdf-generation');
                        document.body.appendChild(p);
                    }, 100
                );

            window.addEventListener('load', () => {
                // Expand all the <details> elements for printing.
                r = document.getElementsByTagName('details');
                for (let i of r)
                    i.open = true;

                try {
                    MathJax.Hub.Register.StartupHook('End', markAllContentHasLoadedForPrinting);
                } catch (e) {
                    markAllContentHasLoadedForPrinting();
                }
            });
        </script>
    <div style="display: none"><a href="#introduction">introduction</a><a href="#overview">overview</a><a href="#detection-index">detection-index</a><a href="#detection-isolation-forest">detection-isolation-forest</a><a href="#detection-pattern-matching">detection-pattern-matching</a><a href="#detection-pipeline">detection-pipeline</a><a href="#slo-index">slo-index</a><a href="#slo-latency">slo-latency</a><a href="#slo-error-rate">slo-error-rate</a><a href="#slo-database-latency">slo-database-latency</a><a href="#slo-request-rate">slo-request-rate</a><a href="#slo-severity-adjustment">slo-severity-adjustment</a><a href="#incidents-index">incidents-index</a><a href="#incidents-state-machine">incidents-state-machine</a><a href="#incidents-confirmation">incidents-confirmation</a><a href="#incidents-fingerprinting">incidents-fingerprinting</a><a href="#reference-decision-matrix">reference-decision-matrix</a><a href="#reference-configuration">reference-configuration</a><a href="#reference-api-payload">reference-api-payload</a><a href="#reference-troubleshooting">reference-troubleshooting</a></div>
        <!-- Start loading toc.js asap -->
        <script src="toc-e4c30426.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">Yaga2 Anomaly Detection Guide</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>
                        <a href="https://github.com/smartbox/yaga2" title="Git repository" aria-label="Git repository">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>Yaga2 is an ML-based anomaly detection system for monitoring service health in production environments. This guide explains how the system detects anomalies, evaluates their operational significance, and manages incident lifecycles.</p>
<h2 id="what-problem-does-yaga2-solve"><a class="header" href="#what-problem-does-yaga2-solve">What Problem Does Yaga2 Solve?</a></h2>
<p>Modern microservice architectures generate massive amounts of metrics data. Manually setting static thresholds for every service and metric combination is:</p>
<ul>
<li><strong>Impractical</strong>: Hundreds of services × multiple metrics × different time periods = thousands of thresholds</li>
<li><strong>Brittle</strong>: Traffic patterns change seasonally, after deployments, and as services evolve</li>
<li><strong>Noisy</strong>: Static thresholds either miss real issues or generate alert fatigue</li>
</ul>
<p>Yaga2 addresses these challenges by:</p>
<ol>
<li><strong>Learning normal behavior</strong> from historical data using machine learning</li>
<li><strong>Detecting deviations</strong> that represent genuine anomalies, not just threshold breaches</li>
<li><strong>Evaluating operational impact</strong> to determine if anomalies actually matter</li>
<li><strong>Managing alert lifecycle</strong> to reduce noise and prevent duplicate alerts</li>
</ol>
<h2 id="core-concepts"><a class="header" href="#core-concepts">Core Concepts</a></h2>
<h3 id="what-is-anomaly-detection"><a class="header" href="#what-is-anomaly-detection">What is Anomaly Detection?</a></h3>
<p>Anomaly detection identifies data points that deviate significantly from expected patterns. Unlike threshold-based alerting (e.g., “alert if latency &gt; 500ms”), anomaly detection learns what “normal” looks like for each service and flags when behavior deviates from that baseline.</p>
<p><strong>Example:</strong></p>
<ul>
<li>Service A normally has 200ms latency → 350ms is anomalous</li>
<li>Service B normally has 800ms latency → 350ms is actually <em>better</em> than normal</li>
<li>A static 500ms threshold would miss the anomaly for Service A and false-alarm for Service B</li>
</ul>
<h3 id="what-is-an-slo"><a class="header" href="#what-is-an-slo">What is an SLO?</a></h3>
<p><strong>SLO (Service Level Objective)</strong> is a target level of service reliability. It defines what “good enough” means for your service.</p>
<p>Common SLOs include:</p>
<ul>
<li><strong>Latency SLO</strong>: “99% of requests complete in under 500ms”</li>
<li><strong>Availability SLO</strong>: “Service is available 99.9% of the time”</li>
<li><strong>Error Rate SLO</strong>: “Less than 0.5% of requests result in errors”</li>
</ul>
<p><strong>Why SLOs Matter for Anomaly Detection:</strong></p>
<p>An anomaly might be statistically significant but operationally irrelevant:</p>
<ul>
<li>ML detects latency at 280ms (unusual compared to baseline of 150ms)</li>
<li>But SLO says 500ms is acceptable</li>
<li>Result: The anomaly is real but doesn’t warrant immediate action</li>
</ul>
<p>Yaga2 combines ML detection with SLO evaluation to answer two questions:</p>
<ol>
<li><strong>Is this unusual?</strong> (ML detection)</li>
<li><strong>Does it matter?</strong> (SLO evaluation)</li>
</ol>
<h3 id="what-is-isolation-forest"><a class="header" href="#what-is-isolation-forest">What is Isolation Forest?</a></h3>
<p><strong>Isolation Forest</strong> is the machine learning algorithm at the heart of Yaga2’s anomaly detection. It’s an “unsupervised” algorithm, meaning it doesn’t need labeled examples of anomalies—it learns what’s normal from your data and identifies deviations.</p>
<p>The key insight: <strong>Anomalies are easier to isolate than normal points.</strong></p>
<p>Think of it like a game of “20 questions” for data points:</p>
<ul>
<li>Normal data points are similar to many others, requiring many questions to identify uniquely</li>
<li>Anomalies are outliers that can be identified with just a few questions</li>
</ul>
<p>We’ll explore Isolation Forest in detail in the <a href="#isolation-forest-ml-detection">Detection Layer</a> section.</p>
<h3 id="what-is-an-incident"><a class="header" href="#what-is-an-incident">What is an Incident?</a></h3>
<p>An <strong>incident</strong> is a tracked occurrence of an anomaly pattern. Yaga2 doesn’t just detect anomalies—it tracks them over time to:</p>
<ul>
<li><strong>Confirm</strong> anomalies aren’t transient glitches (require 2+ consecutive detections)</li>
<li><strong>Track duration</strong> of ongoing issues</li>
<li><strong>Prevent duplicates</strong> by recognizing the same issue across detection cycles</li>
<li><strong>Manage resolution</strong> with grace periods to avoid flapping</li>
</ul>
<h2 id="system-architecture"><a class="header" href="#system-architecture">System Architecture</a></h2>
<pre><code>┌─────────────────────────────────────────────────────────────────────────────┐
│                              Yaga2 Pipeline                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   ┌───────────────┐                                                         │
│   │ VictoriaMetrics│    Collect 5 core metrics every 2-3 minutes            │
│   │   (Metrics)    │    • request_rate    • dependency_latency                  │
│   └───────┬───────┘    • app_latency      • db_latency                      │
│           │            • error_rate                                         │
│           ▼                                                                 │
│   ┌───────────────────────────────────────────────────────────────────┐     │
│   │                     1. DETECTION LAYER                            │     │
│   │  ┌─────────────────────────┐    ┌─────────────────────────────┐   │     │
│   │  │    Isolation Forest     │    │    Pattern Matching         │   │     │
│   │  │    (ML Detection)       │───▶│    (Interpretation)         │   │     │
│   │  │                         │    │                             │   │     │
│   │  │ "Is this unusual?"      │    │ "What does it mean?"        │   │     │
│   │  └─────────────────────────┘    └─────────────────────────────┘   │     │
│   │                                                                   │     │
│   │  Output: anomaly detected, severity, pattern name                 │     │
│   └───────────────────────────────────────────────────────────────────┘     │
│           │                                                                 │
│           ▼                                                                 │
│   ┌───────────────────────────────────────────────────────────────────┐     │
│   │                     2. SLO EVALUATION LAYER                       │     │
│   │                                                                   │     │
│   │  Compare metrics against operational thresholds:                  │     │
│   │  • Latency vs SLO targets (acceptable/warning/critical)           │     │
│   │  • Error rate vs SLO targets                                      │     │
│   │  • Database latency vs baseline ratio                             │     │
│   │  • Request rate for surge/cliff                                   │     │
│   │                                                                   │     │
│   │  "Does this anomaly matter operationally?"                        │     │
│   │                                                                   │     │
│   │  Output: adjusted severity (critical/high/low), SLO status        │     │
│   └───────────────────────────────────────────────────────────────────┘     │
│           │                                                                 │
│           ▼                                                                 │
│   ┌───────────────────────────────────────────────────────────────────┐     │
│   │                     3. INCIDENT LIFECYCLE                         │     │
│   │                                                                   │     │
│   │  SUSPECTED ──▶ OPEN ──▶ RECOVERING ──▶ CLOSED                     │     │
│   │   (wait)     (alert)    (grace)       (resolve)                   │     │
│   │                                                                   │     │
│   │  • Confirmation: 2 consecutive detections before alerting         │     │
│   │  • Grace period: 3 cycles without detection before closing        │     │
│   │  • Fingerprinting: Track same issue across time                   │     │
│   │                                                                   │     │
│   │  "Should we alert now, or wait for confirmation?"                 │     │
│   └───────────────────────────────────────────────────────────────────┘     │
│           │                                                                 │
│           ▼                                                                 │
│   ┌───────────────┐                                                         │
│   │  Web API      │    Alert payload sent for confirmed incidents           │
│   │  Dashboard    │    Resolution payload sent when incidents close         │
│   └───────────────┘                                                         │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>
<h2 id="time-aware-detection"><a class="header" href="#time-aware-detection">Time-Aware Detection</a></h2>
<p>Service behavior varies significantly by time:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Time Period</th><th>Typical Behavior</th></tr>
</thead>
<tbody>
<tr><td>Business hours (Mon-Fri 8-18)</td><td>High traffic, tight latency requirements</td></tr>
<tr><td>Evening (Mon-Fri 18-22)</td><td>Moderate traffic</td></tr>
<tr><td>Night (22-06)</td><td>Low traffic, batch jobs</td></tr>
<tr><td>Weekend</td><td>Different patterns entirely</td></tr>
</tbody>
</table>
</div>
<p>A 3 AM traffic level that would be alarming at 3 PM is completely normal at night. Yaga2 trains <strong>separate models for each time period</strong> to avoid false positives from expected behavioral differences.</p>
<h2 id="the-five-metrics"><a class="header" href="#the-five-metrics">The Five Metrics</a></h2>
<p>Yaga2 monitors five core metrics for each service:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Metric</th><th>What It Measures</th><th>Why It Matters</th></tr>
</thead>
<tbody>
<tr><td><strong>request_rate</strong></td><td>Requests per second</td><td>Traffic volume - sudden drops or spikes indicate problems</td></tr>
<tr><td><strong>application_latency</strong></td><td>Server processing time (ms)</td><td>User experience - slow responses frustrate users</td></tr>
<tr><td><strong>dependency_latency</strong></td><td>External dependency call time (ms)</td><td>Downstream issues - if a dependency is slow, you’ll be slow</td></tr>
<tr><td><strong>database_latency</strong></td><td>Database query time (ms)</td><td>Database health - often the bottleneck</td></tr>
<tr><td><strong>error_rate</strong></td><td>Failed requests (0-1 ratio)</td><td>Reliability - errors directly impact users</td></tr>
</tbody>
</table>
</div>
<h2 id="how-to-use-this-guide"><a class="header" href="#how-to-use-this-guide">How to Use This Guide</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>If You Want To…</th><th>Read…</th></tr>
</thead>
<tbody>
<tr><td>Understand how ML detection works</td><td><a href="#isolation-forest-ml-detection">Isolation Forest</a></td></tr>
<tr><td>Learn about named patterns</td><td><a href="#pattern-matching">Pattern Matching</a></td></tr>
<tr><td>Configure SLO thresholds</td><td><a href="slo/README.html">SLO Evaluation</a></td></tr>
<tr><td>Understand alert timing</td><td><a href="incidents/README.html">Incident Lifecycle</a></td></tr>
<tr><td>Troubleshoot issues</td><td><a href="#troubleshooting">Troubleshooting</a></td></tr>
<tr><td>Quick reference</td><td><a href="#decision-matrix">Decision Matrix</a></td></tr>
</tbody>
</table>
</div>
<h2 id="quick-reference"><a class="header" href="#quick-reference">Quick Reference</a></h2>
<h3 id="severity-levels"><a class="header" href="#severity-levels">Severity Levels</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Severity</th><th>Meaning</th><th>Action Required</th></tr>
</thead>
<tbody>
<tr><td><strong>critical</strong></td><td>SLO breached, users impacted</td><td>Immediate response</td></tr>
<tr><td><strong>high</strong></td><td>Approaching SLO limits</td><td>Investigate promptly</td></tr>
<tr><td><strong>low</strong></td><td>Anomaly detected but within SLO</td><td>Monitor, no action</td></tr>
</tbody>
</table>
</div>
<h3 id="alert-flow-summary"><a class="header" href="#alert-flow-summary">Alert Flow Summary</a></h3>
<pre><code>Anomaly detected → SUSPECTED (no alert, wait for confirmation)
                        │
                        ▼ (detected again)
                    OPEN (alert sent!)
                        │
                        ▼ (not detected)
                  RECOVERING (grace period)
                        │
                        ▼ (still not detected)
                    CLOSED (resolution sent)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="system-overview"><a class="header" href="#system-overview">System Overview</a></h1>
<p>The Yaga2 anomaly detection system processes metrics through multiple layers to produce actionable alerts.</p>
<h2 id="architecture"><a class="header" href="#architecture">Architecture</a></h2>
<pre><code>┌─────────────────────────────────────────────────────────────────────────┐
│                         Inference Pipeline                               │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  ┌──────────────┐                                                        │
│  │ Victoria     │                                                        │
│  │ Metrics      │──┐                                                     │
│  └──────────────┘  │                                                     │
│                    ▼                                                     │
│  ┌─────────────────────────────────────────────────────────────────┐    │
│  │                    Detection Layer                               │    │
│  │  ┌─────────────────┐    ┌─────────────────┐                     │    │
│  │  │ Isolation Forest│───▶│ Pattern Matching │                     │    │
│  │  │ (ML Anomaly)    │    │ (Interpretation) │                     │    │
│  │  └─────────────────┘    └─────────────────┘                     │    │
│  └─────────────────────────────────────────────────────────────────┘    │
│                    │                                                     │
│                    ▼                                                     │
│  ┌─────────────────────────────────────────────────────────────────┐    │
│  │                    SLO Evaluation Layer                          │    │
│  │  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────────┐            │    │
│  │  │ Latency │ │ Errors  │ │ DB Lat  │ │ Request Rate│            │    │
│  │  └────┬────┘ └────┬────┘ └────┬────┘ └──────┬──────┘            │    │
│  │       └───────────┴───────────┴─────────────┘                    │    │
│  │                         │                                         │    │
│  │                         ▼                                         │    │
│  │              ┌─────────────────────┐                             │    │
│  │              │ Severity Adjustment │                             │    │
│  │              └─────────────────────┘                             │    │
│  └─────────────────────────────────────────────────────────────────┘    │
│                    │                                                     │
│                    ▼                                                     │
│  ┌─────────────────────────────────────────────────────────────────┐    │
│  │                    Incident Lifecycle                            │    │
│  │                                                                   │    │
│  │  SUSPECTED ──▶ OPEN ──▶ RECOVERING ──▶ CLOSED                   │    │
│  │      │          │           │             │                      │    │
│  │   (wait)    (alert)     (grace)      (resolve)                  │    │
│  └─────────────────────────────────────────────────────────────────┘    │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
</code></pre>
<h2 id="metrics-collected"><a class="header" href="#metrics-collected">Metrics Collected</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Metric</th><th>Description</th><th>Unit</th></tr>
</thead>
<tbody>
<tr><td><code>request_rate</code></td><td>Incoming requests per second</td><td>req/s</td></tr>
<tr><td><code>application_latency</code></td><td>Server-side processing time</td><td>ms</td></tr>
<tr><td><code>dependency_latency</code></td><td>External dependency call time</td><td>ms</td></tr>
<tr><td><code>database_latency</code></td><td>Database query time</td><td>ms</td></tr>
<tr><td><code>error_rate</code></td><td>Percentage of failed requests</td><td>ratio (0-1)</td></tr>
</tbody>
</table>
</div>
<h2 id="time-aware-detection-1"><a class="header" href="#time-aware-detection-1">Time-Aware Detection</a></h2>
<p>The system uses <strong>time-aware models</strong> trained separately for each behavioral period:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Period</th><th>Hours</th><th>Days</th></tr>
</thead>
<tbody>
<tr><td><code>business_hours</code></td><td>08:00 - 18:00</td><td>Mon-Fri</td></tr>
<tr><td><code>evening_hours</code></td><td>18:00 - 22:00</td><td>Mon-Fri</td></tr>
<tr><td><code>night_hours</code></td><td>22:00 - 06:00</td><td>Mon-Fri</td></tr>
<tr><td><code>weekend_day</code></td><td>08:00 - 22:00</td><td>Sat-Sun</td></tr>
<tr><td><code>weekend_night</code></td><td>22:00 - 08:00</td><td>Sat-Sun</td></tr>
</tbody>
</table>
</div>
<p>This prevents false positives from expected behavioral differences (e.g., low traffic at 3 AM is normal).</p>
<h2 id="two-pass-detection"><a class="header" href="#two-pass-detection">Two-Pass Detection</a></h2>
<p>For accurate cascade analysis, detection runs in two passes:</p>
<ol>
<li><strong>Pass 1</strong>: Detect anomalies for all services (no dependency context)</li>
<li><strong>Pass 2</strong>: Re-analyze latency anomalies with dependency context</li>
</ol>
<p>This enables identifying root cause services in dependency chains.</p>
<h2 id="output"><a class="header" href="#output">Output</a></h2>
<p>Each inference run produces:</p>
<ul>
<li><strong>Anomaly alerts</strong> for services with detected issues</li>
<li><strong>Resolution notifications</strong> for incidents that cleared</li>
<li><strong>Enrichment data</strong> (exceptions, service graph) when available</li>
</ul>
<p>See <a href="#api-payload-reference">API Payload</a> for the complete output format.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="detection-layer"><a class="header" href="#detection-layer">Detection Layer</a></h1>
<p>The detection layer identifies anomalies using a <strong>sequential pipeline</strong> where ML detection triggers pattern interpretation.</p>
<h2 id="pipeline-flow"><a class="header" href="#pipeline-flow">Pipeline Flow</a></h2>
<pre><code>Current Metrics
      │
      ▼
┌─────────────────────────────────────┐
│   Phase 1: Isolation Forest (IF)    │
│   - Per-metric anomaly scoring      │
│   - Multivariate relationship check │
│                                     │
│   Output: List of AnomalySignals    │
└──────────────────┬──────────────────┘
                   │
                   ▼
┌─────────────────────────────────────┐
│   Phase 2: Pattern Interpretation   │
│   - Convert signals to levels       │
│   - Match against named patterns    │
│   - Build interpreted anomaly       │
└──────────────────┬──────────────────┘
                   │
                   ▼
           Single Alert
     (with pattern interpretation)
</code></pre>
<h2 id="key-concepts"><a class="header" href="#key-concepts">Key Concepts</a></h2>
<h3 id="anomaly-signal"><a class="header" href="#anomaly-signal">Anomaly Signal</a></h3>
<p>When IF detects an anomaly, it produces a signal:</p>
<pre><code class="language-json">{
  "metric": "application_latency",
  "score": -0.35,
  "direction": "high",
  "percentile": 92.0
}
</code></pre>
<h3 id="metric-levels"><a class="header" href="#metric-levels">Metric Levels</a></h3>
<p>Signals are converted to semantic levels for pattern matching:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Percentile</th><th>Level</th></tr>
</thead>
<tbody>
<tr><td>&gt; 95</td><td><code>very_high</code></td></tr>
<tr><td>90 - 95</td><td><code>high</code></td></tr>
<tr><td>80 - 90</td><td><code>elevated</code></td></tr>
<tr><td>70 - 80</td><td><code>moderate</code></td></tr>
<tr><td>10 - 70</td><td><code>normal</code></td></tr>
<tr><td>5 - 10</td><td><code>low</code></td></tr>
<tr><td>&lt; 5</td><td><code>very_low</code></td></tr>
</tbody>
</table>
</div>
<h3 id="lower-is-better-metrics"><a class="header" href="#lower-is-better-metrics">Lower-is-Better Metrics</a></h3>
<p>Some metrics treat low values as improvements, not anomalies:</p>
<ul>
<li><code>database_latency</code> - Faster queries are good</li>
<li><code>dependency_latency</code> - Faster dependencies are good</li>
<li><code>error_rate</code> - Fewer errors are good</li>
</ul>
<p>For these metrics, when IF flags them as “low”, they are treated as <code>normal</code>.</p>
<h2 id="detection-methods"><a class="header" href="#detection-methods">Detection Methods</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Method</th><th>Type</th><th>Best For</th></tr>
</thead>
<tbody>
<tr><td><a href="#isolation-forest-ml-detection">Isolation Forest</a></td><td>ML</td><td>Novel/unknown anomalies</td></tr>
<tr><td><a href="#pattern-matching">Pattern Matching</a></td><td>Rule-based</td><td>Known operational scenarios</td></tr>
</tbody>
</table>
</div>
<h2 id="sections"><a class="header" href="#sections">Sections</a></h2>
<ul>
<li><a href="#isolation-forest-ml-detection">Isolation Forest (ML)</a> - How ML detection works</li>
<li><a href="#pattern-matching">Pattern Matching</a> - Named patterns and interpretations</li>
<li><a href="#detection-pipeline">Detection Pipeline</a> - End-to-end detection flow</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="isolation-forest-ml-detection"><a class="header" href="#isolation-forest-ml-detection">Isolation Forest (ML Detection)</a></h1>
<p>Isolation Forest is an unsupervised machine learning algorithm that detects anomalies by measuring how easy it is to “isolate” a data point. This chapter explains how it works, why it’s effective for service metrics, and how Yaga2 uses it.</p>
<h2 id="what-is-unsupervised-learning"><a class="header" href="#what-is-unsupervised-learning">What is Unsupervised Learning?</a></h2>
<p>Machine learning algorithms fall into two main categories:</p>
<ul>
<li><strong>Supervised learning</strong>: Requires labeled training data. For anomaly detection, you’d need examples labeled “normal” and “anomaly.”</li>
<li><strong>Unsupervised learning</strong>: Learns patterns from unlabeled data. No need to manually identify anomalies in advance.</li>
</ul>
<p><strong>Why unsupervised matters for anomaly detection:</strong></p>
<p>Labeling anomalies is impractical:</p>
<ul>
<li>You can’t predict all future failure modes</li>
<li>What’s “anomalous” changes as services evolve</li>
<li>Manual labeling is expensive and error-prone</li>
</ul>
<p>Isolation Forest learns what’s “normal” from your historical data and flags anything that deviates—no labels required.</p>
<h2 id="the-core-insight"><a class="header" href="#the-core-insight">The Core Insight</a></h2>
<p><strong>Anomalies are easier to isolate than normal points.</strong></p>
<p>Think about it visually:</p>
<pre><code>Normal data (clustered):           Anomaly (isolated):

        ●  ●  ●
      ● ● ●● ● ●                              ●
     ● ● ●●●● ● ●                     (far from cluster)
      ● ● ●● ● ●
        ●  ●  ●

   Many similar points               Alone in feature space
   Hard to distinguish               Easy to identify
</code></pre>
<p>If you were to randomly draw dividing lines to separate data points:</p>
<ul>
<li><strong>Normal points</strong>: Buried in a cluster, need many cuts to isolate</li>
<li><strong>Anomalies</strong>: Sitting alone, isolated in just 1-2 cuts</li>
</ul>
<h2 id="how-isolation-forest-works"><a class="header" href="#how-isolation-forest-works">How Isolation Forest Works</a></h2>
<h3 id="step-1-build-random-trees-training"><a class="header" href="#step-1-build-random-trees-training">Step 1: Build Random Trees (Training)</a></h3>
<p>Isolation Forest builds many random decision trees (called “isolation trees”):</p>
<pre><code>                    All Data Points
                          │
                ┌─────────┴─────────┐
                │ Split on latency  │
                │ at random value   │
                │ (e.g., 234ms)     │
                └─────────┬─────────┘
                          │
            ┌─────────────┴─────────────┐
            │                           │
     latency &lt; 234ms              latency &gt;= 234ms
            │                           │
            ▼                           ▼
     ┌──────────────┐            ┌──────────────┐
     │ Split on     │            │ Split on     │
     │ request_rate │            │ error_rate   │
     │ at 150 req/s │            │ at 0.05      │
     └──────────────┘            └──────────────┘
            │                           │
           ...                         ...
     (continue until each             (continue)
      point is isolated)
</code></pre>
<p>Each tree:</p>
<ol>
<li><strong>Randomly picks a feature</strong> (e.g., latency, error_rate)</li>
<li><strong>Randomly picks a split value</strong> between the min and max of that feature</li>
<li><strong>Recursively partitions</strong> until each point is isolated</li>
</ol>
<h3 id="step-2-measure-path-length"><a class="header" href="#step-2-measure-path-length">Step 2: Measure Path Length</a></h3>
<p>For each data point, count how many splits were needed to isolate it:</p>
<pre><code>Anomaly path:                    Normal point path:

Root ──▶ Left ──▶ ISOLATED       Root ──▶ Right ──▶ Left ──▶ Right ──▶ Left ──▶ ISOLATED

Path length: 2                   Path length: 5
(suspicious!)                    (normal)
</code></pre>
<p><strong>Key principle</strong>:</p>
<ul>
<li>Short paths → Anomalous (easy to isolate)</li>
<li>Long paths → Normal (hard to isolate)</li>
</ul>
<h3 id="step-3-compute-anomaly-score"><a class="header" href="#step-3-compute-anomaly-score">Step 3: Compute Anomaly Score</a></h3>
<p>Average the path lengths across all trees and normalize:</p>
<pre><code>                        2^(-average_path_length / c(n))
Anomaly Score = ─────────────────────────────────────────
                              (normalized)
</code></pre>
<p>Where <code>c(n)</code> is the average path length of an unsuccessful search in a binary search tree of n samples.</p>
<p><strong>Score interpretation:</strong></p>
<ul>
<li><strong>Score ≈ 1</strong>: Highly anomalous (very short paths)</li>
<li><strong>Score ≈ 0.5</strong>: On the boundary</li>
<li><strong>Score ≈ 0</strong>: Normal (long paths)</li>
</ul>
<p>Yaga2 uses the scikit-learn <code>decision_function()</code> which returns scores in [-1, 1]:</p>
<ul>
<li><strong>Negative scores</strong>: Anomalous (more negative = more anomalous)</li>
<li><strong>Positive scores</strong>: Normal</li>
</ul>
<h2 id="visual-example"><a class="header" href="#visual-example">Visual Example</a></h2>
<p>Consider 2D data with latency and request_rate:</p>
<pre><code>request_rate
     │
 500 │                    ★ Anomaly A
     │                      (high rate,
     │                       isolated quickly)
 300 │    ●●●●●
     │   ●●●●●●●
 200 │  ●●●●●●●●●   Normal cluster
     │   ●●●●●●●    (many splits needed)
 100 │    ●●●●●
     │
  50 │  ★ Anomaly B
     │  (low rate, isolated quickly)
     │
     └────────────────────────────────── latency
          100    200    300    400    500

Random split 1: latency &lt; 350ms
Random split 2: request_rate &lt; 400

Result:
- Anomaly A: Isolated in 2 splits (score ≈ -0.5)
- Anomaly B: Isolated in 2 splits (score ≈ -0.5)
- Normal points: Need 5-8 splits (score ≈ 0.1)
</code></pre>
<h2 id="why-isolation-forest-is-ideal-for-service-metrics"><a class="header" href="#why-isolation-forest-is-ideal-for-service-metrics">Why Isolation Forest is Ideal for Service Metrics</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Property</th><th>Why It Matters for Metrics</th></tr>
</thead>
<tbody>
<tr><td><strong>Unsupervised</strong></td><td>No need to label historical incidents</td></tr>
<tr><td><strong>Fast</strong></td><td>O(n log n) - handles high-volume metrics</td></tr>
<tr><td><strong>Handles skew</strong></td><td>Latency distributions are typically long-tailed</td></tr>
<tr><td><strong>No distribution assumptions</strong></td><td>Unlike z-score, doesn’t assume Gaussian</td></tr>
<tr><td><strong>Works with multiple features</strong></td><td>Can detect unusual metric <em>combinations</em></td></tr>
</tbody>
</table>
</div>
<h3 id="comparison-with-other-methods"><a class="header" href="#comparison-with-other-methods">Comparison with Other Methods</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Method</th><th>Limitation for Metrics</th></tr>
</thead>
<tbody>
<tr><td><strong>Z-score / Standard Deviation</strong></td><td>Assumes normal distribution - latency is skewed</td></tr>
<tr><td><strong>Static thresholds</strong></td><td>Brittle, need constant tuning</td></tr>
<tr><td><strong>LSTM / Deep Learning</strong></td><td>Requires labeled data, computationally expensive</td></tr>
<tr><td><strong>Local Outlier Factor</strong></td><td>Slow at inference time</td></tr>
<tr><td><strong>One-Class SVM</strong></td><td>Sensitive to hyperparameters</td></tr>
</tbody>
</table>
</div>
<h2 id="model-types-in-yaga2"><a class="header" href="#model-types-in-yaga2">Model Types in Yaga2</a></h2>
<h3 id="univariate-models-per-metric"><a class="header" href="#univariate-models-per-metric">Univariate Models (Per-Metric)</a></h3>
<p>Separate model for each metric, detecting single-metric anomalies:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Model</th><th>What It Detects</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>request_rate_isolation</code></td><td>Traffic anomalies</td><td>Sudden drop to 10% of normal</td></tr>
<tr><td><code>application_latency_isolation</code></td><td>Response time issues</td><td>Latency spike to 500ms (normally 100ms)</td></tr>
<tr><td><code>error_rate_isolation</code></td><td>Error spikes</td><td>Error rate jumps to 5% (normally 0.1%)</td></tr>
<tr><td><code>dependency_latency_isolation</code></td><td>Dependency slowness</td><td>External calls taking 2s (normally 200ms)</td></tr>
<tr><td><code>database_latency_isolation</code></td><td>Database issues</td><td>Query time spikes</td></tr>
</tbody>
</table>
</div>
<h3 id="multivariate-model-combined"><a class="header" href="#multivariate-model-combined">Multivariate Model (Combined)</a></h3>
<p>One model considering all metrics together. This detects unusual <strong>combinations</strong> that look normal individually:</p>
<pre><code>Example: Fast-Fail Pattern

Individual metrics:
  request_rate: 100 req/s     ← Normal
  application_latency: 5ms    ← Very fast (good?)
  error_rate: 40%             ← High

Analysis:
  - Each metric might not trigger univariate detection
  - But the COMBINATION is suspicious:
    "Very fast responses + high errors = requests failing before processing"

  Multivariate model catches this pattern!
</code></pre>
<h2 id="contamination-rate"><a class="header" href="#contamination-rate">Contamination Rate</a></h2>
<p><strong>Contamination</strong> is a key hyperparameter—the expected proportion of anomalies in training data.</p>
<pre><code class="language-python">IsolationForest(contamination=0.05)  # Expect 5% anomalies
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Contamination</th><th>Effect</th><th>Use Case</th></tr>
</thead>
<tbody>
<tr><td><strong>0.01-0.03</strong></td><td>Very strict, few anomalies flagged</td><td>Critical services where false positives are costly</td></tr>
<tr><td><strong>0.05</strong></td><td>Balanced (default)</td><td>Most services</td></tr>
<tr><td><strong>0.08-0.10</strong></td><td>More sensitive, more anomalies flagged</td><td>Noisy services, development environments</td></tr>
</tbody>
</table>
</div>
<h3 id="how-yaga2-sets-contamination"><a class="header" href="#how-yaga2-sets-contamination">How Yaga2 Sets Contamination</a></h3>
<p>Yaga2 estimates contamination automatically using <strong>knee detection</strong>:</p>
<ol>
<li>Train a preliminary model with <code>contamination="auto"</code></li>
<li>Get anomaly scores for all training data</li>
<li>Sort scores and find the “knee” (bend in the curve)</li>
<li>Points beyond the knee are considered anomalies</li>
<li>Their proportion becomes the estimated contamination</li>
</ol>
<pre><code>Anomaly Scores (sorted)

     │
  0  │────────────────────●●●●●●●●●●●●●●●
     │                   ●
     │                  ●
-0.2 │                 ●  ← Knee point
     │               ●
     │             ●
-0.5 │          ●
     │       ●
     │    ●
-0.8 │ ●
     └─────────────────────────────────────
       1   20   40   60   80  100 (samples)

Knee at sample 95 → contamination ≈ 5%
</code></pre>
<h2 id="severity-thresholds"><a class="header" href="#severity-thresholds">Severity Thresholds</a></h2>
<p>Anomaly scores are mapped to severity levels:</p>
<h3 id="calibrated-thresholds-preferred"><a class="header" href="#calibrated-thresholds-preferred">Calibrated Thresholds (Preferred)</a></h3>
<p>During training, thresholds are calibrated from validation data:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Severity</th><th>Percentile</th><th>Meaning</th></tr>
</thead>
<tbody>
<tr><td><strong>Critical</strong></td><td>Bottom 0.1%</td><td>1 in 1000 - extremely rare</td></tr>
<tr><td><strong>High</strong></td><td>Bottom 1%</td><td>1 in 100 - significant</td></tr>
<tr><td><strong>Medium</strong></td><td>Bottom 5%</td><td>1 in 20 - moderate</td></tr>
<tr><td><strong>Low</strong></td><td>Bottom 10%</td><td>1 in 10 - minor</td></tr>
</tbody>
</table>
</div>
<h3 id="fallback-thresholds"><a class="header" href="#fallback-thresholds">Fallback Thresholds</a></h3>
<p>If calibration data unavailable:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Severity</th><th>Score Range</th></tr>
</thead>
<tbody>
<tr><td><strong>Critical</strong></td><td>&lt; -0.6</td></tr>
<tr><td><strong>High</strong></td><td>-0.6 to -0.3</td></tr>
<tr><td><strong>Medium</strong></td><td>-0.3 to -0.1</td></tr>
<tr><td><strong>Low</strong></td><td>&gt;= -0.1</td></tr>
</tbody>
</table>
</div>
<h2 id="training-process"><a class="header" href="#training-process">Training Process</a></h2>
<h3 id="data-requirements"><a class="header" href="#data-requirements">Data Requirements</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Requirement</th><th>Value</th><th>Why</th></tr>
</thead>
<tbody>
<tr><td>Training period</td><td>30 days</td><td>Capture weekly patterns</td></tr>
<tr><td>Min univariate samples</td><td>500</td><td>Statistical significance</td></tr>
<tr><td>Min multivariate samples</td><td>1000</td><td>Need more for cross-metric patterns</td></tr>
<tr><td>Data granularity</td><td>5 minutes</td><td>Balance detail vs noise</td></tr>
</tbody>
</table>
</div>
<h3 id="training-steps"><a class="header" href="#training-steps">Training Steps</a></h3>
<ol>
<li><strong>Fetch 30 days of metrics</strong> from VictoriaMetrics</li>
<li><strong>Segment by time period</strong> (business hours, night, weekend)</li>
<li><strong>Temporal split</strong>: 80% training, 20% validation (chronological)</li>
<li><strong>Estimate contamination</strong> from data distribution</li>
<li><strong>Train univariate models</strong> for each metric</li>
<li><strong>Train multivariate model</strong> on all metrics</li>
<li><strong>Calibrate severity thresholds</strong> on validation data</li>
<li><strong>Save models and metadata</strong></li>
</ol>
<h3 id="why-temporal-split"><a class="header" href="#why-temporal-split">Why Temporal Split?</a></h3>
<pre><code>WRONG: Random split
─────────────────────
Training:   ●  ●  ●  ●  ●  ●  ●  ●  (random samples)
Validation:    ●     ●     ●     ●  (random samples)

Problem: Training might include future data → data leakage


CORRECT: Temporal split
───────────────────────
Training:   ●──●──●──●──●──●──●──●──●──│  (first 80%)
Validation:                            │──●──●──●──●  (last 20%)
                                       │
                                    Split point

Model learns only from past, tested on "future" data
</code></pre>
<h2 id="time-aware-models"><a class="header" href="#time-aware-models">Time-Aware Models</a></h2>
<p>Service behavior varies by time of day:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Time Period</th><th>Characteristics</th></tr>
</thead>
<tbody>
<tr><td>business_hours (08-18 weekdays)</td><td>High traffic, tight latency</td></tr>
<tr><td>evening_hours (18-22 weekdays)</td><td>Moderate traffic</td></tr>
<tr><td>night_hours (22-06 weekdays)</td><td>Low traffic, batch jobs</td></tr>
<tr><td>weekend_day (08-22 Sat-Sun)</td><td>Variable patterns</td></tr>
<tr><td>weekend_night (22-08 Sat-Sun)</td><td>Very low traffic</td></tr>
</tbody>
</table>
</div>
<p><strong>Why separate models?</strong></p>
<p>Without time awareness:</p>
<pre><code>3 AM: 10 requests/second
  → Model: "This is way below the 500 req/s average - ANOMALY!"
  → Reality: This is normal at 3 AM
  → Result: False positive
</code></pre>
<p>With time-aware models:</p>
<pre><code>3 AM: 10 requests/second
  → Night model average: 15 req/s
  → Model: "This is close to the night baseline - NORMAL"
  → Result: Correct!
</code></pre>
<h2 id="strengths-and-limitations"><a class="header" href="#strengths-and-limitations">Strengths and Limitations</a></h2>
<h3 id="strengths"><a class="header" href="#strengths">Strengths</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Strength</th><th>Explanation</th></tr>
</thead>
<tbody>
<tr><td><strong>No labeled data needed</strong></td><td>Learns from normal patterns, no need to tag anomalies</td></tr>
<tr><td><strong>Fast training and inference</strong></td><td>O(n log n) complexity, scales to millions of points</td></tr>
<tr><td><strong>Works with skewed data</strong></td><td>Latency distributions are naturally long-tailed</td></tr>
<tr><td><strong>Detects novel anomalies</strong></td><td>Can find patterns never seen before</td></tr>
<tr><td><strong>Handles multiple dimensions</strong></td><td>Multivariate model catches unusual combinations</td></tr>
</tbody>
</table>
</div>
<h3 id="limitations"><a class="header" href="#limitations">Limitations</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Limitation</th><th>How Yaga2 Mitigates</th></tr>
</thead>
<tbody>
<tr><td><strong>No semantic understanding</strong></td><td>Pattern matching interprets what anomalies mean</td></tr>
<tr><td><strong>Context-blind</strong></td><td>Time-aware models reduce false positives</td></tr>
<tr><td><strong>Can produce false positives</strong></td><td>SLO evaluation filters operationally insignificant anomalies</td></tr>
<tr><td><strong>Training data quality matters</strong></td><td>Data quality checks and minimum sample requirements</td></tr>
<tr><td><strong>Gradual drift may be missed</strong></td><td>Daily retraining, drift detection</td></tr>
</tbody>
</table>
</div>
<h2 id="how-detection-works-at-inference-time"><a class="header" href="#how-detection-works-at-inference-time">How Detection Works at Inference Time</a></h2>
<pre><code>1. Current Metrics Arrive
   {request_rate: 450, latency: 280, errors: 0.02, ...}
                    │
                    ▼
2. Determine Time Period
   Current time: 10:30 AM Wednesday → "business_hours"
                    │
                    ▼
3. Load Appropriate Models
   booking/business_hours/model.joblib
                    │
                    ▼
4. Scale Metrics
   RobustScaler normalizes to training distribution
                    │
                    ▼
5. Score Each Model
   Univariate: latency_score = -0.35 (high!)
               request_score = 0.1 (normal)
               error_score = 0.05 (normal)
   Multivariate: combined_score = -0.28 (elevated)
                    │
                    ▼
6. Map to Severity
   latency_score -0.35 → severity: "high"
                    │
                    ▼
7. Pattern Matching
   High latency + normal traffic + normal errors
   → Pattern: "latency_spike_recent"
                    │
                    ▼
8. Output Anomaly
   {
     "pattern_name": "latency_spike_recent",
     "severity": "high",
     "score": -0.35,
     "description": "Latency spike: 280ms (normally 120ms)"
   }
</code></pre>
<h2 id="further-reading"><a class="header" href="#further-reading">Further Reading</a></h2>
<ul>
<li><a href="#pattern-matching">Pattern Matching</a> - How anomalies are interpreted</li>
<li><a href="#detection-pipeline">Detection Pipeline</a> - End-to-end detection flow</li>
<li><a href="slo/README.html">SLO Evaluation</a> - How severity is adjusted</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="pattern-matching"><a class="header" href="#pattern-matching">Pattern Matching</a></h1>
<p>Pattern matching interprets ML signals by comparing metric combinations against known operational scenarios. This chapter explains what pattern matching is, how it works, and provides detailed examples of each pattern type.</p>
<h2 id="what-is-pattern-matching"><a class="header" href="#what-is-pattern-matching">What is Pattern Matching?</a></h2>
<p>While Isolation Forest answers <strong>“Is this unusual?”</strong>, pattern matching answers <strong>“What does this unusual behavior mean?”</strong></p>
<p>Pattern matching is a <strong>rule-based interpretation layer</strong> that recognizes known operational scenarios by examining how metrics relate to each other.</p>
<h3 id="why-pattern-matching"><a class="header" href="#why-pattern-matching">Why Pattern Matching?</a></h3>
<p>ML detection tells you something is anomalous, but not what it means:</p>
<pre><code>ML Output:
  application_latency: anomalous (score: -0.35)
  request_rate: anomalous (score: -0.28)
  error_rate: normal

Human question: "So... is this bad? What should I do?"
</code></pre>
<p>Pattern matching adds semantic meaning:</p>
<pre><code>Pattern Match: traffic_surge_degrading

Interpretation: "You have a traffic surge (3x normal) that's causing
  latency degradation (450ms vs 150ms normal). Errors are still normal,
  so you're approaching capacity but not failing yet."

Recommended Actions:
  1. Scale horizontally if possible
  2. Check resource bottlenecks (CPU, connections)
  3. Monitor error rate for signs of impending failure
</code></pre>
<h3 id="ml-vs-pattern-matching"><a class="header" href="#ml-vs-pattern-matching">ML vs Pattern Matching</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Aspect</th><th>Isolation Forest</th><th>Pattern Matching</th></tr>
</thead>
<tbody>
<tr><td><strong>Approach</strong></td><td>Statistical (unsupervised ML)</td><td>Rule-based (expert knowledge)</td></tr>
<tr><td><strong>Question answered</strong></td><td>“Is this unusual?”</td><td>“What does it mean?”</td></tr>
<tr><td><strong>Learns from</strong></td><td>Historical data</td><td>Domain expertise</td></tr>
<tr><td><strong>Catches</strong></td><td>Novel/unknown issues</td><td>Known operational scenarios</td></tr>
<tr><td><strong>Output</strong></td><td>Anomaly score</td><td>Named pattern + recommendations</td></tr>
<tr><td><strong>Adapts</strong></td><td>Automatically via training</td><td>Manually via pattern definitions</td></tr>
</tbody>
</table>
</div>
<h3 id="how-they-work-together"><a class="header" href="#how-they-work-together">How They Work Together</a></h3>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│                   Sequential Detection Pipeline                      │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│   Current Metrics                                                   │
│   ┌────────────────────────────────────┐                           │
│   │ request_rate: 850 req/s           │                           │
│   │ application_latency: 450ms         │                           │
│   │ error_rate: 1.5%                   │                           │
│   │ database_latency: 15ms             │                           │
│   └─────────────────┬──────────────────┘                           │
│                     │                                               │
│                     ▼                                               │
│   ┌─────────────────────────────────────────────────────────────┐   │
│   │  Phase 1: Isolation Forest Detection                        │   │
│   │                                                             │   │
│   │  For each metric, compute anomaly score:                    │   │
│   │    request_rate: score=-0.28, direction=high               │   │
│   │    application_latency: score=-0.35, direction=high        │   │
│   │    error_rate: score=-0.05, direction=normal               │   │
│   │    database_latency: score=0.1, direction=normal           │   │
│   │                                                             │   │
│   │  Output: List of AnomalySignal objects                     │   │
│   └─────────────────────────┬───────────────────────────────────┘   │
│                             │                                       │
│                             ▼                                       │
│   ┌─────────────────────────────────────────────────────────────┐   │
│   │  Phase 2: Pattern Interpretation                            │   │
│   │                                                             │   │
│   │  Convert signals to levels:                                 │   │
│   │    request_rate: high                                      │   │
│   │    application_latency: high                               │   │
│   │    error_rate: normal                                      │   │
│   │    database_latency: normal                                │   │
│   │                                                             │   │
│   │  Match against patterns:                                    │   │
│   │    traffic_surge_failing? No (errors not high)             │   │
│   │    traffic_surge_degrading? ✓ YES                          │   │
│   │                                                             │   │
│   │  Output: Interpreted anomaly with recommendations          │   │
│   └─────────────────────────────────────────────────────────────┘   │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h2 id="how-pattern-matching-works"><a class="header" href="#how-pattern-matching-works">How Pattern Matching Works</a></h2>
<h3 id="step-1-convert-if-signals-to-metric-levels"><a class="header" href="#step-1-convert-if-signals-to-metric-levels">Step 1: Convert IF Signals to Metric Levels</a></h3>
<p>Each metric is classified based on its Isolation Forest signal:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Percentile Range</th><th>Level</th></tr>
</thead>
<tbody>
<tr><td>&gt; 95th</td><td><code>very_high</code></td></tr>
<tr><td>90th - 95th</td><td><code>high</code></td></tr>
<tr><td>80th - 90th</td><td><code>elevated</code></td></tr>
<tr><td>70th - 80th</td><td><code>moderate</code></td></tr>
<tr><td>10th - 70th</td><td><code>normal</code></td></tr>
<tr><td>5th - 10th</td><td><code>low</code></td></tr>
<tr><td>&lt; 5th</td><td><code>very_low</code></td></tr>
<tr><td>No IF signal</td><td><code>normal</code></td></tr>
</tbody>
</table>
</div>
<p><strong>Example:</strong></p>
<pre><code>IF says: latency is at 92nd percentile (score: -0.35)
Level: high (between 90th-95th)
</code></pre>
<h3 id="step-2-handle-lower-is-better-metrics"><a class="header" href="#step-2-handle-lower-is-better-metrics">Step 2: Handle “Lower is Better” Metrics</a></h3>
<p>Some metrics are better when low:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Metric</th><th>Lower is Better?</th><th>Why</th></tr>
</thead>
<tbody>
<tr><td><code>error_rate</code></td><td>Yes</td><td>0% errors is ideal</td></tr>
<tr><td><code>database_latency</code></td><td>Yes</td><td>Faster DB is better</td></tr>
<tr><td><code>dependency_latency</code></td><td>Yes</td><td>Faster dependencies is better</td></tr>
<tr><td><code>application_latency</code></td><td>No*</td><td>Low + high errors = fast-fail</td></tr>
</tbody>
</table>
</div>
<p>*Application latency is NOT in “lower is better” because low latency with high errors indicates fast failures (requests rejected before processing).</p>
<p>When IF flags a “lower is better” metric as anomalously low, it’s treated as <code>normal</code>:</p>
<pre><code>IF says: database_latency is unusually low (faster than normal)
Pattern matching: Treat as "normal" (this is good, not a problem)
</code></pre>
<h3 id="step-3-match-against-pattern-conditions"><a class="header" href="#step-3-match-against-pattern-conditions">Step 3: Match Against Pattern Conditions</a></h3>
<p>Each pattern has conditions that must be met:</p>
<pre><code class="language-python">"traffic_surge_degrading": {
    "conditions": {
        "request_rate": "high",           # Must be high
        "application_latency": "high",    # Must be high
        "error_rate": "normal",           # Must be normal
    }
}
</code></pre>
<p>The pattern matches only if <strong>all conditions</strong> are satisfied.</p>
<h3 id="step-4-generate-interpreted-output"><a class="header" href="#step-4-generate-interpreted-output">Step 4: Generate Interpreted Output</a></h3>
<p>When a pattern matches, generate:</p>
<ul>
<li>Human-readable description</li>
<li>Semantic interpretation</li>
<li>Recommended actions</li>
<li>Contributing metrics</li>
</ul>
<h2 id="named-patterns---detailed-reference"><a class="header" href="#named-patterns---detailed-reference">Named Patterns - Detailed Reference</a></h2>
<h3 id="traffic-patterns"><a class="header" href="#traffic-patterns">Traffic Patterns</a></h3>
<p>These patterns relate to changes in traffic volume (request rate).</p>
<h4 id="traffic_surge_healthy"><a class="header" href="#traffic_surge_healthy"><code>traffic_surge_healthy</code></a></h4>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│  PATTERN: traffic_surge_healthy                                     │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  Conditions:                                                        │
│    request_rate: high                                              │
│    application_latency: normal                                     │
│    error_rate: normal                                              │
│                                                                     │
│  Severity: low                                                      │
│                                                                     │
│  What It Means:                                                     │
│    Traffic has increased significantly (e.g., 3x normal) but       │
│    the service is handling it gracefully. Latency and errors       │
│    remain normal - your system has headroom.                       │
│                                                                     │
│  Possible Causes:                                                   │
│    • Marketing campaign launched                                   │
│    • Viral content / social media mention                          │
│    • Seasonal peak (holiday shopping)                              │
│    • Organic growth                                                │
│                                                                     │
│  Recommended Actions:                                               │
│    • Monitor for signs of degradation                              │
│    • Verify traffic is legitimate (not bot/attack)                 │
│    • Consider pre-emptive scaling if trend continues               │
│                                                                     │
│  Example Scenario:                                                  │
│    Black Friday starts. Traffic jumps from 100 req/s to 350 req/s. │
│    Latency stays at 150ms, errors at 0.1%.                         │
│    System is handling the surge well.                              │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h4 id="traffic_surge_degrading"><a class="header" href="#traffic_surge_degrading"><code>traffic_surge_degrading</code></a></h4>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│  PATTERN: traffic_surge_degrading                                   │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  Conditions:                                                        │
│    request_rate: high                                              │
│    application_latency: high                                       │
│    error_rate: normal                                              │
│                                                                     │
│  Severity: high                                                     │
│                                                                     │
│  What It Means:                                                     │
│    Traffic surge is causing performance degradation. Users are     │
│    experiencing slower responses, but requests are still           │
│    completing successfully. You're approaching capacity.           │
│                                                                     │
│  Why Errors Are Still Normal:                                       │
│    The system is slowing down to cope with load rather than        │
│    failing. This is often due to:                                  │
│    • Thread pools filling up (requests queue)                      │
│    • Connection pools near exhaustion                              │
│    • CPU at high utilization but not 100%                          │
│                                                                     │
│  Recommended Actions:                                               │
│    • IMMEDIATE: Scale horizontally if possible                     │
│    • CHECK: Resource utilization (CPU, memory, connections)        │
│    • MONITOR: Error rate for signs of impending failure            │
│    • CONSIDER: Enable request throttling to protect backend        │
│                                                                     │
│  Example Scenario:                                                  │
│    Traffic: 850 req/s (normally 200 req/s)                         │
│    Latency: 450ms (normally 120ms)                                 │
│    Errors: 0.5% (normal)                                           │
│                                                                     │
│    Users notice slowness but can still complete transactions.      │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h4 id="traffic_surge_failing"><a class="header" href="#traffic_surge_failing"><code>traffic_surge_failing</code></a></h4>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│  PATTERN: traffic_surge_failing                                     │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  Conditions:                                                        │
│    request_rate: high                                              │
│    application_latency: high                                       │
│    error_rate: high                                                │
│                                                                     │
│  Severity: critical                                                 │
│                                                                     │
│  What It Means:                                                     │
│    Service is at or beyond capacity. The traffic surge has         │
│    overwhelmed the system - both latency and errors are elevated.  │
│    Users are actively being impacted.                              │
│                                                                     │
│  Why This Is Critical:                                              │
│    High errors + high latency = users are both waiting AND         │
│    failing. This is the worst user experience - slow failures.     │
│                                                                     │
│  Recommended Actions:                                               │
│    • IMMEDIATE: Scale horizontally or add capacity                 │
│    • IMMEDIATE: Enable rate limiting to protect backend            │
│    • INVESTIGATE: Confirm traffic is legitimate vs attack          │
│    • PREPARE: Have rollback ready if recent deployment caused it   │
│                                                                     │
│  Example Scenario:                                                  │
│    Traffic: 1200 req/s (normally 200 req/s)                        │
│    Latency: 2500ms (normally 120ms)                                │
│    Errors: 15%                                                     │
│                                                                     │
│    System is collapsing under load. Many users seeing errors,      │
│    others waiting forever for timeouts.                            │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h4 id="traffic_cliff"><a class="header" href="#traffic_cliff"><code>traffic_cliff</code></a></h4>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│  PATTERN: traffic_cliff                                             │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  Conditions:                                                        │
│    request_rate: very_low (sudden drop to &lt;10% of normal)          │
│                                                                     │
│  Severity: critical                                                 │
│                                                                     │
│  What It Means:                                                     │
│    Traffic has suddenly dropped to near-zero. This often           │
│    indicates an upstream problem preventing requests from          │
│    reaching the service.                                           │
│                                                                     │
│  Common Causes:                                                     │
│    • DNS failure (users can't resolve hostname)                    │
│    • Load balancer misconfigured (routing to wrong backend)        │
│    • Firewall rule blocking traffic                                │
│    • CDN/edge failure                                              │
│    • Upstream service failure                                      │
│    • Network partition                                             │
│                                                                     │
│  Why Not Low Latency Pattern:                                       │
│    The few requests getting through might show normal latency,     │
│    but the problem is that NO requests are arriving.               │
│                                                                     │
│  Recommended Actions:                                               │
│    • IMMEDIATE: Check upstream services and load balancers         │
│    • CHECK: DNS resolution from multiple locations                 │
│    • VERIFY: Network connectivity and firewall rules               │
│    • CORRELATE: Check if other services are also affected          │
│                                                                     │
│  Example Scenario:                                                  │
│    Traffic: 5 req/s (normally 200 req/s) - 97.5% drop             │
│                                                                     │
│    At 2:15 PM, traffic suddenly drops from 200 req/s to 5 req/s.  │
│    Investigation reveals DNS TTL expired and DNS provider had      │
│    an outage, causing resolution failures.                         │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="error-patterns"><a class="header" href="#error-patterns">Error Patterns</a></h3>
<p>These patterns relate to error rate anomalies.</p>
<h4 id="error_rate_elevated"><a class="header" href="#error_rate_elevated"><code>error_rate_elevated</code></a></h4>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│  PATTERN: error_rate_elevated                                       │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  Conditions:                                                        │
│    request_rate: normal                                            │
│    application_latency: normal                                     │
│    error_rate: high                                                │
│                                                                     │
│  Severity: high                                                     │
│                                                                     │
│  What It Means:                                                     │
│    Error rate has increased significantly, but traffic and         │
│    latency are normal. This usually indicates a specific code      │
│    path or endpoint is failing, not a systemic issue.              │
│                                                                     │
│  Common Causes:                                                     │
│    • Bug in a specific endpoint                                    │
│    • External dependency failing for certain operations            │
│    • Bad input data causing validation failures                    │
│    • Feature flag enabled broken code                              │
│    • Database constraint violations                                │
│                                                                     │
│  Why Latency Is Normal:                                             │
│    Most requests succeed normally - only a subset is failing.      │
│    The average latency doesn't change much because successes       │
│    dominate the metrics.                                           │
│                                                                     │
│  Recommended Actions:                                               │
│    • INVESTIGATE: Check exception types in exception_context       │
│    • CHECK: Recent deployments or configuration changes            │
│    • IDENTIFY: Which endpoints/operations are failing              │
│    • VERIFY: External dependency health                            │
│                                                                     │
│  Example Scenario:                                                  │
│    Traffic: 150 req/s (normal)                                     │
│    Latency: 120ms (normal)                                         │
│    Errors: 3.5% (normally 0.1%)                                    │
│                                                                     │
│    Exception breakdown shows 85% of errors are                     │
│    "PaymentGatewayException" - the payment provider is having      │
│    issues, but only checkout flow is affected.                     │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h4 id="error_rate_critical"><a class="header" href="#error_rate_critical"><code>error_rate_critical</code></a></h4>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│  PATTERN: error_rate_critical                                       │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  Conditions:                                                        │
│    request_rate: normal                                            │
│    application_latency: normal                                     │
│    error_rate: very_high (&gt;5%)                                     │
│                                                                     │
│  Severity: critical                                                 │
│                                                                     │
│  What It Means:                                                     │
│    A significant portion of requests are failing. While the        │
│    service responds quickly (suggesting infrastructure is OK),     │
│    business logic is failing at a high rate.                       │
│                                                                     │
│  Common Causes:                                                     │
│    • Critical bug in recent deployment                             │
│    • Database schema mismatch after migration                      │
│    • External API contract changed                                 │
│    • Authentication/authorization failure                          │
│    • Missing configuration or secrets                              │
│                                                                     │
│  Recommended Actions:                                               │
│    • IMMEDIATE: Check recent deployments (rollback candidate)      │
│    • INVESTIGATE: Exception breakdown for root cause               │
│    • CHECK: Database connectivity and schema                       │
│    • VERIFY: All required configuration/secrets present            │
│                                                                     │
│  Example Scenario:                                                  │
│    Traffic: 150 req/s (normal)                                     │
│    Latency: 80ms (normal - fast errors)                           │
│    Errors: 25%                                                     │
│                                                                     │
│    Exception breakdown: 100% "DatabaseSchemaException"             │
│    Recent deployment added a required column, but migration        │
│    didn't run in production.                                       │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="latency-patterns"><a class="header" href="#latency-patterns">Latency Patterns</a></h3>
<p>These patterns relate to response time anomalies.</p>
<h4 id="latency_spike_recent"><a class="header" href="#latency_spike_recent"><code>latency_spike_recent</code></a></h4>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│  PATTERN: latency_spike_recent                                      │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  Conditions:                                                        │
│    request_rate: normal                                            │
│    application_latency: high                                       │
│    error_rate: normal                                              │
│                                                                     │
│  Severity: high                                                     │
│                                                                     │
│  What It Means:                                                     │
│    Latency increased without traffic change. Something changed     │
│    in the system - this is NOT a capacity issue (traffic is        │
│    normal). Likely a recent deployment, config change, or          │
│    dependency degradation.                                         │
│                                                                     │
│  Why Traffic Being Normal Matters:                                  │
│    If traffic were high, latency increase would make sense         │
│    (capacity). With normal traffic, the slowdown is internal.      │
│                                                                     │
│  Common Causes:                                                     │
│    • Recent deployment introduced inefficiency                     │
│    • Database query plan regression                                │
│    • New logging/tracing overhead                                  │
│    • Downstream service slower                                     │
│    • GC pressure from memory leak                                  │
│    • Missing cache (cold cache after restart)                      │
│                                                                     │
│  Recommended Actions:                                               │
│    • IMMEDIATE: Check deployments in last 2 hours                  │
│    • CHECK: Configuration changes (feature flags, settings)        │
│    • CHECK: External dependency response times                     │
│    • CHECK: Database query performance                             │
│    • CHECK: GC behavior and memory pressure                        │
│                                                                     │
│  Example Scenario:                                                  │
│    Traffic: 150 req/s (normal)                                     │
│    Latency: 450ms (normally 150ms)                                 │
│    Errors: 0.1% (normal)                                           │
│                                                                     │
│    Timeline shows latency jumped at 10:30 AM.                      │
│    Deployment log shows release v2.3.4 deployed at 10:28 AM.       │
│    New release added detailed audit logging that's synchronous.    │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h4 id="internal_latency_issue"><a class="header" href="#internal_latency_issue"><code>internal_latency_issue</code></a></h4>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│  PATTERN: internal_latency_issue                                    │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  Conditions:                                                        │
│    application_latency: high                                       │
│    dependency_latency: normal                                          │
│    database_latency: normal                                        │
│                                                                     │
│  Severity: high                                                     │
│                                                                     │
│  What It Means:                                                     │
│    The service itself is slow, but all its dependencies            │
│    (database, external services) are fast. The problem is          │
│    INTERNAL to the application code.                               │
│                                                                     │
│  Why This Is Significant:                                           │
│    Eliminates external causes. The fix must be in this service,    │
│    not upstream/downstream.                                        │
│                                                                     │
│  Common Causes:                                                     │
│    • CPU-intensive computation                                     │
│    • Memory allocation/GC pressure                                 │
│    • Lock contention / thread starvation                           │
│    • Inefficient algorithm (O(n²) loop)                           │
│    • Synchronous I/O blocking event loop                           │
│    • Missing async/await causing serialization                     │
│                                                                     │
│  Recommended Actions:                                               │
│    • PROFILE: CPU profiling to find hot paths                      │
│    • CHECK: Thread dumps for lock contention                       │
│    • CHECK: Memory usage and GC logs                               │
│    • REVIEW: Recent code changes for algorithmic issues            │
│                                                                     │
│  Example Scenario:                                                  │
│    App latency: 800ms                                              │
│    Client latency: 50ms (fast external calls)                      │
│    DB latency: 10ms (fast queries)                                 │
│                                                                     │
│    Where's the extra 740ms? Profiling reveals a nested loop        │
│    iterating over large collections - O(n²) complexity.            │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h4 id="database_bottleneck"><a class="header" href="#database_bottleneck"><code>database_bottleneck</code></a></h4>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│  PATTERN: database_bottleneck                                       │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  Conditions:                                                        │
│    database_latency: high                                          │
│    application_latency: high                                       │
│    db_latency_ratio: &gt; 0.7 (DB is &gt;70% of total latency)          │
│                                                                     │
│  Severity: high                                                     │
│                                                                     │
│  What It Means:                                                     │
│    Database operations dominate response time. The database        │
│    is the bottleneck - fixing app code won't help much.            │
│                                                                     │
│  Why The Ratio Matters:                                             │
│    If DB is 15ms out of 500ms total, DB isn't the problem.         │
│    If DB is 400ms out of 500ms total, DB IS the problem.           │
│                                                                     │
│  Common Causes:                                                     │
│    • Missing database index                                        │
│    • Query plan regression (statistics stale)                      │
│    • Lock contention in database                                   │
│    • Database server under-provisioned                             │
│    • N+1 query pattern                                             │
│    • Full table scans                                              │
│                                                                     │
│  Recommended Actions:                                               │
│    • INVESTIGATE: Slow query logs                                  │
│    • CHECK: Missing indexes on frequently queried columns          │
│    • CHECK: Database CPU and I/O metrics                           │
│    • ANALYZE: Query execution plans                                │
│    • CONSIDER: Query caching or read replicas                      │
│                                                                     │
│  Example Scenario:                                                  │
│    App latency: 650ms                                              │
│    DB latency: 520ms (80% of total)                                │
│    Client latency: 10ms                                            │
│                                                                     │
│    Slow query log shows: SELECT * FROM orders WHERE                │
│    customer_id = ? taking 500ms. Missing index on customer_id.     │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h4 id="downstream_cascade"><a class="header" href="#downstream_cascade"><code>downstream_cascade</code></a></h4>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│  PATTERN: downstream_cascade                                        │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  Conditions:                                                        │
│    dependency_latency: high                                            │
│    application_latency: high                                       │
│    dependency_latency_ratio: &gt; 0.6 (external calls &gt;60% of total)      │
│                                                                     │
│  Severity: high                                                     │
│                                                                     │
│  What It Means:                                                     │
│    External dependency calls are dominating response time.         │
│    The service you're looking at is slow BECAUSE something         │
│    it depends on is slow.                                          │
│                                                                     │
│  Why This Pattern Exists:                                           │
│    Helps you avoid blaming the wrong service. If booking           │
│    is slow because search is slow, fix search, not booking.        │
│                                                                     │
│  What To Look At:                                                   │
│    • service_graph_context shows downstream call breakdown         │
│    • Identify which downstream service is slowest                  │
│    • That service is likely the root cause                         │
│                                                                     │
│  Recommended Actions:                                               │
│    • FOCUS: Investigate the slow downstream service                │
│    • CHECK: service_graph_context for detailed breakdown           │
│    • CORRELATE: Does downstream service also have anomalies?       │
│    • CONSIDER: Circuit breaker to fail fast                        │
│                                                                     │
│  Example Scenario:                                                  │
│    App latency: 1200ms                                             │
│    Client latency: 950ms (79% of total)                            │
│    DB latency: 15ms                                                │
│                                                                     │
│    service_graph_context shows:                                    │
│      search-api: 850ms avg (highest)                               │
│      payment-api: 50ms avg                                         │
│      inventory-api: 40ms avg                                       │
│                                                                     │
│    Root cause: search-api is slow, causing booking to be slow.     │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="fast-fail-patterns"><a class="header" href="#fast-fail-patterns">Fast-Fail Patterns</a></h3>
<p>These patterns indicate requests failing before normal processing.</p>
<h4 id="fast_rejection"><a class="header" href="#fast_rejection"><code>fast_rejection</code></a></h4>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│  PATTERN: fast_rejection                                            │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  Conditions:                                                        │
│    application_latency: very_low                                   │
│    error_rate: very_high                                           │
│                                                                     │
│  Severity: critical                                                 │
│                                                                     │
│  What It Means:                                                     │
│    Requests are failing VERY fast. They're being rejected          │
│    before reaching normal application logic. Something is          │
│    blocking requests at the entry point.                           │
│                                                                     │
│  Why Low Latency + High Errors Is Bad:                              │
│    Normal errors take time (processing, then failure).             │
│    Fast errors mean immediate rejection - no processing at all.    │
│                                                                     │
│  Common Causes:                                                     │
│    • Circuit breaker OPEN (rejecting all requests)                 │
│    • Rate limiter active (over quota)                              │
│    • Authentication/authorization failure                          │
│    • Invalid API key or expired token                              │
│    • TLS certificate mismatch                                      │
│    • Connection pool exhausted (immediate rejection)               │
│                                                                     │
│  Recommended Actions:                                               │
│    • IMMEDIATE: Check circuit breaker status                       │
│    • CHECK: Rate limiter configuration and current rate            │
│    • CHECK: Authentication service health                          │
│    • VERIFY: API keys and certificates are valid                   │
│                                                                     │
│  Example Scenario:                                                  │
│    Latency: 5ms (normally 150ms)                                   │
│    Errors: 95%                                                     │
│                                                                     │
│    5ms is just enough time to check auth and reject.               │
│    Auth service is down, causing all requests to fail at           │
│    the middleware layer before reaching business logic.            │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h4 id="fast_failure"><a class="header" href="#fast_failure"><code>fast_failure</code></a></h4>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│  PATTERN: fast_failure                                              │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  Conditions:                                                        │
│    application_latency: low                                        │
│    error_rate: high                                                │
│                                                                     │
│  Severity: critical                                                 │
│                                                                     │
│  What It Means:                                                     │
│    Requests are failing quickly - faster than normal processing    │
│    would take. Similar to fast_rejection but with slightly         │
│    more processing time (maybe some validation runs).              │
│                                                                     │
│  The Latency-Error Relationship:                                    │
│    Normal request: Validate → Process → DB → External → Return     │
│    Fast failure: Validate → Fail (skip processing)                 │
│                                                                     │
│  Common Causes:                                                     │
│    • Early validation failure                                      │
│    • Feature flag disabled critical path                           │
│    • Missing required configuration                                │
│    • Database connection failure (fail after connect attempt)      │
│                                                                     │
│  Recommended Actions:                                               │
│    • CHECK: Application logs for error patterns                    │
│    • CHECK: Required configuration and feature flags               │
│    • CHECK: Database and cache connectivity                        │
│    • REVIEW: Recent deployments for breaking changes               │
│                                                                     │
│  Example Scenario:                                                  │
│    Latency: 25ms (normally 200ms)                                  │
│    Errors: 80%                                                     │
│                                                                     │
│    Logs show: "Redis connection failed"                            │
│    Redis cluster is unreachable. Service tries to connect          │
│    (25ms timeout), fails, returns error.                           │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="cascade-patterns"><a class="header" href="#cascade-patterns">Cascade Patterns</a></h3>
<p>These patterns relate to dependency failures propagating.</p>
<h4 id="upstream_cascade"><a class="header" href="#upstream_cascade"><code>upstream_cascade</code></a></h4>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│  PATTERN: upstream_cascade                                          │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  Conditions:                                                        │
│    application_latency: high                                       │
│    _dependency_context: upstream_anomaly                           │
│                                                                     │
│  Severity: high                                                     │
│                                                                     │
│  What It Means:                                                     │
│    This service is slow, AND at least one of its upstream          │
│    dependencies also has an anomaly. The root cause is likely      │
│    in the upstream service, not here.                              │
│                                                                     │
│  How Cascade Detection Works:                                       │
│    1. Yaga2 runs detection for all services (Pass 1)               │
│    2. For services with latency anomalies, check dependencies      │
│    3. If a dependency has an anomaly, it's a cascade               │
│    4. Trace the chain to find the root cause service               │
│                                                                     │
│  Why This Matters:                                                  │
│    Without cascade detection, you'd get 5 alerts for 5 services    │
│    when only 1 service (the root cause) needs fixing.              │
│                                                                     │
│  Example Dependency Chain:                                          │
│    mobile-api → booking → vms → titan                              │
│                                                                     │
│    If titan has a database issue, all 4 services will be slow.     │
│    Cascade detection identifies titan as root cause.               │
│                                                                     │
│  cascade_analysis Output:                                           │
│    {                                                                │
│      "is_cascade": true,                                           │
│      "root_cause_service": "titan",                                │
│      "affected_chain": ["titan", "vms", "booking", "mobile-api"],  │
│      "cascade_type": "upstream_cascade"                            │
│    }                                                                │
│                                                                     │
│  Recommended Actions:                                               │
│    • FOCUS: Investigate root_cause_service first                   │
│    • IGNORE: Don't fix downstream services (they'll recover)       │
│    • CORRELATE: Check root cause service's anomaly for details     │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h2 id="pattern-matching-in-action"><a class="header" href="#pattern-matching-in-action">Pattern Matching in Action</a></h2>
<h3 id="complete-example-traffic-surge"><a class="header" href="#complete-example-traffic-surge">Complete Example: Traffic Surge</a></h3>
<pre><code>Input Metrics at 2:30 PM:
─────────────────────────────────────────────────────────────
  request_rate: 850 req/s (baseline: 200 req/s)
  application_latency: 420ms (baseline: 120ms)
  error_rate: 0.5% (baseline: 0.1%)
  database_latency: 15ms (baseline: 12ms)
  dependency_latency: 45ms (baseline: 40ms)

Phase 1: Isolation Forest
─────────────────────────────────────────────────────────────
  request_rate:        score=-0.42  percentile=96  → very_high
  application_latency: score=-0.35  percentile=94  → high
  error_rate:          score=-0.08  percentile=72  → normal
  database_latency:    score=0.05   percentile=55  → normal
  dependency_latency:      score=0.02   percentile=52  → normal

Phase 2: Level Classification
─────────────────────────────────────────────────────────────
  request_rate: very_high (&gt;95th percentile)
  application_latency: high (90-95th percentile)
  error_rate: normal (errors not anomalous)
  database_latency: normal (not flagged by IF)
  dependency_latency: normal (not flagged by IF)

Phase 3: Pattern Matching
─────────────────────────────────────────────────────────────
  Checking: traffic_surge_failing
    Conditions: request_rate=high ✓, latency=high ✓, errors=high ✗
    Result: NO MATCH (errors are normal)

  Checking: traffic_surge_degrading
    Conditions: request_rate=high ✓, latency=high ✓, errors=normal ✓
    Result: MATCH ✓

Output:
─────────────────────────────────────────────────────────────
{
  "pattern_name": "traffic_surge_degrading",
  "severity": "high",
  "description": "Traffic surge causing slowdown: 850 req/s driving
    latency to 420ms (errors stable at 0.50%)",
  "interpretation": "Service is slowing under load but not failing -
    approaching capacity. Users experiencing degraded performance but
    requests are completing.",
  "recommended_actions": [
    "IMMEDIATE: Scale horizontally if possible",
    "CHECK: Resource bottlenecks (CPU, memory, connections)",
    "MONITOR: Error rate for signs of impending failure",
    "CONSIDER: Enable request throttling to protect backend"
  ],
  "metric_levels": {
    "request_rate": "very_high",
    "application_latency": "high",
    "error_rate": "normal"
  }
}
</code></pre>
<h2 id="adding-new-patterns"><a class="header" href="#adding-new-patterns">Adding New Patterns</a></h2>
<p>To add a new pattern, edit <code>smartbox_anomaly/detection/interpretations.py</code>:</p>
<pre><code class="language-python">MULTIVARIATE_PATTERNS["cache_miss_storm"] = PatternDefinition(
    name="cache_miss_storm",
    conditions={
        "request_rate": "normal",
        "database_latency": "high",
        "db_latency_ratio": "&gt; 0.8",
    },
    severity="high",
    message_template=(
        "Cache miss storm: {database_latency:.0f}ms DB latency "
        "({db_latency_ratio:.0%} of total response time)"
    ),
    interpretation=(
        "Database is handling requests that should be cached. "
        "Cache service may be down or TTL expired for hot keys."
    ),
    recommended_actions=[
        "CHECK: Cache service health (Redis/Memcached)",
        "CHECK: Cache hit rate metrics",
        "VERIFY: Cache TTL hasn't expired for hot keys",
        "CONSIDER: Implementing cache warming",
    ],
)
</code></pre>
<h3 id="pattern-conditions"><a class="header" href="#pattern-conditions">Pattern Conditions</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Condition Type</th><th>Example</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>Level</td><td><code>"request_rate": "high"</code></td><td>Metric must be at this level</td></tr>
<tr><td>Ratio</td><td><code>"db_latency_ratio": "&gt; 0.7"</code></td><td>Ratio must exceed threshold</td></tr>
<tr><td>Any</td><td><code>"error_rate": "any"</code></td><td>Matches any level</td></tr>
<tr><td>Dependency</td><td><code>"_dependency_context": "upstream_anomaly"</code></td><td>Cascade detection</td></tr>
</tbody>
</table>
</div>
<h3 id="recommendation-prefixes"><a class="header" href="#recommendation-prefixes">Recommendation Prefixes</a></h3>
<p>Use these prefixes for consistent action ordering:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Prefix</th><th>Priority</th><th>Use For</th></tr>
</thead>
<tbody>
<tr><td><code>IMMEDIATE</code></td><td>1</td><td>Actions needed right now</td></tr>
<tr><td><code>CHECK</code></td><td>2</td><td>Things to investigate</td></tr>
<tr><td><code>INVESTIGATE</code></td><td>3</td><td>Deeper analysis needed</td></tr>
<tr><td><code>MONITOR</code></td><td>4</td><td>Ongoing observation</td></tr>
<tr><td><code>CONSIDER</code></td><td>5</td><td>Optional improvements</td></tr>
</tbody>
</table>
</div>
<h2 id="further-reading-1"><a class="header" href="#further-reading-1">Further Reading</a></h2>
<ul>
<li><a href="#isolation-forest-ml-detection">Isolation Forest</a> - How ML detection works</li>
<li><a href="#detection-pipeline">Detection Pipeline</a> - End-to-end detection flow</li>
<li><a href="#api-payload-reference">API Payload Reference</a> - Output format</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="detection-pipeline"><a class="header" href="#detection-pipeline">Detection Pipeline</a></h1>
<p>The complete detection pipeline from metrics collection to anomaly output.</p>
<h2 id="pipeline-phases"><a class="header" href="#pipeline-phases">Pipeline Phases</a></h2>
<h3 id="phase-1-metrics-collection"><a class="header" href="#phase-1-metrics-collection">Phase 1: Metrics Collection</a></h3>
<pre><code>VictoriaMetrics
      │
      ▼
┌─────────────────────────────┐
│ Collect 5 core metrics      │
│ - request_rate              │
│ - application_latency       │
│ - dependency_latency       │
│ - database_latency         │
│ - error_rate               │
└─────────────────────────────┘
</code></pre>
<h3 id="phase-2-input-validation"><a class="header" href="#phase-2-input-validation">Phase 2: Input Validation</a></h3>
<p>Metrics are validated before processing:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Check</th><th>Action</th></tr>
</thead>
<tbody>
<tr><td>NaN/Inf values</td><td>Replaced with 0.0</td></tr>
<tr><td>Negative rates</td><td>Capped at 0.0</td></tr>
<tr><td>Negative latencies</td><td>Capped at 0.0</td></tr>
<tr><td>Extreme latencies (&gt;5 min)</td><td>Capped at 300,000ms</td></tr>
<tr><td>Extreme request rates (&gt;1M/s)</td><td>Capped at 1,000,000</td></tr>
<tr><td>Error rates &gt; 100%</td><td>Capped at 1.0</td></tr>
</tbody>
</table>
</div>
<p>Validation warnings are included in the output.</p>
<h3 id="phase-3-pass-1-detection"><a class="header" href="#phase-3-pass-1-detection">Phase 3: Pass 1 Detection</a></h3>
<p>First pass detects anomalies without dependency context:</p>
<pre><code>For each service:
    1. Load time-aware model (based on current time period)
    2. Run univariate IF on each metric
    3. Run multivariate IF on combined metrics
    4. Collect anomaly signals
    5. Match signals against patterns
    6. Store result
</code></pre>
<h3 id="phase-4-pass-2-detection-cascade-analysis"><a class="header" href="#phase-4-pass-2-detection-cascade-analysis">Phase 4: Pass 2 Detection (Cascade Analysis)</a></h3>
<p>Second pass re-analyzes services with latency anomalies:</p>
<pre><code>For each service with latency anomaly:
    1. Build dependency context from Pass 1 results
    2. Check upstream dependencies for anomalies
    3. Re-run pattern matching with dependency info
    4. Add cascade_analysis if root cause found
</code></pre>
<p><strong>Dependency Context</strong>:</p>
<pre><code class="language-json">{
  "upstream_status": "anomaly",
  "dependencies": {
    "vms": {"has_anomaly": true, "type": "database_bottleneck"},
    "titan": {"has_anomaly": true, "type": "latency_elevated"}
  },
  "root_cause_service": "titan"
}
</code></pre>
<h3 id="phase-5-slo-evaluation"><a class="header" href="#phase-5-slo-evaluation">Phase 5: SLO Evaluation</a></h3>
<p>Each anomaly is evaluated against SLO thresholds:</p>
<pre><code>For each detected anomaly:
    1. Evaluate latency vs SLO
    2. Evaluate error rate vs SLO
    3. Evaluate database latency vs baseline
    4. Check request rate (surge/cliff)
    5. Determine combined SLO status
    6. Adjust severity if needed
</code></pre>
<p>See <a href="slo/README.html">SLO Evaluation</a> for details.</p>
<h3 id="phase-6-incident-lifecycle"><a class="header" href="#phase-6-incident-lifecycle">Phase 6: Incident Lifecycle</a></h3>
<p>Anomalies enter the incident lifecycle:</p>
<pre><code>anomaly detected
      │
      ▼
┌─────────────────────────────┐
│ Check existing incidents    │
│ - Same fingerprint exists?  │
│ - Status of that incident?  │
└─────────────────────────────┘
      │
      ├─── No existing ──▶ CREATE (SUSPECTED)
      │
      ├─── SUSPECTED ──▶ Increment consecutive_detections
      │                   If &gt;= 2: transition to OPEN
      │
      ├─── OPEN ──▶ CONTINUE (update occurrence_count)
      │
      └─── RECOVERING ──▶ Return to OPEN
</code></pre>
<p>See <a href="incidents/README.html">Incident Lifecycle</a> for details.</p>
<h2 id="output-structure"><a class="header" href="#output-structure">Output Structure</a></h2>
<p>Final output for each service:</p>
<pre><code class="language-json">{
  "alert_type": "anomaly_detected",
  "service_name": "booking",
  "timestamp": "2024-01-15T10:30:00",
  "time_period": "business_hours",
  "overall_severity": "high",
  "anomaly_count": 1,

  "anomalies": {
    "latency_spike_recent": {
      "type": "consolidated",
      "severity": "high",
      "detection_signals": [...],
      "cascade_analysis": {...}
    }
  },

  "current_metrics": {...},
  "slo_evaluation": {...},
  "fingerprinting": {...}
}
</code></pre>
<h2 id="error-handling"><a class="header" href="#error-handling">Error Handling</a></h2>
<h3 id="metrics-unavailable"><a class="header" href="#metrics-unavailable">Metrics Unavailable</a></h3>
<p>When VictoriaMetrics is unreachable:</p>
<pre><code class="language-json">{
  "alert_type": "metrics_unavailable",
  "service_name": "booking",
  "error": "Metrics collection failed",
  "failed_metrics": ["request_rate", "application_latency"],
  "skipped_reason": "critical_metrics_unavailable"
}
</code></pre>
<p>Detection is skipped to prevent false alerts from missing data.</p>
<h3 id="model-not-found"><a class="header" href="#model-not-found">Model Not Found</a></h3>
<p>When no trained model exists:</p>
<pre><code class="language-json">{
  "alert_type": "error",
  "service_name": "new-service",
  "error_message": "No trained model found for time period"
}
</code></pre>
<p>Run training to create models for new services.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="slo-evaluation-layer"><a class="header" href="#slo-evaluation-layer">SLO Evaluation Layer</a></h1>
<p>The SLO (Service Level Objective) evaluation layer adjusts anomaly severity based on operational thresholds. This chapter explains what SLOs are, why they matter for anomaly detection, and how Yaga2 uses them.</p>
<h2 id="what-is-an-slo-1"><a class="header" href="#what-is-an-slo-1">What is an SLO?</a></h2>
<p><strong>SLO (Service Level Objective)</strong> is a target level of service reliability that you promise to maintain. It’s the answer to: <em>“What does ‘good enough’ look like for this service?”</em></p>
<h3 id="the-sli--slo--sla-hierarchy"><a class="header" href="#the-sli--slo--sla-hierarchy">The SLI → SLO → SLA Hierarchy</a></h3>
<p>Understanding SLOs requires understanding the full hierarchy:</p>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│                    Service Level Hierarchy                          │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│   ┌───────────────────────────────────────────────────────────┐     │
│   │                        SLA                                │     │
│   │              Service Level Agreement                      │     │
│   │                                                           │     │
│   │   "If we breach this, there are consequences"             │     │
│   │   Example: 99.9% uptime or customer gets credit           │     │
│   │                                                           │     │
│   │   ┌───────────────────────────────────────────────────┐   │     │
│   │   │                      SLO                          │   │     │
│   │   │            Service Level Objective                │   │     │
│   │   │                                                   │   │     │
│   │   │   "This is our internal target"                   │   │     │
│   │   │   Example: 99.95% uptime (stricter than SLA)      │   │     │
│   │   │                                                   │   │     │
│   │   │   ┌───────────────────────────────────────────┐   │   │     │
│   │   │   │                  SLI                      │   │   │     │
│   │   │   │        Service Level Indicator            │   │   │     │
│   │   │   │                                           │   │   │     │
│   │   │   │   "This is what we measure"               │   │   │     │
│   │   │   │   Example: Request success rate           │   │   │     │
│   │   │   │                                           │   │   │     │
│   │   │   └───────────────────────────────────────────┘   │   │     │
│   │   └───────────────────────────────────────────────────┘   │     │
│   └───────────────────────────────────────────────────────────┘     │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Term</th><th>Definition</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><strong>SLI</strong> (Indicator)</td><td>The metric you measure</td><td>Request latency in milliseconds</td></tr>
<tr><td><strong>SLO</strong> (Objective)</td><td>The target value for that metric</td><td>99% of requests &lt; 500ms</td></tr>
<tr><td><strong>SLA</strong> (Agreement)</td><td>Legal/business commitment with consequences</td><td>99.9% uptime or refund</td></tr>
</tbody>
</table>
</div>
<h3 id="common-slo-types"><a class="header" href="#common-slo-types">Common SLO Types</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>SLO Type</th><th>What It Measures</th><th>Example Target</th></tr>
</thead>
<tbody>
<tr><td><strong>Availability</strong></td><td>Is the service responding?</td><td>99.9% of requests succeed</td></tr>
<tr><td><strong>Latency</strong></td><td>How fast does it respond?</td><td>95% of requests &lt; 300ms</td></tr>
<tr><td><strong>Error Rate</strong></td><td>How many requests fail?</td><td>&lt; 0.1% error rate</td></tr>
<tr><td><strong>Throughput</strong></td><td>How much can it handle?</td><td>1000 req/s capacity</td></tr>
</tbody>
</table>
</div>
<h3 id="why-slos-matter"><a class="header" href="#why-slos-matter">Why SLOs Matter</a></h3>
<p>Without SLOs, you have two problems:</p>
<p><strong>Problem 1: Over-alerting</strong></p>
<pre><code>❌ Static threshold: Alert if latency &gt; 200ms
   ↓
   You get 50 alerts per day for "normal" spikes
   ↓
   Alert fatigue → Team ignores alerts
   ↓
   Real incidents get missed
</code></pre>
<p><strong>Problem 2: Under-alerting</strong></p>
<pre><code>❌ ML-only detection: Alert when "unusual"
   ↓
   Latency at 400ms is unusual (normally 100ms)
   ↓
   But users don't notice 400ms latency
   ↓
   Alert noise for non-issues
</code></pre>
<p><strong>The SLO Solution:</strong></p>
<pre><code>✅ SLO-aware detection:
   ↓
   ML detects 400ms latency as unusual
   ↓
   SLO evaluation: 400ms &lt; 500ms acceptable threshold
   ↓
   Severity adjusted to "low" (no page, just log)
   ↓
   Team focuses on real problems
</code></pre>
<h2 id="why-slos-matter-for-anomaly-detection"><a class="header" href="#why-slos-matter-for-anomaly-detection">Why SLOs Matter for Anomaly Detection</a></h2>
<p>ML-based anomaly detection answers: <strong>“Is this unusual?”</strong></p>
<p>But unusual doesn’t mean bad:</p>
<ul>
<li>A traffic spike during a sale is unusual but good</li>
<li>Latency at 300ms when it’s usually 100ms is unusual but might be acceptable</li>
<li>Error rate dropping to 0% is unusual but definitely not a problem</li>
</ul>
<p>SLO evaluation answers a different question: <strong>“Does this impact users?”</strong></p>
<h3 id="the-two-question-model"><a class="header" href="#the-two-question-model">The Two-Question Model</a></h3>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│                    Anomaly Evaluation                               │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│   Question 1: Is this unusual?                                      │
│   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━                                      │
│   Answered by: Machine Learning (Isolation Forest)                  │
│   Method: Compare against historical baseline                       │
│   Output: "Yes, this is 2σ above normal"                           │
│                                                                     │
│                         │                                           │
│                         ▼                                           │
│                                                                     │
│   Question 2: Does it matter operationally?                         │
│   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━                          │
│   Answered by: SLO Evaluation                                       │
│   Method: Compare against operational thresholds                    │
│   Output: "No, still within acceptable SLO"                        │
│                                                                     │
│                         │                                           │
│                         ▼                                           │
│                                                                     │
│   Final Decision                                                    │
│   ━━━━━━━━━━━━━━━                                                   │
│   Unusual: ✓ Yes                                                    │
│   Impactful: ✗ No                                                   │
│   Action: Log for awareness, don't page                            │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="example-the-280ms-latency-case"><a class="header" href="#example-the-280ms-latency-case">Example: The 280ms Latency Case</a></h3>
<p>Consider a booking service:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Metric</th><th>Value</th></tr>
</thead>
<tbody>
<tr><td>Current latency</td><td>280ms</td></tr>
<tr><td>Historical baseline</td><td>150ms</td></tr>
<tr><td>Statistical deviation</td><td>+2.3σ (p91 percentile)</td></tr>
</tbody>
</table>
</div>
<p><strong>ML-only approach:</strong></p>
<pre><code>ML says: "280ms is unusual! That's 2.3σ above normal!"
Result: HIGH severity alert
Problem: Engineer pages at 3 AM for non-issue
</code></pre>
<p><strong>SLO-aware approach:</strong></p>
<pre><code>ML says: "280ms is unusual"
SLO says: "280ms &lt; 300ms acceptable threshold"
SLO says: "280ms is 93% toward the limit (proximity = 0.93)"
Result: LOW severity (logged but no page)
Benefit: Engineer sleeps, system still tracks the deviation
</code></pre>
<h2 id="how-yaga2s-slo-layer-works"><a class="header" href="#how-yaga2s-slo-layer-works">How Yaga2’s SLO Layer Works</a></h2>
<h3 id="evaluation-flow"><a class="header" href="#evaluation-flow">Evaluation Flow</a></h3>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│                    SLO Evaluation Pipeline                          │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│   Input: ML Detection Result                                        │
│   ┌────────────────────────────────────┐                           │
│   │ pattern: latency_spike_recent      │                           │
│   │ severity: high                     │                           │
│   │ score: -0.35                       │                           │
│   │ metrics:                           │                           │
│   │   latency: 280ms                   │                           │
│   │   error_rate: 0.1%                 │                           │
│   │   request_rate: 150/s              │                           │
│   └─────────────────┬──────────────────┘                           │
│                     │                                               │
│                     ▼                                               │
│   ┌─────────────────────────────────────────────────────────────┐   │
│   │              Evaluate Each SLO Component                    │   │
│   ├──────────────┬──────────────┬─────────────┬────────────────┤   │
│   │   Latency    │  Error Rate  │  DB Latency │   Traffic      │   │
│   │   280ms      │   0.1%       │  2.5ms      │   150/s        │   │
│   │   vs 300ms   │   vs 0.5%    │  vs 5ms     │   vs 50/s      │   │
│   │   ↓          │   ↓          │   ↓         │   ↓            │   │
│   │   ok (93%)   │   ok (20%)   │  ok (below  │   ok (3x but   │   │
│   │              │              │     floor)  │   no impact)   │   │
│   └──────┬───────┴──────┬───────┴──────┬──────┴───────┬────────┘   │
│          │              │              │              │             │
│          └──────────────┴──────────────┴──────────────┘             │
│                         │                                           │
│                         ▼                                           │
│              ┌─────────────────────┐                               │
│              │  Combined Status    │                               │
│              │  ─────────────────  │                               │
│              │  Status: ok         │                               │
│              │  Proximity: 0.93    │  (closest to breach)          │
│              │  Impact: none       │                               │
│              └──────────┬──────────┘                               │
│                         │                                           │
│                         ▼                                           │
│              ┌─────────────────────┐                               │
│              │  Severity Adjust    │                               │
│              │  ─────────────────  │                               │
│              │  Original: high     │                               │
│              │  Adjusted: low      │  (SLO ok → low)               │
│              │  Changed: true      │                               │
│              └─────────────────────┘                               │
│                                                                     │
│   Output: Adjusted Detection Result                                 │
│   ┌────────────────────────────────────┐                           │
│   │ pattern: latency_spike_recent      │                           │
│   │ severity: low                      │  ← adjusted               │
│   │ original_severity: high            │  ← preserved              │
│   │ slo_status: ok                     │                           │
│   │ slo_proximity: 0.93                │                           │
│   │ explanation: "Within acceptable    │                           │
│   │   SLO thresholds"                  │                           │
│   └────────────────────────────────────┘                           │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="slo-components-in-yaga2"><a class="header" href="#slo-components-in-yaga2">SLO Components in Yaga2</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Component</th><th>Threshold Type</th><th>Evaluation Method</th></tr>
</thead>
<tbody>
<tr><td><a href="#latency-evaluation">Latency</a></td><td>Absolute (ms)</td><td>Compare against acceptable/warning/critical thresholds</td></tr>
<tr><td><a href="#error-rate-evaluation">Error Rate</a></td><td>Absolute (%)</td><td>Compare against acceptable/warning/critical with floor suppression</td></tr>
<tr><td><a href="#database-latency-evaluation">Database Latency</a></td><td>Ratio-based</td><td>Compare ratio against training baseline (1.5x, 2x, 3x, 5x)</td></tr>
<tr><td><a href="#request-rate-evaluation-surgecliff">Request Rate</a></td><td>Ratio-based</td><td>Detect surge (≥200%) or cliff (≤50%) with correlation</td></tr>
</tbody>
</table>
</div>
<h3 id="slo-status"><a class="header" href="#slo-status">SLO Status</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Status</th><th>What It Means</th><th>Severity Impact</th></tr>
</thead>
<tbody>
<tr><td><code>ok</code></td><td>All metrics within acceptable limits</td><td>Severity → <code>low</code></td></tr>
<tr><td><code>elevated</code></td><td>Above acceptable, below warning</td><td>Severity stays as-is</td></tr>
<tr><td><code>warning</code></td><td>Approaching SLO breach</td><td>Severity → <code>high</code></td></tr>
<tr><td><code>breached</code></td><td>SLO threshold exceeded</td><td>Severity → <code>critical</code></td></tr>
</tbody>
</table>
</div>
<h3 id="severity-adjustment-rules"><a class="header" href="#severity-adjustment-rules">Severity Adjustment Rules</a></h3>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│                    Severity Adjustment Matrix                       │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│   Original Severity    SLO Status      Final Severity               │
│   ─────────────────    ──────────      ──────────────               │
│                                                                     │
│   critical             ok              → low                        │
│   critical             elevated        → high                       │
│   critical             warning         → critical                   │
│   critical             breached        → critical                   │
│                                                                     │
│   high                 ok              → low                        │
│   high                 elevated        → high                       │
│   high                 warning         → high                       │
│   high                 breached        → critical                   │
│                                                                     │
│   medium               ok              → low                        │
│   medium               warning         → high                       │
│                                                                     │
│   low                  any             → low                        │
│                                                                     │
│   (any)                breached        → critical (always)          │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<p><strong>Key Insight:</strong> When SLO status is <code>ok</code>, severity is <strong>always</strong> adjusted to <code>low</code>, regardless of the original ML-assigned severity. This ensures alerts reflect operational impact, not just statistical deviation.</p>
<h2 id="practical-examples"><a class="header" href="#practical-examples">Practical Examples</a></h2>
<h3 id="example-1-acceptable-anomaly"><a class="header" href="#example-1-acceptable-anomaly">Example 1: Acceptable Anomaly</a></h3>
<pre><code>Service: booking
─────────────────────────────────────────────────────────────

Current Metrics:
  latency: 420ms (usually 200ms)
  error_rate: 0.2%

ML Detection:
  Pattern: latency_spike_recent
  Severity: high
  Reason: Latency is 3σ above baseline

SLO Evaluation:
  Latency: 420ms &lt; 500ms acceptable ✓
  Errors: 0.2% &lt; 0.5% acceptable ✓
  Status: ok

Final Decision:
  Severity: low
  Action: Log for awareness, no page
  Message: "Anomaly detected but within SLO"
</code></pre>
<h3 id="example-2-warning-zone"><a class="header" href="#example-2-warning-zone">Example 2: Warning Zone</a></h3>
<pre><code>Service: search
─────────────────────────────────────────────────────────────

Current Metrics:
  latency: 750ms (acceptable: 500ms, warning: 800ms)
  error_rate: 0.8% (acceptable: 0.5%, warning: 1%)

ML Detection:
  Pattern: latency_spike_recent
  Severity: high

SLO Evaluation:
  Latency: 750ms &gt; 500ms acceptable, &lt; 800ms warning
  Errors: 0.8% &gt; 0.5% acceptable, &lt; 1% warning
  Status: warning
  Proximity: 0.94 (close to warning threshold)

Final Decision:
  Severity: high (maintained)
  Action: Alert on-call for investigation
  Message: "Approaching SLO limits"
</code></pre>
<h3 id="example-3-slo-breach"><a class="header" href="#example-3-slo-breach">Example 3: SLO Breach</a></h3>
<pre><code>Service: checkout
─────────────────────────────────────────────────────────────

Current Metrics:
  latency: 2500ms (critical: 1000ms)
  error_rate: 5%

ML Detection:
  Pattern: traffic_surge_failing
  Severity: critical

SLO Evaluation:
  Latency: 2500ms &gt; 1000ms critical ✗
  Errors: 5% &gt; 2% critical ✗
  Status: breached

Final Decision:
  Severity: critical (confirmed)
  Action: Immediate page, potential incident
  Message: "SLO breached - users impacted"
</code></pre>
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<h3 id="default-slo-thresholds"><a class="header" href="#default-slo-thresholds">Default SLO Thresholds</a></h3>
<pre><code class="language-json">{
  "slos": {
    "enabled": true,
    "allow_downgrade_to_informational": true,
    "require_slo_breach_for_critical": true,
    "defaults": {
      "latency_acceptable_ms": 500,
      "latency_warning_ms": 800,
      "latency_critical_ms": 1000,
      "error_rate_acceptable": 0.005,
      "error_rate_warning": 0.01,
      "error_rate_critical": 0.02,
      "error_rate_floor": 0.001,
      "database_latency_floor_ms": 5.0,
      "database_latency_ratios": {
        "info": 1.5,
        "warning": 2.0,
        "high": 3.0,
        "critical": 5.0
      }
    }
  }
}
</code></pre>
<h3 id="per-service-overrides"><a class="header" href="#per-service-overrides">Per-Service Overrides</a></h3>
<p>Different services have different requirements:</p>
<pre><code class="language-json">{
  "slos": {
    "services": {
      "checkout": {
        "latency_acceptable_ms": 300,
        "latency_critical_ms": 500,
        "error_rate_acceptable": 0.001
      },
      "search": {
        "latency_acceptable_ms": 200,
        "latency_critical_ms": 400
      },
      "admin-panel": {
        "latency_acceptable_ms": 2000,
        "error_rate_acceptable": 0.01
      }
    }
  }
}
</code></pre>
<h3 id="busy-period-configuration"><a class="header" href="#busy-period-configuration">Busy Period Configuration</a></h3>
<p>During high-traffic periods (holidays, sales), thresholds can be automatically relaxed:</p>
<pre><code class="language-json">{
  "slos": {
    "defaults": {
      "busy_period_factor": 1.5
    },
    "busy_periods": [
      {
        "start": "2024-12-20T00:00:00",
        "end": "2025-01-05T23:59:59"
      }
    ]
  }
}
</code></pre>
<p>During busy periods:</p>
<ul>
<li>500ms acceptable → 750ms acceptable</li>
<li>0.5% error acceptable → 0.75% error acceptable</li>
</ul>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<h3 id="1-set-slos-based-on-user-experience"><a class="header" href="#1-set-slos-based-on-user-experience">1. Set SLOs Based on User Experience</a></h3>
<pre><code>❌ Bad: "Let's set latency SLO at 100ms because it sounds good"
   Problem: Your P50 latency is already 150ms

✓ Good: "Users report issues above 500ms, let's set acceptable at 400ms"
   Benefit: SLO reflects actual user impact
</code></pre>
<h3 id="2-use-the-error-budget-concept"><a class="header" href="#2-use-the-error-budget-concept">2. Use the Error Budget Concept</a></h3>
<p>If your SLO is 99.9% availability:</p>
<ul>
<li>Error budget = 0.1% of requests can fail</li>
<li>Monthly budget ≈ 43 minutes of downtime</li>
<li>Use this to decide when to deploy vs stabilize</li>
</ul>
<h3 id="3-have-different-slos-for-different-services"><a class="header" href="#3-have-different-slos-for-different-services">3. Have Different SLOs for Different Services</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Service</th><th>Latency SLO</th><th>Why</th></tr>
</thead>
<tbody>
<tr><td>Checkout</td><td>300ms</td><td>Revenue-critical, users waiting</td></tr>
<tr><td>Search</td><td>200ms</td><td>User experience, interactive</td></tr>
<tr><td>Admin Panel</td><td>2000ms</td><td>Internal tool, less critical</td></tr>
<tr><td>Batch Jobs</td><td>30000ms</td><td>Background, no users waiting</td></tr>
</tbody>
</table>
</div>
<h3 id="4-review-and-adjust-slos-quarterly"><a class="header" href="#4-review-and-adjust-slos-quarterly">4. Review and Adjust SLOs Quarterly</a></h3>
<ul>
<li>Analyze false positive rate</li>
<li>Check if SLOs match user complaints</li>
<li>Tighten SLOs as systems improve</li>
</ul>
<h2 id="further-reading-2"><a class="header" href="#further-reading-2">Further Reading</a></h2>
<ul>
<li><a href="#latency-evaluation">Latency Evaluation</a> - Response time SLO checking</li>
<li><a href="#error-rate-evaluation">Error Rate Evaluation</a> - Error percentage SLO checking</li>
<li><a href="#database-latency-evaluation">Database Latency</a> - Ratio-based DB latency evaluation</li>
<li><a href="#request-rate-evaluation-surgecliff">Request Rate</a> - Traffic surge and cliff detection</li>
<li><a href="#severity-adjustment">Severity Adjustment</a> - How severity is adjusted based on SLO status</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="latency-evaluation"><a class="header" href="#latency-evaluation">Latency Evaluation</a></h1>
<p>Latency evaluation compares current response time against SLO thresholds.</p>
<h2 id="thresholds"><a class="header" href="#thresholds">Thresholds</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Level</th><th>Default</th><th>Meaning</th></tr>
</thead>
<tbody>
<tr><td><code>acceptable</code></td><td>500ms</td><td>Latency below this is fine</td></tr>
<tr><td><code>warning</code></td><td>800ms</td><td>Approaching SLO limit</td></tr>
<tr><td><code>critical</code></td><td>1000ms</td><td>SLO breach</td></tr>
</tbody>
</table>
</div>
<h2 id="evaluation-logic"><a class="header" href="#evaluation-logic">Evaluation Logic</a></h2>
<pre><code>Current Latency
      │
      ▼
┌─────────────────────────────────────────────────────────┐
│  Is latency &lt; acceptable?                               │
│  ────────────────────────                               │
│  YES → status: "ok", proximity: latency/acceptable      │
│  NO  → Continue...                                      │
└─────────────────────────────────────────────────────────┘
      │
      ▼
┌─────────────────────────────────────────────────────────┐
│  Is latency &lt; warning?                                  │
│  ─────────────────────                                  │
│  YES → status: "elevated", minor concern                │
│  NO  → Continue...                                      │
└─────────────────────────────────────────────────────────┘
      │
      ▼
┌─────────────────────────────────────────────────────────┐
│  Is latency &lt; critical?                                 │
│  ──────────────────────                                 │
│  YES → status: "warning", investigate soon              │
│  NO  → status: "breached", immediate action             │
└─────────────────────────────────────────────────────────┘
</code></pre>
<h2 id="proximity-score"><a class="header" href="#proximity-score">Proximity Score</a></h2>
<p>The <code>proximity</code> value indicates how close to breach (0.0 - 1.0+):</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Proximity</th><th>Interpretation</th></tr>
</thead>
<tbody>
<tr><td>0.0 - 0.5</td><td>Comfortable margin</td></tr>
<tr><td>0.5 - 0.8</td><td>Getting close</td></tr>
<tr><td>0.8 - 1.0</td><td>Near threshold</td></tr>
<tr><td>&gt; 1.0</td><td>Threshold exceeded</td></tr>
</tbody>
</table>
</div>
<h2 id="output-example"><a class="header" href="#output-example">Output Example</a></h2>
<pre><code class="language-json">{
  "latency_evaluation": {
    "status": "warning",
    "value": 750.0,
    "threshold_acceptable": 500,
    "threshold_warning": 800,
    "threshold_critical": 1000,
    "proximity": 0.94
  }
}
</code></pre>
<h2 id="per-service-configuration"><a class="header" href="#per-service-configuration">Per-Service Configuration</a></h2>
<p>Critical services can have stricter thresholds:</p>
<pre><code class="language-json">{
  "slos": {
    "services": {
      "booking": {
        "latency_acceptable_ms": 300,
        "latency_warning_ms": 400,
        "latency_critical_ms": 500
      },
      "search": {
        "latency_acceptable_ms": 200,
        "latency_critical_ms": 400
      }
    }
  }
}
</code></pre>
<h2 id="busy-period-handling"><a class="header" href="#busy-period-handling">Busy Period Handling</a></h2>
<p>During configured busy periods, thresholds are relaxed by <code>busy_period_factor</code>:</p>
<pre><code class="language-json">{
  "slos": {
    "defaults": {
      "busy_period_factor": 1.5
    },
    "busy_periods": [
      {"start": "2024-12-20T00:00:00", "end": "2025-01-05T23:59:59"}
    ]
  }
}
</code></pre>
<p>During busy periods:</p>
<ul>
<li>500ms acceptable → 750ms acceptable</li>
<li>1000ms critical → 1500ms critical</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="error-rate-evaluation"><a class="header" href="#error-rate-evaluation">Error Rate Evaluation</a></h1>
<p>Error rate evaluation compares current error percentage against SLO thresholds.</p>
<h2 id="thresholds-1"><a class="header" href="#thresholds-1">Thresholds</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Level</th><th>Default</th><th>Meaning</th></tr>
</thead>
<tbody>
<tr><td><code>acceptable</code></td><td>0.5%</td><td>Error rate below this is fine</td></tr>
<tr><td><code>warning</code></td><td>1.0%</td><td>Approaching SLO limit</td></tr>
<tr><td><code>critical</code></td><td>2.0%</td><td>SLO breach</td></tr>
</tbody>
</table>
</div>
<h2 id="error-rate-floor-suppression"><a class="header" href="#error-rate-floor-suppression">Error Rate Floor (Suppression)</a></h2>
<p>To prevent alert noise from tiny error rate deviations, an optional <code>error_rate_floor</code> can suppress anomalies when errors are operationally insignificant.</p>
<pre><code class="language-json">{
  "slos": {
    "services": {
      "booking": {
        "error_rate_acceptable": 0.002,
        "error_rate_floor": 0.002
      }
    }
  }
}
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Error Rate</th><th>Floor (0.2%)</th><th>Result</th></tr>
</thead>
<tbody>
<tr><td>0.01%</td><td>Below floor</td><td><strong>Suppressed</strong> (no alert)</td></tr>
<tr><td>0.15%</td><td>Below floor</td><td><strong>Suppressed</strong> (no alert)</td></tr>
<tr><td>0.25%</td><td>Above floor</td><td>Alert fires</td></tr>
</tbody>
</table>
</div>
<h2 id="evaluation-logic-1"><a class="header" href="#evaluation-logic-1">Evaluation Logic</a></h2>
<pre><code>Current Error Rate
      │
      ▼
┌─────────────────────────────────────────────────────────┐
│  Is error_rate &lt; floor?                                 │
│  ──────────────────────                                 │
│  YES → Suppress anomaly entirely (no alert)             │
│  NO  → Continue...                                      │
└─────────────────────────────────────────────────────────┘
      │
      ▼
┌─────────────────────────────────────────────────────────┐
│  Is error_rate &lt; acceptable?                            │
│  ───────────────────────────                            │
│  YES → status: "ok", within_acceptable: true            │
│  NO  → Continue...                                      │
└─────────────────────────────────────────────────────────┘
      │
      ▼
┌─────────────────────────────────────────────────────────┐
│  Is error_rate &lt; warning?                               │
│  ────────────────────────                               │
│  YES → status: "elevated"                               │
│  NO  → Continue...                                      │
└─────────────────────────────────────────────────────────┘
      │
      ▼
┌─────────────────────────────────────────────────────────┐
│  Is error_rate &lt; critical?                              │
│  ─────────────────────────                              │
│  YES → status: "warning"                                │
│  NO  → status: "breached"                               │
└─────────────────────────────────────────────────────────┘
</code></pre>
<h2 id="output-example-1"><a class="header" href="#output-example-1">Output Example</a></h2>
<pre><code class="language-json">{
  "error_rate_evaluation": {
    "status": "ok",
    "value": 0.001,
    "value_percent": "0.10%",
    "threshold_acceptable": 0.005,
    "threshold_warning": 0.01,
    "threshold_critical": 0.02,
    "within_acceptable": true
  }
}
</code></pre>
<h2 id="when-floor-is-active"><a class="header" href="#when-floor-is-active">When Floor is Active</a></h2>
<pre><code class="language-json">{
  "slo_context": {
    "current_value": 0.005,
    "current_value_percent": "0.50%",
    "acceptable_threshold": 0.002,
    "critical_threshold": 0.01,
    "suppression_threshold": 0.002,
    "within_acceptable": false
  }
}
</code></pre>
<h2 id="per-service-configuration-1"><a class="header" href="#per-service-configuration-1">Per-Service Configuration</a></h2>
<pre><code class="language-json">{
  "slos": {
    "services": {
      "booking": {
        "error_rate_acceptable": 0.002,
        "error_rate_warning": 0.005,
        "error_rate_critical": 0.01,
        "error_rate_floor": 0.002
      },
      "admin-api": {
        "error_rate_acceptable": 0.01,
        "error_rate_critical": 0.05
      }
    }
  }
}
</code></pre>
<h2 id="exception-enrichment"><a class="header" href="#exception-enrichment">Exception Enrichment</a></h2>
<p>When error SLO is breached (HIGH or CRITICAL severity), exception context is automatically queried and added to the alert:</p>
<pre><code class="language-json">{
  "exception_context": {
    "service_name": "search",
    "total_exception_rate": 0.35,
    "top_exceptions": [
      {"type": "R2D2Exception", "rate": 0.217, "percentage": 62.0},
      {"type": "UserInputException", "rate": 0.083, "percentage": 23.7}
    ]
  }
}
</code></pre>
<p>This helps identify which exception types are causing the error spike.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="database-latency-evaluation"><a class="header" href="#database-latency-evaluation">Database Latency Evaluation</a></h1>
<p>Database latency uses a <strong>hybrid approach</strong>: noise floor filtering + ratio-based thresholds against training baseline.</p>
<h2 id="why-ratio-based"><a class="header" href="#why-ratio-based">Why Ratio-Based?</a></h2>
<p>Unlike application latency with fixed SLO targets, database latency varies significantly:</p>
<ul>
<li>A fast service might have 5ms DB latency normally</li>
<li>A reporting service might have 500ms DB latency normally</li>
</ul>
<p><strong>Ratio-based thresholds</strong> adapt to each service’s baseline.</p>
<h2 id="noise-floor"><a class="header" href="#noise-floor">Noise Floor</a></h2>
<p>Very low database latency values are filtered as operationally meaningless:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Latency</th><th>Floor (5ms)</th><th>Result</th></tr>
</thead>
<tbody>
<tr><td>2ms</td><td>Below floor</td><td>Always <code>ok</code></td></tr>
<tr><td>0.3ms → 0.4ms</td><td>Below floor</td><td>Ignored</td></tr>
<tr><td>10ms</td><td>Above floor</td><td>Evaluate ratio</td></tr>
</tbody>
</table>
</div>
<p>This prevents alerts for sub-millisecond changes that the ML might flag as anomalous.</p>
<h2 id="ratio-thresholds"><a class="header" href="#ratio-thresholds">Ratio Thresholds</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Status</th><th>Ratio</th><th>Meaning</th></tr>
</thead>
<tbody>
<tr><td><code>ok</code></td><td>&lt; 1.5x</td><td>Within normal variance</td></tr>
<tr><td><code>info</code></td><td>1.5x - 2x</td><td>Slightly elevated</td></tr>
<tr><td><code>warning</code></td><td>2x - 3x</td><td>Elevated, investigate</td></tr>
<tr><td><code>high</code></td><td>3x - 5x</td><td>Significantly elevated</td></tr>
<tr><td><code>critical</code></td><td>≥ 5x</td><td>SLO breach</td></tr>
</tbody>
</table>
</div>
<h2 id="evaluation-logic-2"><a class="header" href="#evaluation-logic-2">Evaluation Logic</a></h2>
<pre><code>Current DB Latency
      │
      ▼
┌─────────────────────────────────────────────────────────┐
│  Is latency &lt; floor_ms?                                 │
│  ──────────────────────                                 │
│  YES → status: "ok", below_floor: true                  │
│  NO  → Continue...                                      │
└─────────────────────────────────────────────────────────┘
      │
      ▼
┌─────────────────────────────────────────────────────────┐
│  Calculate ratio = current / baseline_mean              │
│  ──────────────────────────────────────                 │
│  Example: 25ms / 10ms = 2.5x                            │
└─────────────────────────────────────────────────────────┘
      │
      ▼
┌─────────────────────────────────────────────────────────┐
│  Map ratio to status:                                   │
│  ────────────────────                                   │
│  &lt; 1.5 → "ok"                                           │
│  &lt; 2.0 → "info"                                         │
│  &lt; 3.0 → "warning"                                      │
│  &lt; 5.0 → "high"                                         │
│  ≥ 5.0 → "critical"                                     │
└─────────────────────────────────────────────────────────┘
</code></pre>
<h2 id="output-examples"><a class="header" href="#output-examples">Output Examples</a></h2>
<h3 id="below-floor"><a class="header" href="#below-floor">Below Floor</a></h3>
<pre><code class="language-json">{
  "database_latency_evaluation": {
    "status": "ok",
    "value_ms": 2.0,
    "baseline_mean_ms": 1.0,
    "ratio": 0.0,
    "below_floor": true,
    "floor_ms": 5.0,
    "explanation": "Below noise floor (2.0ms &lt; 5.0ms)"
  }
}
</code></pre>
<h3 id="elevated-ratio"><a class="header" href="#elevated-ratio">Elevated Ratio</a></h3>
<pre><code class="language-json">{
  "database_latency_evaluation": {
    "status": "warning",
    "value_ms": 25.0,
    "baseline_mean_ms": 10.0,
    "ratio": 2.5,
    "below_floor": false,
    "floor_ms": 5.0,
    "thresholds": {
      "info": 1.5,
      "warning": 2.0,
      "high": 3.0,
      "critical": 5.0
    },
    "explanation": "DB latency elevated: 25.0ms is 2.5x baseline (10.0ms)"
  }
}
</code></pre>
<h2 id="per-service-configuration-2"><a class="header" href="#per-service-configuration-2">Per-Service Configuration</a></h2>
<p>Services with different DB performance characteristics can have custom thresholds:</p>
<pre><code class="language-json">{
  "slos": {
    "services": {
      "search": {
        "database_latency_floor_ms": 2.0,
        "database_latency_ratios": {
          "info": 1.3,
          "warning": 1.5,
          "high": 2.0,
          "critical": 3.0
        }
      },
      "reporting": {
        "database_latency_floor_ms": 50.0,
        "database_latency_ratios": {
          "info": 2.0,
          "warning": 3.0,
          "high": 5.0,
          "critical": 10.0
        }
      }
    }
  }
}
</code></pre>
<h2 id="relationship-to-pattern-matching"><a class="header" href="#relationship-to-pattern-matching">Relationship to Pattern Matching</a></h2>
<p>Database latency evaluation complements pattern matching:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Pattern</th><th>Database Latency Evaluation</th></tr>
</thead>
<tbody>
<tr><td><code>database_degradation</code></td><td>DB ratio 2-3x, compensating</td></tr>
<tr><td><code>database_bottleneck</code></td><td>DB ratio ≥3x, dominant latency</td></tr>
</tbody>
</table>
</div>
<p>The SLO evaluation provides the <strong>ratio context</strong> that pattern matching uses for severity.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="request-rate-evaluation-surgecliff"><a class="header" href="#request-rate-evaluation-surgecliff">Request Rate Evaluation (Surge/Cliff)</a></h1>
<p>Request rate evaluation detects significant traffic changes: <strong>surges</strong> (spikes) and <strong>cliffs</strong> (drops).</p>
<h2 id="key-insight"><a class="header" href="#key-insight">Key Insight</a></h2>
<p>Traffic anomalies use <strong>correlation-based severity</strong>:</p>
<ul>
<li>A surge alone is often benign (marketing campaign, organic growth)</li>
<li>A surge becomes problematic when causing SLO issues</li>
<li>A cliff is inherently concerning (may indicate upstream failure)</li>
</ul>
<h2 id="thresholds-2"><a class="header" href="#thresholds-2">Thresholds</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Type</th><th>Threshold</th><th>Default</th></tr>
</thead>
<tbody>
<tr><td>Surge</td><td>≥ 200% of baseline</td><td>2x normal traffic</td></tr>
<tr><td>Cliff</td><td>≤ 50% of baseline</td><td>Half normal traffic</td></tr>
</tbody>
</table>
</div>
<h2 id="surge-severity-logic"><a class="header" href="#surge-severity-logic">Surge Severity Logic</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Condition</th><th>Severity</th><th>Rationale</th></tr>
</thead>
<tbody>
<tr><td>Surge only</td><td><code>informational</code></td><td>Normal growth or campaign</td></tr>
<tr><td>Surge + latency SLO breach</td><td><code>warning</code></td><td>Capacity stress</td></tr>
<tr><td>Surge + error SLO breach</td><td><code>high</code></td><td>Active incident</td></tr>
</tbody>
</table>
</div>
<h2 id="cliff-severity-logic"><a class="header" href="#cliff-severity-logic">Cliff Severity Logic</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Condition</th><th>Severity</th><th>Rationale</th></tr>
</thead>
<tbody>
<tr><td>Cliff off-peak</td><td><code>warning</code></td><td>May be expected</td></tr>
<tr><td>Cliff peak hours</td><td><code>high</code></td><td>Likely incident</td></tr>
<tr><td>Cliff + errors</td><td><code>critical</code></td><td>Confirmed incident</td></tr>
</tbody>
</table>
</div>
<h2 id="evaluation-flow-1"><a class="header" href="#evaluation-flow-1">Evaluation Flow</a></h2>
<pre><code>                    ┌───────────────────────────────────────────┐
                    │         Calculate Traffic Ratio           │
                    │    ratio = current / baseline_mean        │
                    └───────────────────────────────────────────┘
                                        │
               ┌────────────────────────┼────────────────────────┐
               │                        │                        │
               ▼                        ▼                        ▼
        ratio ≥ 2.0              0.5 &lt; ratio &lt; 2.0          ratio ≤ 0.5
         (SURGE)                    (NORMAL)                  (CLIFF)
               │                        │                        │
               ▼                        │                        ▼
    ┌──────────────────────┐           │          ┌──────────────────────┐
    │ Check SLO correlation │           │          │ Check time of day    │
    │ - Latency breach?     │           │          │ - Peak hours?        │
    │ - Error breach?       │           │          │ - Errors elevated?   │
    └──────────────────────┘           │          └──────────────────────┘
               │                        │                        │
               ▼                        ▼                        ▼
       severity based               "ok"                  severity based
       on correlation               status                on peak/errors
</code></pre>
<h2 id="output-examples-1"><a class="header" href="#output-examples-1">Output Examples</a></h2>
<h3 id="surge-standalone"><a class="header" href="#surge-standalone">Surge (Standalone)</a></h3>
<pre><code class="language-json">{
  "request_rate_evaluation": {
    "status": "info",
    "type": "surge",
    "severity": "informational",
    "value_rps": 250.0,
    "baseline_mean_rps": 100.0,
    "ratio": 2.5,
    "threshold_percent": 200.0,
    "correlated_with_latency": false,
    "correlated_with_errors": false,
    "explanation": "Traffic surge (2.5x baseline) without SLO impact. Normal growth or campaign traffic."
  }
}
</code></pre>
<h3 id="surge-with-latency-impact"><a class="header" href="#surge-with-latency-impact">Surge with Latency Impact</a></h3>
<pre><code class="language-json">{
  "request_rate_evaluation": {
    "status": "warning",
    "type": "surge",
    "severity": "warning",
    "value_rps": 350.0,
    "baseline_mean_rps": 100.0,
    "ratio": 3.5,
    "correlated_with_latency": true,
    "correlated_with_errors": false,
    "explanation": "Traffic surge (3.5x baseline) correlating with latency SLO breach - capacity issue."
  }
}
</code></pre>
<h3 id="cliff-peak-hours"><a class="header" href="#cliff-peak-hours">Cliff (Peak Hours)</a></h3>
<pre><code class="language-json">{
  "request_rate_evaluation": {
    "status": "high",
    "type": "cliff",
    "severity": "high",
    "value_rps": 30.0,
    "baseline_mean_rps": 100.0,
    "ratio": 0.3,
    "threshold_percent": 50.0,
    "is_peak_hours": true,
    "correlated_with_errors": false,
    "explanation": "Traffic cliff (0.3x baseline) during peak hours - investigate potential incident."
  }
}
</code></pre>
<h3 id="cliff-with-errors"><a class="header" href="#cliff-with-errors">Cliff with Errors</a></h3>
<pre><code class="language-json">{
  "request_rate_evaluation": {
    "status": "critical",
    "type": "cliff",
    "severity": "critical",
    "value_rps": 15.0,
    "baseline_mean_rps": 100.0,
    "ratio": 0.15,
    "is_peak_hours": true,
    "correlated_with_errors": true,
    "explanation": "Traffic cliff (0.15x baseline) with errors - likely upstream failure or routing issue."
  }
}
</code></pre>
<h2 id="configuration-1"><a class="header" href="#configuration-1">Configuration</a></h2>
<pre><code class="language-json">{
  "slos": {
    "defaults": {
      "request_rate_surge_threshold": 2.0,
      "request_rate_cliff_threshold": 0.5
    },
    "services": {
      "booking": {
        "request_rate_evaluation": {
          "surge": {
            "threshold": 3.0
          },
          "cliff": {
            "standalone_severity": "high",
            "peak_hours_severity": "critical"
          }
        }
      }
    }
  }
}
</code></pre>
<h2 id="relationship-to-pattern-matching-1"><a class="header" href="#relationship-to-pattern-matching-1">Relationship to Pattern Matching</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Pattern</th><th>Request Rate Evaluation</th></tr>
</thead>
<tbody>
<tr><td><code>traffic_surge_healthy</code></td><td>Surge, no SLO correlation</td></tr>
<tr><td><code>traffic_surge_degrading</code></td><td>Surge + latency correlation</td></tr>
<tr><td><code>traffic_surge_failing</code></td><td>Surge + error correlation</td></tr>
<tr><td><code>traffic_cliff</code></td><td>Cliff detected</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="severity-adjustment"><a class="header" href="#severity-adjustment">Severity Adjustment</a></h1>
<p>The final step of SLO evaluation: adjusting ML-assigned severity based on operational impact.</p>
<h2 id="core-principle"><a class="header" href="#core-principle">Core Principle</a></h2>
<p><strong>SLO status determines final severity, not ML confidence.</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>SLO Status</th><th>Final Severity</th><th>Rationale</th></tr>
</thead>
<tbody>
<tr><td><code>ok</code></td><td><code>low</code></td><td>Anomaly detected but operationally acceptable</td></tr>
<tr><td><code>warning</code></td><td><code>high</code></td><td>Approaching limits, should investigate</td></tr>
<tr><td><code>breached</code></td><td><code>critical</code></td><td>SLO exceeded, requires action</td></tr>
</tbody>
</table>
</div>
<h2 id="adjustment-matrix"><a class="header" href="#adjustment-matrix">Adjustment Matrix</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ML Severity</th><th>SLO Status</th><th>Final Severity</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td>critical</td><td>ok</td><td><strong>low</strong></td><td>280ms latency (unusual but &lt; 300ms SLO)</td></tr>
<tr><td>critical</td><td>warning</td><td>high</td><td>750ms latency (approaching 800ms SLO)</td></tr>
<tr><td>critical</td><td>breached</td><td>critical</td><td>1200ms latency (&gt; 1000ms SLO)</td></tr>
<tr><td>high</td><td>ok</td><td><strong>low</strong></td><td>Same principle</td></tr>
<tr><td>high</td><td>warning</td><td>high</td><td>No change needed</td></tr>
<tr><td>high</td><td>breached</td><td>critical</td><td>Escalated</td></tr>
<tr><td>medium</td><td>ok</td><td><strong>low</strong></td><td>Minor anomaly, within SLO</td></tr>
<tr><td>medium</td><td>warning</td><td>high</td><td>Escalated due to SLO proximity</td></tr>
<tr><td>low</td><td>breached</td><td>critical</td><td>Even low anomaly escalated if SLO breached</td></tr>
</tbody>
</table>
</div>
<h2 id="key-behavior-change-v131"><a class="header" href="#key-behavior-change-v131">Key Behavior Change (v1.3.1)</a></h2>
<p><strong>Before v1.3.1</strong>: SLO <code>ok</code> adjusted to <code>medium</code>
<strong>After v1.3.1</strong>: SLO <code>ok</code> adjusts to <code>low</code></p>
<p>This ensures “operationally acceptable” consistently means “low priority.”</p>
<h2 id="adjustment-flow"><a class="header" href="#adjustment-flow">Adjustment Flow</a></h2>
<pre><code>┌─────────────────────────────────────────────────────────────────┐
│                    Severity Adjustment                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   ML Detection                SLO Evaluation                    │
│   ────────────               ───────────────                    │
│   severity: critical         slo_status: ok                     │
│                                                                 │
│          │                          │                           │
│          └──────────┬───────────────┘                           │
│                     │                                           │
│                     ▼                                           │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │  Rule: If slo_status == "ok" → severity = "low"         │   │
│   │  Rule: If slo_status == "warning" → severity &gt;= "high"  │   │
│   │  Rule: If slo_status == "breached" → severity = "critical│   │
│   └─────────────────────────────────────────────────────────┘   │
│                     │                                           │
│                     ▼                                           │
│                                                                 │
│   Final Output:                                                 │
│   ─────────────                                                 │
│   overall_severity: low                                         │
│   slo_evaluation:                                               │
│     original_severity: critical                                 │
│     adjusted_severity: low                                      │
│     severity_changed: true                                      │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
</code></pre>
<h2 id="output-example-2"><a class="header" href="#output-example-2">Output Example</a></h2>
<pre><code class="language-json">{
  "overall_severity": "low",
  "slo_evaluation": {
    "original_severity": "critical",
    "adjusted_severity": "low",
    "severity_changed": true,
    "slo_status": "ok",
    "slo_proximity": 0.56,
    "operational_impact": "informational",
    "explanation": "Severity adjusted from critical to low based on SLO evaluation. Anomaly detected but metrics within acceptable SLO thresholds (latency: 280ms &lt; 300ms, errors: 0.10% &lt; 0.50%)."
  }
}
</code></pre>
<h2 id="operational-impact-levels"><a class="header" href="#operational-impact-levels">Operational Impact Levels</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Level</th><th>Meaning</th><th>Action</th></tr>
</thead>
<tbody>
<tr><td><code>none</code></td><td>No anomaly or fully normal</td><td>No action</td></tr>
<tr><td><code>informational</code></td><td>Anomaly noted but acceptable</td><td>Log only</td></tr>
<tr><td><code>actionable</code></td><td>Approaching limits</td><td>Investigate</td></tr>
<tr><td><code>critical</code></td><td>SLO breached</td><td>Immediate action</td></tr>
</tbody>
</table>
</div>
<h2 id="configuration-options"><a class="header" href="#configuration-options">Configuration Options</a></h2>
<pre><code class="language-json">{
  "slos": {
    "enabled": true,
    "allow_downgrade_to_informational": true,
    "require_slo_breach_for_critical": true
  }
}
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Default</th><th>Effect</th></tr>
</thead>
<tbody>
<tr><td><code>enabled</code></td><td>true</td><td>Enable/disable SLO evaluation</td></tr>
<tr><td><code>allow_downgrade_to_informational</code></td><td>true</td><td>Allow high/critical → low when SLO ok</td></tr>
<tr><td><code>require_slo_breach_for_critical</code></td><td>true</td><td>Only allow critical if SLO breached</td></tr>
</tbody>
</table>
</div>
<h2 id="root-overall_severity"><a class="header" href="#root-overall_severity">root <code>overall_severity</code></a></h2>
<p><strong>Important (v1.3.1)</strong>: The root-level <code>overall_severity</code> field now correctly reflects the SLO-adjusted value.</p>
<pre><code class="language-json">{
  "overall_severity": "low",          // ← SLO-adjusted value
  "slo_evaluation": {
    "original_severity": "critical",  // ← ML-assigned value
    "adjusted_severity": "low"        // ← Same as overall_severity
  }
}
</code></pre>
<p>Previously, <code>overall_severity</code> could show <code>critical</code> while <code>adjusted_severity</code> showed <code>low</code>.</p>
<h2 id="alert-suppression"><a class="header" href="#alert-suppression">Alert Suppression</a></h2>
<p>When SLO evaluation results in <code>low</code> severity:</p>
<ul>
<li>Alert is still generated (for logging/visibility)</li>
<li>Alert may be filtered by downstream systems (e.g., skip PagerDuty)</li>
<li>Dashboard shows alert with low priority indicator</li>
</ul>
<p>To completely suppress alerts below a threshold, use downstream alert filtering rules.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="incident-lifecycle"><a class="header" href="#incident-lifecycle">Incident Lifecycle</a></h1>
<p>The incident lifecycle manages anomaly state over time, reducing alert noise through confirmation requirements and grace periods. This chapter explains how Yaga2 tracks anomalies as they are detected, confirmed, and eventually resolved.</p>
<h2 id="what-problem-does-incident-lifecycle-solve"><a class="header" href="#what-problem-does-incident-lifecycle-solve">What Problem Does Incident Lifecycle Solve?</a></h2>
<p>Raw anomaly detection produces a stream of “yes/no” decisions every inference cycle. Without state management, this creates significant operational problems:</p>
<h3 id="problem-1-transient-spikes-cause-alert-noise"><a class="header" href="#problem-1-transient-spikes-cause-alert-noise">Problem 1: Transient Spikes Cause Alert Noise</a></h3>
<pre><code>Without lifecycle management:
─────────────────────────────
10:00  Latency 450ms  → ALERT!  (ops team paged)
10:03  Latency 120ms  → Resolved
10:06  Latency 455ms  → ALERT!  (ops team paged again)
10:09  Latency 118ms  → Resolved
10:12  Latency 460ms  → ALERT!  (ops team paged again)
10:15  Latency 115ms  → Resolved

Result: 3 alerts in 15 minutes for what might be normal variance
        Ops team frustrated, starts ignoring alerts
</code></pre>
<h3 id="problem-2-brief-recovery-causes-flapping"><a class="header" href="#problem-2-brief-recovery-causes-flapping">Problem 2: Brief Recovery Causes Flapping</a></h3>
<pre><code>Without lifecycle management:
─────────────────────────────
10:00  Error rate 5%   → ALERT!
10:03  Error rate 5%   → Continue
10:06  Error rate 4.8% → Continue
10:09  Error rate 0.5% → Resolved  (brief dip)
10:12  Error rate 5.2% → ALERT!   (new alert!)
10:15  Error rate 5%   → Continue

Result: Same ongoing incident treated as two separate incidents
        Resolution was premature - the issue wasn't actually fixed
</code></pre>
<h3 id="problem-3-no-incident-correlation"><a class="header" href="#problem-3-no-incident-correlation">Problem 3: No Incident Correlation</a></h3>
<pre><code>Without lifecycle management:
─────────────────────────────
Each detection is independent. No way to answer:
- "How long has this been happening?"
- "Is this the same issue from yesterday?"
- "How many times has this pattern occurred?"
</code></pre>
<h2 id="the-solution-stateful-incident-tracking"><a class="header" href="#the-solution-stateful-incident-tracking">The Solution: Stateful Incident Tracking</a></h2>
<p>Yaga2 solves these problems with a <strong>state machine</strong> that tracks anomalies through a defined lifecycle:</p>
<pre><code>┌───────────────────────────────────────────────────────────────────────────┐
│                    THE INCIDENT LIFECYCLE                                  │
├───────────────────────────────────────────────────────────────────────────┤
│                                                                           │
│    First Detection                                                        │
│         │                                                                 │
│         ▼                                                                 │
│    ┌─────────────┐                                                        │
│    │  SUSPECTED  │   "We saw something - let's wait to be sure"           │
│    │             │   • No alert sent yet                                  │
│    │  (waiting)  │   • Needs 2 consecutive detections                     │
│    └──────┬──────┘                                                        │
│           │                                                               │
│           │ Detected again (confirmed!)                                   │
│           ▼                                                               │
│    ┌─────────────┐                                                        │
│    │    OPEN     │   "This is real - notify the team"                     │
│    │             │◄─────────────┐                                         │
│    │ (alerting)  │   detected   │ • Alert sent to web API                 │
│    └──────┬──────┘   again      │ • Tracking duration and occurrences     │
│           │          │          │                                         │
│           │ Not detected                                                  │
│           ▼          │          │                                         │
│    ┌─────────────┐   │          │                                         │
│    │ RECOVERING  │───┘          │                                         │
│    │             │   "Anomaly cleared - but let's wait"                   │
│    │  (waiting)  │   • No resolution yet                                  │
│    └──────┬──────┘   • Grace period in progress                           │
│           │                                                               │
│           │ Still not detected (confirmed resolved!)                      │
│           ▼                                                               │
│    ┌─────────────┐                                                        │
│    │   CLOSED    │   "Incident resolved - notify the team"                │
│    │             │   • Resolution sent to web API                         │
│    │ (resolved)  │   • Incident history preserved                         │
│    └─────────────┘                                                        │
│                                                                           │
└───────────────────────────────────────────────────────────────────────────┘
</code></pre>
<h2 id="core-concepts-1"><a class="header" href="#core-concepts-1">Core Concepts</a></h2>
<h3 id="fingerprint-vs-incident"><a class="header" href="#fingerprint-vs-incident">Fingerprint vs Incident</a></h3>
<p>These two concepts are central to incident tracking:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Concept</th><th>What It Is</th><th>Analogy</th></tr>
</thead>
<tbody>
<tr><td><strong>Fingerprint ID</strong></td><td>Pattern identifier (same pattern = same ID)</td><td>The “type” of crime</td></tr>
<tr><td><strong>Incident ID</strong></td><td>Unique occurrence identifier</td><td>A specific “case file”</td></tr>
</tbody>
</table>
</div>
<p><strong>Fingerprint ID</strong> is <strong>deterministic</strong> - it’s computed from the anomaly content:</p>
<pre><code>hash("booking_business_hours_latency_spike_recent")
  → anomaly_8d4a011b83ca
</code></pre>
<p>The same anomaly pattern on the same service always gets the same fingerprint ID.</p>
<p><strong>Incident ID</strong> is <strong>unique</strong> - each new occurrence gets a fresh identifier:</p>
<pre><code>Random UUID generation
  → incident_1dcbafc91480
</code></pre>
<h3 id="why-both-ids"><a class="header" href="#why-both-ids">Why Both IDs?</a></h3>
<p>The same anomaly pattern can occur multiple times:</p>
<pre><code>                    Fingerprint: anomaly_abc123
               (latency_spike_recent on booking)
                             │
         ┌───────────────────┼───────────────────┐
         │                   │                   │
    Incident #1         Incident #2         Incident #3
    Jan 10, 14:00       Jan 15, 09:30       Jan 22, 16:45
    Duration: 45min     Duration: 2hrs      Duration: 20min

    Each is a separate occurrence of the same pattern type
</code></pre>
<p>This enables powerful analysis:</p>
<ul>
<li>“This pattern has occurred 3 times this month”</li>
<li>“Average duration is 58 minutes”</li>
<li>“Longest incident was 2 hours on Jan 15”</li>
</ul>
<h2 id="states-explained"><a class="header" href="#states-explained">States Explained</a></h2>
<h3 id="suspected---first-detection"><a class="header" href="#suspected---first-detection">SUSPECTED - First Detection</a></h3>
<p>When an anomaly is detected for the first time, it enters the SUSPECTED state:</p>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│  STATE: SUSPECTED                                                   │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  What happened:                                                     │
│    ML detected something unusual                                    │
│                                                                     │
│  What we do:                                                        │
│    Wait for confirmation (not alert yet)                            │
│                                                                     │
│  Why wait?                                                          │
│    Single-point anomalies are often noise:                          │
│    • Brief network blip                                             │
│    • One slow database query                                        │
│    • Measurement variance                                           │
│                                                                     │
│  Payload fields:                                                    │
│    status: "SUSPECTED"                                              │
│    is_confirmed: false                                              │
│    confirmation_pending: true                                       │
│    cycles_to_confirm: 1                                             │
│                                                                     │
│  Web API:  NOT notified (to prevent orphaned incidents)             │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="open---confirmed-incident"><a class="header" href="#open---confirmed-incident">OPEN - Confirmed Incident</a></h3>
<p>After 2+ consecutive detections, the incident is <strong>confirmed</strong>:</p>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│  STATE: OPEN                                                        │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  What happened:                                                     │
│    Anomaly persisted for 2+ detection cycles                        │
│                                                                     │
│  What we do:                                                        │
│    Send alert to web API                                            │
│    Track duration and occurrence count                              │
│                                                                     │
│  Why confirm?                                                       │
│    Persistent anomalies are likely real issues:                     │
│    • Not a brief spike                                              │
│    • Sustained degradation                                          │
│    • Worth investigating                                            │
│                                                                     │
│  Payload fields:                                                    │
│    status: "OPEN"                                                   │
│    is_confirmed: true                                               │
│    newly_confirmed: true (on first cycle only)                      │
│    occurrence_count: incrementing                                   │
│                                                                     │
│  Web API:  Alert sent!                                              │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="recovering---grace-period"><a class="header" href="#recovering---grace-period">RECOVERING - Grace Period</a></h3>
<p>When the anomaly stops being detected, the incident enters a grace period:</p>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│  STATE: RECOVERING                                                  │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  What happened:                                                     │
│    Anomaly not detected in last cycle                               │
│                                                                     │
│  What we do:                                                        │
│    Wait before sending resolution                                   │
│    Watch for anomaly return                                         │
│                                                                     │
│  Why wait?                                                          │
│    Brief recovery is often not real resolution:                     │
│    • Issue may return in next cycle                                 │
│    • Prevents "flapping" alerts                                     │
│    • More reliable resolution signal                                │
│                                                                     │
│  Payload fields:                                                    │
│    status: "RECOVERING"                                             │
│    missed_cycles: 1, 2, ...                                         │
│                                                                     │
│  Web API:  No update yet (waiting for confirmation)                 │
│                                                                     │
│  If anomaly returns:  → Back to OPEN                                │
│  If still clear:      → Continue grace period                       │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="closed---resolved"><a class="header" href="#closed---resolved">CLOSED - Resolved</a></h3>
<p>After 3+ cycles without detection, the incident is confirmed resolved:</p>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│  STATE: CLOSED                                                      │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  What happened:                                                     │
│    Anomaly not detected for 3+ consecutive cycles                   │
│                                                                     │
│  What we do:                                                        │
│    Send resolution to web API                                       │
│    Record incident history                                          │
│                                                                     │
│  Resolution contains:                                               │
│    • Total duration                                                 │
│    • Occurrence count                                               │
│    • Final severity                                                 │
│    • Resolution reason                                              │
│                                                                     │
│  Payload fields:                                                    │
│    incident_action: "CLOSE"                                         │
│    resolution_reason: "resolved"                                    │
│    incident_duration_minutes: final value                           │
│    total_occurrences: final count                                   │
│                                                                     │
│  Web API:  Resolution sent!                                         │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h2 id="cycle-based-timing"><a class="header" href="#cycle-based-timing">Cycle-Based Timing</a></h2>
<p>The lifecycle is based on <strong>detection cycles</strong>, not wall-clock time:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Parameter</th><th>Default</th><th>With 3-min inference</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td><code>confirmation_cycles</code></td><td>2</td><td>~6 min to confirm</td><td>Prevent false alerts</td></tr>
<tr><td><code>resolution_grace_cycles</code></td><td>3</td><td>~9 min grace period</td><td>Prevent flapping</td></tr>
<tr><td><code>incident_separation_minutes</code></td><td>30</td><td>30 min gap = new incident</td><td>Prevent zombie incidents</td></tr>
</tbody>
</table>
</div>
<h3 id="example-timeline"><a class="header" href="#example-timeline">Example Timeline</a></h3>
<pre><code>Time     Detection   Status      Action
────────────────────────────────────────────────────
10:00    Spike!      SUSPECTED   Wait for confirmation
10:03    Spike!      OPEN        Alert sent (confirmed after 2 cycles)
10:06    Spike!      OPEN        Continue tracking
10:09    Spike!      OPEN        Continue tracking
10:12    Normal      RECOVERING  Grace period starts
10:15    Normal      RECOVERING  Grace period continues
10:18    Normal      CLOSED      Resolution sent (3 cycles without detection)
</code></pre>
<h2 id="why-confirmation"><a class="header" href="#why-confirmation">Why Confirmation?</a></h2>
<p><strong>The problem</strong>: Single-detection alerts create noise.</p>
<pre><code>WITHOUT CONFIRMATION:
─────────────────────
Every detection immediately fires an alert.

┌──────────────────────────────────────────────────────────────────┐
│  10:00  450ms latency detected  →  ALERT!  (page on-call)        │
│  10:03  120ms latency (normal)  →  Resolved                      │
│  10:06  455ms latency detected  →  ALERT!  (page on-call again)  │
│  10:09  118ms latency (normal)  →  Resolved                      │
│  10:12  460ms latency detected  →  ALERT!  (page on-call again)  │
│  10:15  115ms latency (normal)  →  Resolved                      │
└──────────────────────────────────────────────────────────────────┘

Result: 3 alerts for transient spikes
        On-call engineer interrupted 3 times
        Nothing was actually wrong
</code></pre>
<p><strong>The solution</strong>: Require consecutive detections.</p>
<pre><code>WITH CONFIRMATION (2 cycles):
─────────────────────────────
Only sustained anomalies fire alerts.

┌──────────────────────────────────────────────────────────────────┐
│  10:00  450ms latency detected  →  SUSPECTED (wait)              │
│  10:03  120ms latency (normal)  →  SUSPECTED expires quietly     │
│  10:06  455ms latency detected  →  SUSPECTED (wait)              │
│  10:09  460ms latency detected  →  OPEN - ALERT! (confirmed)     │
│  10:12  455ms latency detected  →  OPEN (continuing)             │
│  10:15  450ms latency detected  →  OPEN (continuing)             │
│  10:18  120ms latency (normal)  →  RECOVERING (grace period)     │
│  10:21  115ms latency (normal)  →  RECOVERING (grace period)     │
│  10:24  118ms latency (normal)  →  CLOSED - Resolved             │
└──────────────────────────────────────────────────────────────────┘

Result: 1 alert for a real sustained issue
        Resolution only after confirmed recovery
</code></pre>
<h2 id="why-grace-period"><a class="header" href="#why-grace-period">Why Grace Period?</a></h2>
<p><strong>The problem</strong>: Brief dips in anomaly cause flapping.</p>
<pre><code>WITHOUT GRACE PERIOD:
─────────────────────
Every non-detection immediately resolves.

┌──────────────────────────────────────────────────────────────────┐
│  10:00  Error 5%  →  ALERT!                                      │
│  10:03  Error 5%  →  Continue                                    │
│  10:06  Error 0.5% →  Resolved!  (brief dip)                     │
│  10:09  Error 5%  →  ALERT!  (new incident)                      │
│  10:12  Error 5%  →  Continue                                    │
│  10:15  Error 0.5% →  Resolved!  (another brief dip)             │
│  10:18  Error 5%  →  ALERT!  (third incident)                    │
└──────────────────────────────────────────────────────────────────┘

Result: 3 separate incidents for what's really one ongoing issue
        "Flapping" alerts every time there's a brief improvement
</code></pre>
<p><strong>The solution</strong>: Wait before confirming resolution.</p>
<pre><code>WITH GRACE PERIOD (3 cycles):
─────────────────────────────
Resolution requires sustained recovery.

┌──────────────────────────────────────────────────────────────────┐
│  10:00  Error 5%  →  ALERT!                                      │
│  10:03  Error 5%  →  Continue                                    │
│  10:06  Error 0.5% →  RECOVERING (grace period, not resolved)    │
│  10:09  Error 5%  →  OPEN (back to alerting - wasn't resolved)   │
│  10:12  Error 5%  →  Continue                                    │
│  10:15  Error 0.5% →  RECOVERING (grace period)                  │
│  10:18  Error 0.5% →  RECOVERING (grace continues)               │
│  10:21  Error 0.5% →  CLOSED - Resolved (confirmed recovery)     │
└──────────────────────────────────────────────────────────────────┘

Result: 1 incident throughout entire episode
        Resolution only after confirmed sustained recovery
</code></pre>
<h2 id="staleness-check"><a class="header" href="#staleness-check">Staleness Check</a></h2>
<p><strong>The problem</strong>: Old incidents continuing forever.</p>
<p>If an anomaly is detected, disappears for hours, then reappears, is it the same incident?</p>
<pre><code>10:00  - OPEN incident for latency_spike_recent
10:30  - Last detection
...
15:00  - Same pattern detected again (4.5 hours later!)

Is this the same incident? Probably not - it's a new occurrence.
</code></pre>
<p><strong>The solution</strong>: <code>incident_separation_minutes</code> threshold.</p>
<pre><code>If gap &gt; incident_separation_minutes (default: 30 min):
  → Old incident auto-closed (reason: "auto_stale")
  → New SUSPECTED incident created

10:00  - OPEN incident created
10:30  - Last detection
...
11:15  - Same pattern detected (45 min gap &gt; 30 min threshold)
         → Old incident CLOSED (auto_stale)
         → New SUSPECTED incident created
</code></pre>
<p>This prevents “zombie incidents” that span unrelated events.</p>
<h2 id="confirmed-only-alerts-v132"><a class="header" href="#confirmed-only-alerts-v132">Confirmed-Only Alerts (v1.3.2)</a></h2>
<p><strong>Important</strong>: Only confirmed anomalies are sent to the web API.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>State</th><th>Sent to Web API?</th><th>Why?</th></tr>
</thead>
<tbody>
<tr><td>SUSPECTED</td><td><strong>No</strong></td><td>Not yet confirmed - might be noise</td></tr>
<tr><td>OPEN</td><td><strong>Yes</strong></td><td>Confirmed - real incident</td></tr>
<tr><td>RECOVERING</td><td><strong>Yes</strong></td><td>Still tracking - might return</td></tr>
<tr><td>CLOSED</td><td><strong>Resolution only</strong></td><td>Final status update</td></tr>
</tbody>
</table>
</div>
<h3 id="why-filter-suspected"><a class="header" href="#why-filter-suspected">Why Filter SUSPECTED?</a></h3>
<p>Before v1.3.2, all detections were sent to the web API. This caused <strong>orphaned incidents</strong>:</p>
<pre><code>OLD BEHAVIOR (problematic):
───────────────────────────
10:00  SUSPECTED detected  →  Sent to API  →  API creates OPEN incident
10:03  Not detected        →  SUSPECTED expires quietly
10:06  Not detected        →  (nothing sent)

Result: API has an OPEN incident that will never resolve
        "Orphaned incident" - no resolution was ever sent
</code></pre>
<pre><code>NEW BEHAVIOR (v1.3.2):
──────────────────────
10:00  SUSPECTED detected  →  NOT sent to API (wait for confirmation)
10:03  Not detected        →  SUSPECTED expires quietly

Result: API never knew about this transient detection
        No orphaned incidents
</code></pre>
<h2 id="summary-table"><a class="header" href="#summary-table">Summary Table</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>State</th><th>Alert Sent?</th><th>Resolution Sent?</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td>SUSPECTED</td><td>No</td><td>No</td><td>Wait for confirmation</td></tr>
<tr><td>OPEN</td><td>Yes</td><td>No</td><td>Active alerting</td></tr>
<tr><td>RECOVERING</td><td>No</td><td>No</td><td>Grace period</td></tr>
<tr><td>CLOSED</td><td>No</td><td>Yes</td><td>Final notification</td></tr>
</tbody>
</table>
</div>
<h2 id="sections-1"><a class="header" href="#sections-1">Sections</a></h2>
<ul>
<li><a href="#state-machine">State Machine</a> - Detailed state transitions and rules</li>
<li><a href="#confirmation-logic">Confirmation Logic</a> - How confirmation works</li>
<li><a href="#fingerprinting">Fingerprinting</a> - How incidents are identified and tracked</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="state-machine"><a class="header" href="#state-machine">State Machine</a></h1>
<p>The incident state machine defines exactly how anomalies transition through the lifecycle. This page provides detailed rules, diagrams, and examples for each transition.</p>
<h2 id="what-is-a-state-machine"><a class="header" href="#what-is-a-state-machine">What is a State Machine?</a></h2>
<p>A <strong>state machine</strong> is a model where:</p>
<ul>
<li>The system is always in exactly one <strong>state</strong></li>
<li><strong>Events</strong> trigger transitions between states</li>
<li><strong>Conditions</strong> determine which transition occurs</li>
<li><strong>Actions</strong> execute when transitions happen</li>
</ul>
<p>For incident tracking:</p>
<ul>
<li><strong>States</strong>: SUSPECTED, OPEN, RECOVERING, CLOSED</li>
<li><strong>Events</strong>: “anomaly detected” or “anomaly not detected”</li>
<li><strong>Conditions</strong>: cycle counts, time gaps</li>
<li><strong>Actions</strong>: send alert, send resolution, update counters</li>
</ul>
<pre><code>┌─────────────────────────────────────────────────────────────────────────┐
│                        STATE MACHINE BASICS                             │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│    ┌─────────┐   Event + Condition    ┌─────────┐                       │
│    │ State A │ ──────────────────────►│ State B │                       │
│    └─────────┘         Action         └─────────┘                       │
│                                                                         │
│    Example:                                                             │
│    ┌──────────────┐  Detected again   ┌──────────────┐                  │
│    │  SUSPECTED   │  consecutive ≥ 2  │     OPEN     │                  │
│    │              │ ─────────────────►│              │                  │
│    └──────────────┘  Send alert       └──────────────┘                  │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
</code></pre>
<h2 id="complete-transition-rules"><a class="header" href="#complete-transition-rules">Complete Transition Rules</a></h2>
<p>This table defines all possible state transitions:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>From State</th><th>Event</th><th>Condition</th><th>To State</th><th>Action</th></tr>
</thead>
<tbody>
<tr><td><em>(none)</em></td><td>Detected</td><td>-</td><td>SUSPECTED</td><td>Create incident</td></tr>
<tr><td>SUSPECTED</td><td>Detected</td><td><code>consecutive &lt; N</code></td><td>SUSPECTED</td><td>Increment <code>consecutive_detections</code></td></tr>
<tr><td>SUSPECTED</td><td>Detected</td><td><code>consecutive ≥ N</code></td><td>OPEN</td><td><strong>Send alert</strong></td></tr>
<tr><td>SUSPECTED</td><td>Not detected</td><td><code>missed &lt; M</code></td><td>SUSPECTED</td><td>Increment <code>missed_cycles</code></td></tr>
<tr><td>SUSPECTED</td><td>Not detected</td><td><code>missed ≥ M</code></td><td>CLOSED</td><td>Silent close (<code>suspected_expired</code>)</td></tr>
<tr><td>OPEN</td><td>Detected</td><td>-</td><td>OPEN</td><td>Continue, reset <code>missed_cycles</code></td></tr>
<tr><td>OPEN</td><td>Not detected</td><td>-</td><td>RECOVERING</td><td>Start grace period</td></tr>
<tr><td>RECOVERING</td><td>Detected</td><td>-</td><td>OPEN</td><td>Resume incident</td></tr>
<tr><td>RECOVERING</td><td>Not detected</td><td><code>missed &lt; M</code></td><td>RECOVERING</td><td>Increment <code>missed_cycles</code></td></tr>
<tr><td>RECOVERING</td><td>Not detected</td><td><code>missed ≥ M</code></td><td>CLOSED</td><td><strong>Send resolution</strong></td></tr>
<tr><td><em>(any)</em></td><td>Detected</td><td><code>gap &gt; threshold</code></td><td>SUSPECTED</td><td>Close stale, create new</td></tr>
</tbody>
</table>
</div>
<p>Where:</p>
<ul>
<li><code>N</code> = <code>confirmation_cycles</code> (default: 2)</li>
<li><code>M</code> = <code>resolution_grace_cycles</code> (default: 3)</li>
<li><code>gap</code> = time since <code>last_updated</code></li>
<li><code>threshold</code> = <code>incident_separation_minutes</code> (default: 30)</li>
</ul>
<h2 id="visual-state-diagram"><a class="header" href="#visual-state-diagram">Visual State Diagram</a></h2>
<pre><code>┌──────────────────────────────────────────────────────────────────────────────┐
│                        INCIDENT STATE MACHINE                                 │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│                                                                              │
│         ┌───────────────────────────────────────────────────────────┐        │
│         │                     NO INCIDENT                           │        │
│         │             (no active tracking for this pattern)         │        │
│         └─────────────────────────┬─────────────────────────────────┘        │
│                                   │                                          │
│                                   │ Anomaly detected                         │
│                                   │ (first detection of pattern)             │
│                                   ▼                                          │
│         ┌───────────────────────────────────────────────────────────┐        │
│         │                     SUSPECTED                             │        │
│         │                                                           │        │
│         │  • incident_action: CREATE                                │        │
│         │  • consecutive_detections: 1                              │        │
│         │  • missed_cycles: 0                                       │        │
│         │  • is_confirmed: false                                    │        │
│         │  • confirmation_pending: true                             │        │
│         │                                                           │        │
│         │  Web API: NOT notified                                    │        │
│         └─────────────────────────┬─────────────────────────────────┘        │
│                                   │                                          │
│              ┌────────────────────┼────────────────────┐                     │
│              │                    │                    │                     │
│              │                    │                    │                     │
│              ▼                    ▼                    ▼                     │
│    ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────────────┐       │
│    │  Stay SUSPECTED │ │      OPEN       │ │        CLOSED           │       │
│    │                 │ │   (CONFIRMED)   │ │   (suspected_expired)   │       │
│    │ Event: Detected │ │                 │ │                         │       │
│    │ Cond: consec&lt;N  │ │ Event: Detected │ │ Event: Not detected     │       │
│    │                 │ │ Cond: consec≥N  │ │ Cond: missed≥M          │       │
│    │ Action:         │ │                 │ │                         │       │
│    │ consec++        │ │ Action:         │ │ Action:                 │       │
│    │                 │ │ SEND ALERT      │ │ Silent close            │       │
│    └─────────────────┘ │ newly_confirmed │ │ No alert ever sent      │       │
│                        │ = true          │ │ No resolution sent      │       │
│                        └────────┬────────┘ └─────────────────────────┘       │
│                                 │                                            │
│                                 │                                            │
│                 ┌───────────────┴───────────────┐                            │
│                 │                               │                            │
│                 │ Detected again                │ Not detected               │
│                 ▼                               ▼                            │
│    ┌──────────────────────┐       ┌──────────────────────┐                   │
│    │        OPEN          │       │     RECOVERING       │                   │
│    │     (continue)       │◄──────│                      │                   │
│    │                      │       │ • missed_cycles: 1+  │                   │
│    │ • Reset missed to 0  │Detect │ • Grace period       │                   │
│    │ • occurrence_count++ │ again │ • Watching for       │                   │
│    │ • Update severity    │       │   anomaly return     │                   │
│    │                      │       │                      │                   │
│    └──────────────────────┘       └──────────┬───────────┘                   │
│                                              │                               │
│                                              │ Not detected                  │
│                                              │ (missed ≥ grace_cycles)       │
│                                              ▼                               │
│                                   ┌──────────────────────┐                   │
│                                   │       CLOSED         │                   │
│                                   │     (resolved)       │                   │
│                                   │                      │                   │
│                                   │ Action:              │                   │
│                                   │ SEND RESOLUTION      │                   │
│                                   │                      │                   │
│                                   │ • resolution_reason: │                   │
│                                   │   "resolved"         │                   │
│                                   │ • Final metrics      │                   │
│                                   │   recorded           │                   │
│                                   └──────────────────────┘                   │
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘
</code></pre>
<h2 id="state-by-state-details"><a class="header" href="#state-by-state-details">State-by-State Details</a></h2>
<h3 id="no-incident--suspected"><a class="header" href="#no-incident--suspected">NO INCIDENT → SUSPECTED</a></h3>
<p><strong>Trigger</strong>: First detection of an anomaly pattern</p>
<pre><code>Before:   No active incident for this fingerprint
Event:    ML detects anomaly_latency_spike_recent
After:    SUSPECTED incident created

┌─────────────────────────────────────────────────────────────────────┐
│  Transition: NO INCIDENT → SUSPECTED                               │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  What happens:                                                      │
│    1. Generate fingerprint_id from pattern content                  │
│    2. Check database - no active incident exists                    │
│    3. Create new incident record                                    │
│    4. Set status = SUSPECTED                                        │
│    5. Initialize counters                                           │
│                                                                     │
│  Payload fields set:                                                │
│    fingerprint_id: "anomaly_abc123"  (deterministic hash)           │
│    incident_id: "incident_xyz789"    (new UUID)                     │
│    fingerprint_action: "CREATE"                                     │
│    incident_action: "CREATE"                                        │
│    status: "SUSPECTED"                                              │
│    consecutive_detections: 1                                        │
│    missed_cycles: 0                                                 │
│    occurrence_count: 1                                              │
│    first_seen: &lt;current timestamp&gt;                                  │
│    is_confirmed: false                                              │
│    confirmation_pending: true                                       │
│    cycles_to_confirm: 1                                             │
│                                                                     │
│  Web API: NOT notified (filtered out before sending)                │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="suspected--suspected-still-waiting"><a class="header" href="#suspected--suspected-still-waiting">SUSPECTED → SUSPECTED (Still Waiting)</a></h3>
<p><strong>Trigger</strong>: Anomaly detected again, but not enough cycles yet</p>
<pre><code>Before:   SUSPECTED with consecutive_detections = 1
Event:    Same anomaly detected
Cond:     consecutive_detections &lt; confirmation_cycles
After:    SUSPECTED with consecutive_detections = 2

(With default confirmation_cycles=2, this transition goes to OPEN instead)
</code></pre>
<h3 id="suspected--open-confirmed"><a class="header" href="#suspected--open-confirmed">SUSPECTED → OPEN (Confirmed!)</a></h3>
<p><strong>Trigger</strong>: Anomaly confirmed after N consecutive detections</p>
<pre><code>Before:   SUSPECTED with consecutive_detections = 1
Event:    Same anomaly detected again
Cond:     consecutive_detections ≥ confirmation_cycles (default: 2)
After:    OPEN - alert sent!

┌─────────────────────────────────────────────────────────────────────┐
│  Transition: SUSPECTED → OPEN                                      │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  This is the KEY transition - the moment an alert fires             │
│                                                                     │
│  What happens:                                                      │
│    1. Anomaly detected for 2nd consecutive cycle                    │
│    2. Threshold met: consecutive ≥ confirmation_cycles              │
│    3. Status changes: SUSPECTED → OPEN                              │
│    4. newly_confirmed flag set to true (this cycle only)            │
│    5. Alert sent to web API                                         │
│                                                                     │
│  Payload fields change:                                             │
│    status: "SUSPECTED" → "OPEN"                                     │
│    previous_status: "SUSPECTED"                                     │
│    is_confirmed: false → true                                       │
│    confirmation_pending: true → false                               │
│    newly_confirmed: true  ← Important for tracking!                 │
│    consecutive_detections: 2                                        │
│    occurrence_count: 2                                              │
│                                                                     │
│  Web API: Alert SENT (now included in payload)                      │
│                                                                     │
│  Fingerprinting summary:                                            │
│    overall_action: "CONFIRMED"                                      │
│    newly_confirmed_incidents: [this incident]                       │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="suspected--closed-expired-quietly"><a class="header" href="#suspected--closed-expired-quietly">SUSPECTED → CLOSED (Expired Quietly)</a></h3>
<p><strong>Trigger</strong>: SUSPECTED anomaly not confirmed before grace period ends</p>
<pre><code>Before:   SUSPECTED with missed_cycles = 2
Event:    Anomaly NOT detected
Cond:     missed_cycles ≥ resolution_grace_cycles (default: 3)
After:    CLOSED with reason "suspected_expired"

┌─────────────────────────────────────────────────────────────────────┐
│  Transition: SUSPECTED → CLOSED (suspected_expired)                │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  This is a SILENT close - no alert was ever sent                    │
│                                                                     │
│  What happens:                                                      │
│    1. Anomaly detected once (SUSPECTED)                             │
│    2. Not detected in next 3 cycles                                 │
│    3. Incident closes without ever alerting                         │
│    4. No resolution sent to web API                                 │
│                                                                     │
│  Why no resolution?                                                 │
│    Web API was never notified of this incident.                     │
│    Sending a resolution would be confusing.                         │
│                                                                     │
│  Timeline example:                                                  │
│    10:00  Detected → SUSPECTED (not sent to API)                    │
│    10:03  Not detected → missed: 1                                  │
│    10:06  Not detected → missed: 2                                  │
│    10:09  Not detected → CLOSED (suspected_expired)                 │
│                                                                     │
│  Result: A transient spike that no one ever knew about              │
│          This is the desired behavior for noise reduction           │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="open--open-continue"><a class="header" href="#open--open-continue">OPEN → OPEN (Continue)</a></h3>
<p><strong>Trigger</strong>: Anomaly continues to be detected</p>
<pre><code>Before:   OPEN with occurrence_count = 5
Event:    Same anomaly detected
After:    OPEN with occurrence_count = 6

┌─────────────────────────────────────────────────────────────────────┐
│  Transition: OPEN → OPEN (continue)                                │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  What happens:                                                      │
│    1. Anomaly still detected                                        │
│    2. Counters updated                                              │
│    3. missed_cycles reset to 0 (not in grace period)                │
│    4. Duration updated                                              │
│                                                                     │
│  Payload fields update:                                             │
│    incident_action: "CONTINUE"                                      │
│    occurrence_count: ++                                             │
│    consecutive_detections: ++                                       │
│    missed_cycles: 0 (reset)                                         │
│    incident_duration_minutes: updated                               │
│    last_updated: &lt;current timestamp&gt;                                │
│                                                                     │
│  Severity can change:                                               │
│    If anomaly severity changes (e.g., high → critical):             │
│    severity: "high" → "critical"                                    │
│    severity_changed: true                                           │
│    previous_severity: "high"                                        │
│                                                                     │
│  Web API: Continued alerting                                        │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="open--recovering"><a class="header" href="#open--recovering">OPEN → RECOVERING</a></h3>
<p><strong>Trigger</strong>: Anomaly not detected, entering grace period</p>
<pre><code>Before:   OPEN
Event:    Anomaly NOT detected
After:    RECOVERING with missed_cycles = 1

┌─────────────────────────────────────────────────────────────────────┐
│  Transition: OPEN → RECOVERING                                     │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  What happens:                                                      │
│    1. Anomaly not detected this cycle                               │
│    2. Enter grace period (don't close yet!)                         │
│    3. Watch for anomaly return                                      │
│                                                                     │
│  Why grace period?                                                  │
│    Anomalies often briefly clear before returning:                  │
│    • Brief improvement in latency                                   │
│    • One good measurement among bad                                 │
│    • Flapping behavior                                              │
│                                                                     │
│  Payload fields:                                                    │
│    status: "RECOVERING"                                             │
│    missed_cycles: 1                                                 │
│                                                                     │
│  The fingerprinting summary shows:                                  │
│    status_summary.recovering: 1                                     │
│                                                                     │
│  Web API: No resolution yet                                         │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="recovering--open-anomaly-returns"><a class="header" href="#recovering--open-anomaly-returns">RECOVERING → OPEN (Anomaly Returns)</a></h3>
<p><strong>Trigger</strong>: Anomaly detected again during grace period</p>
<pre><code>Before:   RECOVERING with missed_cycles = 2
Event:    Anomaly detected again
After:    OPEN - back to active alerting

┌─────────────────────────────────────────────────────────────────────┐
│  Transition: RECOVERING → OPEN                                     │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  This is why grace periods exist!                                   │
│                                                                     │
│  What happens:                                                      │
│    1. Anomaly returns before grace period ends                      │
│    2. Resume tracking same incident                                 │
│    3. Reset missed_cycles to 0                                      │
│    4. Increment occurrence_count                                    │
│                                                                     │
│  Without grace period:                                              │
│    - Would have resolved after first miss                           │
│    - Would create NEW incident when anomaly returned                │
│    - Same issue tracked as multiple incidents                       │
│                                                                     │
│  With grace period:                                                 │
│    - Brief clearance doesn't trigger resolution                     │
│    - Same incident continues when anomaly returns                   │
│    - Accurate duration tracking                                     │
│                                                                     │
│  Payload fields:                                                    │
│    status: "RECOVERING" → "OPEN"                                    │
│    missed_cycles: 2 → 0                                             │
│    occurrence_count: ++                                             │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="recovering--closed-resolved"><a class="header" href="#recovering--closed-resolved">RECOVERING → CLOSED (Resolved)</a></h3>
<p><strong>Trigger</strong>: Grace period completed without anomaly return</p>
<pre><code>Before:   RECOVERING with missed_cycles = 2
Event:    Anomaly NOT detected
Cond:     missed_cycles ≥ resolution_grace_cycles (default: 3)
After:    CLOSED with reason "resolved"

┌─────────────────────────────────────────────────────────────────────┐
│  Transition: RECOVERING → CLOSED                                   │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  What happens:                                                      │
│    1. Anomaly not detected for 3+ consecutive cycles                │
│    2. Grace period completed                                        │
│    3. Incident closed                                               │
│    4. Resolution sent to web API                                    │
│                                                                     │
│  Resolution payload:                                                │
│    fingerprint_id: "anomaly_abc123"                                 │
│    incident_id: "incident_xyz789"                                   │
│    fingerprint_action: "RESOLVE"                                    │
│    incident_action: "CLOSE"                                         │
│    resolution_reason: "resolved"                                    │
│    final_severity: "high"                                           │
│    resolved_at: &lt;current timestamp&gt;                                 │
│    total_occurrences: 8                                             │
│    incident_duration_minutes: 45                                    │
│    first_seen: &lt;original timestamp&gt;                                 │
│                                                                     │
│  Web API: Resolution SENT                                           │
│                                                                     │
│  Fingerprinting summary:                                            │
│    overall_action: "RESOLVE"                                        │
│    resolved_incidents: [this incident]                              │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="staleness-any--closed--suspected"><a class="header" href="#staleness-any--closed--suspected">Staleness: Any → CLOSED + SUSPECTED</a></h3>
<p><strong>Trigger</strong>: Same pattern detected after long gap</p>
<pre><code>Before:   OPEN (or RECOVERING) last updated 45 minutes ago
Event:    Anomaly detected
Cond:     gap &gt; incident_separation_minutes (default: 30)
After:    Old incident CLOSED (auto_stale) + New SUSPECTED created

┌─────────────────────────────────────────────────────────────────────┐
│  Staleness Transition                                              │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  What happens:                                                      │
│    1. Same anomaly pattern detected                                 │
│    2. But time gap exceeds threshold (30 min default)               │
│    3. Old incident auto-closed                                      │
│    4. New incident created from scratch                             │
│                                                                     │
│  Why?                                                               │
│    Long gaps usually indicate separate occurrences:                 │
│    • Incident at 10:00, resolved by 10:30                           │
│    • Same pattern at 15:00 - probably new issue                     │
│    • Should be tracked separately                                   │
│                                                                     │
│  Two things happen in same cycle:                                   │
│                                                                     │
│  1. Old incident closed:                                            │
│     resolved_incidents: [{                                          │
│       incident_id: "incident_old",                                  │
│       resolution_reason: "auto_stale",                              │
│       incident_duration_minutes: 30                                 │
│     }]                                                              │
│                                                                     │
│  2. New incident created:                                           │
│     anomalies: {                                                    │
│       "latency_spike_recent": {                                     │
│         fingerprint_id: "anomaly_abc123",  (same pattern)           │
│         incident_id: "incident_new",       (new UUID)               │
│         status: "SUSPECTED"                                         │
│       }                                                             │
│     }                                                               │
│                                                                     │
│  Web API receives:                                                  │
│    - Resolution for old incident (auto_stale)                       │
│    - NOT the new SUSPECTED (filtered)                               │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h2 id="payload-fields-by-state"><a class="header" href="#payload-fields-by-state">Payload Fields by State</a></h2>
<h3 id="suspected-state"><a class="header" href="#suspected-state">SUSPECTED State</a></h3>
<pre><code class="language-json">{
  "fingerprint_id": "anomaly_d18f6ae2bf62",
  "incident_id": "incident_31e9e23d4b2b",
  "fingerprint_action": "CREATE",
  "incident_action": "CREATE",
  "status": "SUSPECTED",
  "consecutive_detections": 1,
  "missed_cycles": 0,
  "occurrence_count": 1,
  "first_seen": "2025-12-17T13:56:06.028585",
  "last_updated": "2025-12-17T13:56:06.028585",
  "incident_duration_minutes": 0,
  "confirmation_pending": true,
  "cycles_to_confirm": 1,
  "is_confirmed": false,
  "newly_confirmed": false
}
</code></pre>
<h3 id="open-state-newly-confirmed"><a class="header" href="#open-state-newly-confirmed">OPEN State (Newly Confirmed)</a></h3>
<pre><code class="language-json">{
  "fingerprint_id": "anomaly_d18f6ae2bf62",
  "incident_id": "incident_31e9e23d4b2b",
  "fingerprint_action": "UPDATE",
  "incident_action": "CONTINUE",
  "status": "OPEN",
  "previous_status": "SUSPECTED",
  "consecutive_detections": 2,
  "missed_cycles": 0,
  "occurrence_count": 2,
  "first_seen": "2025-12-17T13:56:06.028585",
  "last_updated": "2025-12-17T13:59:06.028585",
  "incident_duration_minutes": 3,
  "confirmation_pending": false,
  "is_confirmed": true,
  "newly_confirmed": true
}
</code></pre>
<h3 id="open-state-continuing"><a class="header" href="#open-state-continuing">OPEN State (Continuing)</a></h3>
<pre><code class="language-json">{
  "fingerprint_id": "anomaly_d18f6ae2bf62",
  "incident_id": "incident_31e9e23d4b2b",
  "fingerprint_action": "UPDATE",
  "incident_action": "CONTINUE",
  "status": "OPEN",
  "consecutive_detections": 5,
  "missed_cycles": 0,
  "occurrence_count": 5,
  "first_seen": "2025-12-17T13:56:06.028585",
  "last_updated": "2025-12-17T14:08:06.028585",
  "incident_duration_minutes": 12,
  "is_confirmed": true,
  "newly_confirmed": false
}
</code></pre>
<h3 id="recovering-state"><a class="header" href="#recovering-state">RECOVERING State</a></h3>
<p>Note: RECOVERING incidents appear in <code>status_summary.recovering</code> count but individual anomalies are not in the payload (not detected this cycle).</p>
<p>Fingerprinting summary shows:</p>
<pre><code class="language-json">{
  "fingerprinting": {
    "overall_action": "NO_CHANGE",
    "status_summary": {
      "suspected": 0,
      "confirmed": 0,
      "recovering": 1
    }
  }
}
</code></pre>
<h3 id="closed-state-resolution"><a class="header" href="#closed-state-resolution">CLOSED State (Resolution)</a></h3>
<pre><code class="language-json">{
  "resolved_incidents": [
    {
      "fingerprint_id": "anomaly_d18f6ae2bf62",
      "incident_id": "incident_31e9e23d4b2b",
      "anomaly_name": "latency_spike_recent",
      "fingerprint_action": "RESOLVE",
      "incident_action": "CLOSE",
      "final_severity": "high",
      "resolved_at": "2025-12-17T14:30:00.000000",
      "total_occurrences": 8,
      "incident_duration_minutes": 34,
      "first_seen": "2025-12-17T13:56:06.028585",
      "service_name": "booking",
      "last_detected_by_model": "business_hours",
      "resolution_reason": "resolved"
    }
  ]
}
</code></pre>
<h2 id="resolution-reasons"><a class="header" href="#resolution-reasons">Resolution Reasons</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Reason</th><th>When</th><th>Alert Sent?</th><th>Resolution Sent?</th></tr>
</thead>
<tbody>
<tr><td><code>resolved</code></td><td>Grace period completed</td><td>Yes (earlier)</td><td>Yes</td></tr>
<tr><td><code>suspected_expired</code></td><td>SUSPECTED never confirmed</td><td>No</td><td>No</td></tr>
<tr><td><code>auto_stale</code></td><td>Time gap exceeded threshold</td><td>Yes (earlier)</td><td>Yes</td></tr>
</tbody>
</table>
</div>
<h2 id="configuration-2"><a class="header" href="#configuration-2">Configuration</a></h2>
<pre><code class="language-json">{
  "fingerprinting": {
    "confirmation_cycles": 2,
    "resolution_grace_cycles": 3,
    "incident_separation_minutes": 30,
    "cleanup_max_age_hours": 72
  }
}
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Parameter</th><th>Default</th><th>Range</th><th>Impact</th></tr>
</thead>
<tbody>
<tr><td><code>confirmation_cycles</code></td><td>2</td><td>1-10</td><td>Higher = fewer false positives, more detection delay</td></tr>
<tr><td><code>resolution_grace_cycles</code></td><td>3</td><td>1-10</td><td>Higher = fewer flapping alerts, longer resolution time</td></tr>
<tr><td><code>incident_separation_minutes</code></td><td>30</td><td>5-1440</td><td>Higher = more likely to continue existing incident</td></tr>
<tr><td><code>cleanup_max_age_hours</code></td><td>72</td><td>1-720</td><td>How long closed incidents stay in database</td></tr>
</tbody>
</table>
</div>
<h2 id="complete-example-full-lifecycle"><a class="header" href="#complete-example-full-lifecycle">Complete Example: Full Lifecycle</a></h2>
<pre><code>Time     Event           State          Actions
─────────────────────────────────────────────────────────────────────
10:00    Detected        SUSPECTED      Create incident_abc, no alert
10:03    Detected        OPEN           ALERT SENT! (confirmed)
10:06    Detected        OPEN           Continue tracking
10:09    Detected        OPEN           Continue tracking (count: 4)
10:12    Not detected    RECOVERING     Grace period starts (missed: 1)
10:15    Detected        OPEN           Resume incident (missed: 0)
10:18    Detected        OPEN           Continue tracking
10:21    Not detected    RECOVERING     Grace period (missed: 1)
10:24    Not detected    RECOVERING     Grace period (missed: 2)
10:27    Not detected    CLOSED         RESOLUTION SENT! (resolved)

Result:
  - 1 alert sent (at 10:03)
  - 1 resolution sent (at 10:27)
  - Duration: 27 minutes
  - Occurrences: 6
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="confirmation-logic"><a class="header" href="#confirmation-logic">Confirmation Logic</a></h1>
<p>Confirmation prevents alert noise by requiring multiple consecutive detections before alerting. This chapter explains why confirmation is essential, how it works, and how to tune it for your environment.</p>
<h2 id="what-is-confirmation"><a class="header" href="#what-is-confirmation">What is Confirmation?</a></h2>
<p><strong>Confirmation</strong> is the process of validating that an anomaly is real and persistent before triggering an alert. Instead of alerting immediately when something looks wrong, the system waits to see if the issue persists across multiple detection cycles.</p>
<pre><code>Without Confirmation:           With Confirmation:

Detection 1 → ALERT!           Detection 1 → Wait...
(might be noise)               (could be transient)

Detection 2 → ALERT!           Detection 2 → ALERT!
(might be same issue)          (confirmed: real issue)
</code></pre>
<p>Think of it like a smoke detector: you don’t want it to alarm for every wisp of steam from a shower. You want it to confirm there’s actual smoke before waking everyone up.</p>
<h2 id="why-confirmation-matters"><a class="header" href="#why-confirmation-matters">Why Confirmation Matters</a></h2>
<h3 id="the-problem-alert-fatigue"><a class="header" href="#the-problem-alert-fatigue">The Problem: Alert Fatigue</a></h3>
<p>In production environments, metrics naturally fluctuate. A latency spike might last 30 seconds then disappear. An error rate might briefly increase during a garbage collection pause. Without confirmation, every transient blip becomes an alert.</p>
<p><strong>Without confirmation, operators experience:</strong></p>
<ul>
<li>Multiple alerts per hour for transient issues</li>
<li>“Resolved” notifications seconds after alerts</li>
<li>Loss of trust in the alerting system</li>
<li>Critical alerts buried in noise</li>
</ul>
<h3 id="real-world-example"><a class="header" href="#real-world-example">Real-World Example</a></h3>
<p>Consider a service during normal operation:</p>
<pre><code>TIME      LATENCY   WITHOUT CONFIRMATION    WITH CONFIRMATION
────────────────────────────────────────────────────────────────
10:00     120ms     Normal                  Normal
10:03     450ms     🔴 ALERT: Latency!      ⏳ SUSPECTED (1/2)
10:06     125ms     🟢 RESOLVED             ⏳ Expires (no alert)
10:09     448ms     🔴 ALERT: Latency!      ⏳ SUSPECTED (1/2)
10:12     455ms     (still alerting)        🔴 CONFIRMED (2/2)
10:15     460ms     (still alerting)        📊 Continue tracking
10:18     130ms     🟢 RESOLVED             ⏳ Recovering (1/3)
10:21     128ms     Normal                  ⏳ Recovering (2/3)
10:24     125ms     Normal                  🟢 RESOLVED
</code></pre>
<p><strong>Results:</strong></p>
<ul>
<li>Without confirmation: 2 alerts, 2 resolutions (noisy)</li>
<li>With confirmation: 1 alert, 1 resolution (accurate)</li>
</ul>
<p>The confirmed alert represents the real issue (10:09-10:15), while the transient spike at 10:03 was correctly filtered out.</p>
<h2 id="how-confirmation-works"><a class="header" href="#how-confirmation-works">How Confirmation Works</a></h2>
<h3 id="the-confirmation-counter"><a class="header" href="#the-confirmation-counter">The Confirmation Counter</a></h3>
<p>Each incident tracks how many consecutive cycles it has been detected:</p>
<pre><code>┌────────────────────────────────────────────────────────────────────┐
│                    consecutive_detections Counter                   │
├────────────────────────────────────────────────────────────────────┤
│                                                                    │
│   Cycle 1: Anomaly detected                                        │
│   ┌────────────────────────────────────────────────────────────┐   │
│   │  consecutive_detections: 1                                 │   │
│   │  confirmation_cycles: 2  (configured)                      │   │
│   │  cycles_to_confirm: 1   (remaining)                        │   │
│   │                                                            │   │
│   │  Is 1 &gt;= 2? No → Stay SUSPECTED                           │   │
│   └────────────────────────────────────────────────────────────┘   │
│                                                                    │
│   Cycle 2: Anomaly detected again                                  │
│   ┌────────────────────────────────────────────────────────────┐   │
│   │  consecutive_detections: 2                                 │   │
│   │  confirmation_cycles: 2  (configured)                      │   │
│   │  cycles_to_confirm: 0   (remaining)                        │   │
│   │                                                            │   │
│   │  Is 2 &gt;= 2? Yes → Transition to OPEN                      │   │
│   └────────────────────────────────────────────────────────────┘   │
│                                                                    │
└────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="confirmation-flow-diagram"><a class="header" href="#confirmation-flow-diagram">Confirmation Flow Diagram</a></h3>
<pre><code>                    ┌─────────────────────┐
                    │  Anomaly Detected   │
                    └──────────┬──────────┘
                               │
                               ▼
                    ┌─────────────────────┐
              ┌─────│ Active Incident?    │─────┐
              │     └─────────────────────┘     │
              │NO                               │YES
              ▼                                 ▼
    ┌─────────────────────┐           ┌─────────────────────┐
    │  Create SUSPECTED   │           │  What's the status? │
    │  consecutive = 1    │           └──────────┬──────────┘
    └──────────┬──────────┘                      │
               │                    ┌────────────┼────────────┐
               ▼                    │            │            │
    ┌─────────────────────┐    SUSPECTED       OPEN      RECOVERING
    │  Wait for next      │         │            │            │
    │  detection cycle    │         ▼            ▼            ▼
    └─────────────────────┘    Increment     Continue     Resume to
                               counter       tracking       OPEN
                                   │
                                   ▼
                          ┌─────────────────────┐
                          │ consecutive &gt;=      │
                          │ confirmation_cycles?│
                          └──────────┬──────────┘
                               │           │
                              YES          NO
                               │           │
                               ▼           ▼
                    ┌─────────────────┐  ┌─────────────────┐
                    │  CONFIRMED!     │  │  Still waiting  │
                    │  Status → OPEN  │  │  Stay SUSPECTED │
                    │  Send Alert     │  │  No alert       │
                    └─────────────────┘  └─────────────────┘
</code></pre>
<h2 id="detection-cycles-explained"><a class="header" href="#detection-cycles-explained">Detection Cycles Explained</a></h2>
<h3 id="cycle-1-first-detection-suspected"><a class="header" href="#cycle-1-first-detection-suspected">Cycle 1: First Detection (SUSPECTED)</a></h3>
<p>When an anomaly is first detected, it enters the SUSPECTED state:</p>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│                     Detection Cycle 1                                │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│   Time: 10:00:00                                                    │
│   Event: Latency spike detected (450ms, normally 120ms)             │
│                                                                     │
│   Actions taken:                                                    │
│   ─────────────                                                     │
│   1. Generate fingerprint_id from pattern content                   │
│   2. Check database for active incident with this fingerprint       │
│   3. No active incident found → Create new incident                 │
│   4. Set status = SUSPECTED                                         │
│                                                                     │
│   Result:                                                           │
│   ┌─────────────────────────────────────────────────────────────┐   │
│   │  fingerprint_id: anomaly_8d4a011b83ca                       │   │
│   │  incident_id: incident_1dcbafc91480                         │   │
│   │  status: SUSPECTED                                          │   │
│   │  consecutive_detections: 1                                  │   │
│   │  confirmation_pending: true                                 │   │
│   │  cycles_to_confirm: 1                                       │   │
│   │  is_confirmed: false                                        │   │
│   └─────────────────────────────────────────────────────────────┘   │
│                                                                     │
│   Web API: ❌ NOT notified                                          │
│   Dashboard: ❌ No alert displayed                                  │
│   Reason: Waiting for confirmation                                  │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="cycle-2-confirmation-suspected--open"><a class="header" href="#cycle-2-confirmation-suspected--open">Cycle 2: Confirmation (SUSPECTED → OPEN)</a></h3>
<p>If the same anomaly is detected again in the next cycle:</p>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│                     Detection Cycle 2                                │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│   Time: 10:03:00 (3 minutes later)                                  │
│   Event: Same latency pattern detected again                        │
│                                                                     │
│   Actions taken:                                                    │
│   ─────────────                                                     │
│   1. Generate fingerprint_id from pattern content                   │
│   2. Check database → Found existing SUSPECTED incident             │
│   3. Increment consecutive_detections: 1 → 2                        │
│   4. Check: 2 &gt;= confirmation_cycles (2)? YES!                      │
│   5. Transition status: SUSPECTED → OPEN                            │
│   6. Set newly_confirmed = true                                     │
│   7. Send alert to Web API                                          │
│                                                                     │
│   Result:                                                           │
│   ┌─────────────────────────────────────────────────────────────┐   │
│   │  fingerprint_id: anomaly_8d4a011b83ca                       │   │
│   │  incident_id: incident_1dcbafc91480                         │   │
│   │  status: OPEN                                               │   │
│   │  previous_status: SUSPECTED  ← For tracking the transition  │   │
│   │  consecutive_detections: 2                                  │   │
│   │  confirmation_pending: false                                │   │
│   │  is_confirmed: true                                         │   │
│   │  newly_confirmed: true  ← Signals this is fresh confirmation│   │
│   └─────────────────────────────────────────────────────────────┘   │
│                                                                     │
│   Web API: ✅ Alert sent!                                           │
│   Dashboard: ✅ Alert displayed to operators                        │
│   Reason: Confirmed after 2 consecutive detections                  │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="the-newly_confirmed-flag"><a class="header" href="#the-newly_confirmed-flag">The <code>newly_confirmed</code> Flag</a></h3>
<p>The <code>newly_confirmed</code> flag is crucial for downstream consumers:</p>
<pre><code>When newly_confirmed = true:
─────────────────────────────
• This is the FIRST time this incident is being sent as confirmed
• Web UI should create a new alert entry
• Notification systems should send alerts
• Only set on the exact cycle of confirmation

When newly_confirmed = false:
──────────────────────────────
• Incident was already confirmed in a previous cycle
• Web UI should update existing alert entry
• Notification systems should NOT re-alert
• Set for all subsequent cycles
</code></pre>
<h2 id="key-fields-reference"><a class="header" href="#key-fields-reference">Key Fields Reference</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>consecutive_detections</code></td><td>integer</td><td>How many cycles in a row this anomaly was detected</td></tr>
<tr><td><code>confirmation_pending</code></td><td>boolean</td><td><code>true</code> while still in SUSPECTED state</td></tr>
<tr><td><code>cycles_to_confirm</code></td><td>integer</td><td>Remaining cycles needed for confirmation (0 when confirmed)</td></tr>
<tr><td><code>is_confirmed</code></td><td>boolean</td><td><code>true</code> once status becomes OPEN</td></tr>
<tr><td><code>newly_confirmed</code></td><td>boolean</td><td><code>true</code> only on the exact cycle where SUSPECTED → OPEN</td></tr>
<tr><td><code>previous_status</code></td><td>string</td><td>What the status was before this cycle (for tracking transitions)</td></tr>
</tbody>
</table>
</div>
<h2 id="fingerprinting-summary-object"><a class="header" href="#fingerprinting-summary-object">Fingerprinting Summary Object</a></h2>
<p>The top-level <code>fingerprinting</code> object provides a summary of all confirmation activity:</p>
<pre><code class="language-json">{
  "fingerprinting": {
    "service_name": "booking",
    "model_name": "business_hours",
    "timestamp": "2025-12-17T10:03:00",
    "overall_action": "CONFIRMED",

    "status_summary": {
      "suspected": 0,
      "confirmed": 1,
      "recovering": 0
    },

    "action_summary": {
      "incident_creates": 0,
      "incident_continues": 0,
      "incident_closes": 0,
      "newly_confirmed": 1
    },

    "newly_confirmed_incidents": [
      {
        "fingerprint_id": "anomaly_8d4a011b83ca",
        "incident_id": "incident_1dcbafc91480",
        "anomaly_name": "latency_spike_recent",
        "severity": "high"
      }
    ]
  }
}
</code></pre>
<h3 id="overall-action-values"><a class="header" href="#overall-action-values">Overall Action Values</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Action</th><th>Meaning</th><th>When It Happens</th></tr>
</thead>
<tbody>
<tr><td><code>CREATE</code></td><td>New incident(s) created in SUSPECTED state</td><td>First detection of new anomaly patterns</td></tr>
<tr><td><code>CONFIRMED</code></td><td>Incident(s) transitioned SUSPECTED → OPEN</td><td>Anomaly detected for confirmation_cycles times</td></tr>
<tr><td><code>UPDATE</code></td><td>Existing OPEN incident(s) continued</td><td>Ongoing anomaly still being detected</td></tr>
<tr><td><code>RESOLVE</code></td><td>Incident(s) closed</td><td>Grace period exceeded without detection</td></tr>
<tr><td><code>MIXED</code></td><td>Multiple different actions in one cycle</td><td>E.g., one confirmed while another closes</td></tr>
<tr><td><code>NO_CHANGE</code></td><td>No significant state changes</td><td>Only RECOVERING incidents still waiting</td></tr>
</tbody>
</table>
</div>
<h2 id="web-api-integration"><a class="header" href="#web-api-integration">Web API Integration</a></h2>
<h3 id="confirmed-only-alerts-v132-1"><a class="header" href="#confirmed-only-alerts-v132-1">Confirmed-Only Alerts (v1.3.2)</a></h3>
<p>Starting with version 1.3.2, only <strong>confirmed</strong> anomalies are sent to the web API. This is a critical feature for preventing orphaned incidents.</p>
<pre><code class="language-python"># How the inference engine filters before sending to web API

def process_results(anomalies):
    # Filter to only confirmed anomalies
    confirmed_anomalies = {
        name: anomaly for name, anomaly in anomalies.items()
        if anomaly.get('is_confirmed', False) or
           anomaly.get('status') in ('OPEN', 'RECOVERING')
    }

    if confirmed_anomalies:
        # Send only confirmed anomalies to web API
        send_alert(confirmed_anomalies)
    else:
        # SUSPECTED anomalies are NOT sent
        # This prevents orphaned incidents
        pass
</code></pre>
<h3 id="why-this-matters"><a class="header" href="#why-this-matters">Why This Matters</a></h3>
<p>Before v1.3.2, all anomalies (including SUSPECTED) were sent to the web API. This caused problems:</p>
<pre><code>OLD BEHAVIOR (problematic):
───────────────────────────
10:00  Detection → SUSPECTED → Sent to Web API → Web API creates OPEN incident
10:03  Not detected → SUSPECTED expires → No resolution sent (suspected_expired)
10:06  ...
Result: Orphaned OPEN incident in Web API that never gets resolved!

NEW BEHAVIOR (correct):
───────────────────────────
10:00  Detection → SUSPECTED → NOT sent to Web API (waiting for confirmation)
10:03  Not detected → SUSPECTED expires → Nothing to resolve (never sent)
10:06  ...
Result: No orphaned incident - Web API never knew about it!

OR if it gets confirmed:

10:00  Detection → SUSPECTED → NOT sent to Web API
10:03  Detection → OPEN (confirmed) → Sent to Web API → Web API creates incident
10:06  Detection → OPEN continues → Update sent
...
10:15  Not detected × 3 cycles → CLOSED → Resolution sent
Result: Complete lifecycle - incident created when confirmed, resolved when cleared
</code></pre>
<h2 id="suspected-expiration"><a class="header" href="#suspected-expiration">SUSPECTED Expiration</a></h2>
<p>When an anomaly is detected but then disappears before confirmation:</p>
<pre><code>Timeline of SUSPECTED Expiration:
─────────────────────────────────

10:00:00  Anomaly detected
          ├─ Status: SUSPECTED
          ├─ consecutive_detections: 1
          └─ Web API: NOT notified

10:03:00  Anomaly NOT detected
          ├─ Status: still SUSPECTED
          ├─ missed_cycles: 1
          └─ Web API: still not notified

10:06:00  Anomaly NOT detected
          ├─ Status: still SUSPECTED
          ├─ missed_cycles: 2
          └─ Web API: still not notified

10:09:00  Anomaly NOT detected
          ├─ missed_cycles: 3 &gt;= resolution_grace_cycles
          ├─ Status: SUSPECTED → CLOSED
          ├─ resolution_reason: "suspected_expired"
          ├─ Web API: NO resolution sent (never was an incident there)
          └─ Incident removed from tracking

Result:
─────────
• No alert was ever sent
• No resolution is needed
• Transient issue correctly filtered
• Zero noise in the alerting system
</code></pre>
<h3 id="resolution-reasons-for-suspected"><a class="header" href="#resolution-reasons-for-suspected">Resolution Reasons for SUSPECTED</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Reason</th><th>Description</th><th>Web API Impact</th></tr>
</thead>
<tbody>
<tr><td><code>suspected_expired</code></td><td>Never confirmed, disappeared before confirmation</td><td>Nothing sent (no orphan)</td></tr>
<tr><td><code>resolved</code></td><td>Normal resolution after grace period</td><td>N/A (only applies to OPEN)</td></tr>
<tr><td><code>auto_stale</code></td><td>Time gap exceeded threshold</td><td>N/A (only applies to OPEN)</td></tr>
</tbody>
</table>
</div>
<h2 id="tuning-confirmation-cycles"><a class="header" href="#tuning-confirmation-cycles">Tuning Confirmation Cycles</a></h2>
<p>The <code>confirmation_cycles</code> configuration determines how many consecutive detections are required:</p>
<pre><code class="language-json">{
  "fingerprinting": {
    "confirmation_cycles": 2
  }
}
</code></pre>
<h3 id="trade-off-analysis"><a class="header" href="#trade-off-analysis">Trade-off Analysis</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Value</th><th>Confirmation Time*</th><th>Pros</th><th>Cons</th></tr>
</thead>
<tbody>
<tr><td>1</td><td>Immediate</td><td>Fastest response</td><td>No filtering, all noise</td></tr>
<tr><td><strong>2</strong></td><td>~4-6 min</td><td><strong>Good balance (default)</strong></td><td>1 cycle delay</td></tr>
<tr><td>3</td><td>~6-9 min</td><td>Fewer false positives</td><td>May miss short incidents</td></tr>
<tr><td>4</td><td>~8-12 min</td><td>Very strict filtering</td><td>Risk of missing real issues</td></tr>
<tr><td>5+</td><td>10+ min</td><td>Maximum noise reduction</td><td>Likely too slow for production</td></tr>
</tbody>
</table>
</div>
<p>*Assuming 2-3 minute detection cycles</p>
<h3 id="choosing-the-right-value"><a class="header" href="#choosing-the-right-value">Choosing the Right Value</a></h3>
<p><strong>Use <code>confirmation_cycles: 1</code></strong> when:</p>
<ul>
<li>Testing or debugging the system</li>
<li>You need immediate alerts regardless of noise</li>
<li>You have other mechanisms to filter alerts downstream</li>
</ul>
<p><strong>Use <code>confirmation_cycles: 2</code></strong> (default) when:</p>
<ul>
<li>Running in production</li>
<li>You want balanced noise reduction</li>
<li>Detection cycle is 2-5 minutes</li>
</ul>
<p><strong>Use <code>confirmation_cycles: 3</code></strong> when:</p>
<ul>
<li>You have a noisy environment with frequent transient issues</li>
<li>False positives are more costly than delayed detection</li>
<li>You can tolerate 6-9 minute confirmation delay</li>
</ul>
<p><strong>Use <code>confirmation_cycles: 4+</code></strong> when:</p>
<ul>
<li>You have very long detection cycles (10+ minutes)</li>
<li>You’re monitoring non-critical services</li>
<li>Alert fatigue is a severe problem</li>
</ul>
<h2 id="edge-cases"><a class="header" href="#edge-cases">Edge Cases</a></h2>
<h3 id="pattern-changes-during-confirmation"><a class="header" href="#pattern-changes-during-confirmation">Pattern Changes During Confirmation</a></h3>
<p>If the anomaly pattern changes during confirmation, it’s treated as a new anomaly:</p>
<pre><code>10:00  latency_spike_recent detected → SUSPECTED
10:03  traffic_surge_failing detected → NEW SUSPECTED (different pattern)
       (latency_spike_recent starts expiration countdown)
</code></pre>
<h3 id="multiple-anomalies-confirming-simultaneously"><a class="header" href="#multiple-anomalies-confirming-simultaneously">Multiple Anomalies Confirming Simultaneously</a></h3>
<p>Multiple anomalies can confirm in the same cycle:</p>
<pre><code class="language-json">{
  "fingerprinting": {
    "overall_action": "MIXED",
    "newly_confirmed_incidents": [
      {"anomaly_name": "latency_spike_recent", ...},
      {"anomaly_name": "error_rate_elevated", ...}
    ],
    "action_summary": {
      "newly_confirmed": 2
    }
  }
}
</code></pre>
<h3 id="confirmation-after-recovery"><a class="header" href="#confirmation-after-recovery">Confirmation After Recovery</a></h3>
<p>If an incident is in RECOVERING state and the anomaly reappears, it doesn’t need re-confirmation:</p>
<pre><code>10:00  SUSPECTED (1/2)
10:03  OPEN (confirmed)
10:06  OPEN (continuing)
10:09  RECOVERING (not detected, 1/3)
10:12  OPEN (detected again - immediately returns to OPEN, no confirmation needed)
</code></pre>
<p>This is because the incident was already confirmed before entering RECOVERING.</p>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p>Confirmation is a critical noise-reduction mechanism that:</p>
<ol>
<li><strong>Prevents alert fatigue</strong> by filtering transient spikes</li>
<li><strong>Ensures reliability</strong> by only alerting on persistent issues</li>
<li><strong>Protects the web API</strong> from orphaned incidents (confirmed-only alerts)</li>
<li><strong>Provides clear lifecycle</strong> with trackable state transitions</li>
</ol>
<p>The default configuration (<code>confirmation_cycles: 2</code>) provides a good balance between responsiveness and noise reduction for most production environments.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="fingerprinting"><a class="header" href="#fingerprinting">Fingerprinting</a></h1>
<p>Fingerprinting assigns stable identifiers to anomaly patterns, enabling tracking across detection cycles. This chapter explains the identification system, how it works, and why two different types of IDs are needed.</p>
<h2 id="what-is-fingerprinting"><a class="header" href="#what-is-fingerprinting">What is Fingerprinting?</a></h2>
<p><strong>Fingerprinting</strong> is the process of creating stable, reproducible identifiers for anomaly patterns. When the system detects an anomaly, it generates a “fingerprint” - a unique identifier based on what the anomaly is, not when it happened.</p>
<p>Think of it like identifying a person:</p>
<ul>
<li><strong>Fingerprint</strong> = A person’s actual fingerprint (uniquely identifies the person)</li>
<li><strong>Incident</strong> = A specific encounter with that person (when and where you met them)</li>
</ul>
<pre><code>                   Same Person (Fingerprint)
                            │
        ┌───────────────────┼───────────────────┐
        │                   │                   │
   Meeting #1          Meeting #2          Meeting #3
   (Jan 10, Coffee)    (Jan 15, Office)    (Jan 22, Park)

Each meeting is a unique incident, but it's the same person.
</code></pre>
<h2 id="why-two-id-types"><a class="header" href="#why-two-id-types">Why Two ID Types?</a></h2>
<p>The system uses two different identifiers that serve complementary purposes:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ID Type</th><th>What It Identifies</th><th>Characteristics</th></tr>
</thead>
<tbody>
<tr><td><strong>Fingerprint ID</strong></td><td>The anomaly pattern itself</td><td>Stable, deterministic, content-based</td></tr>
<tr><td><strong>Incident ID</strong></td><td>A specific occurrence of the pattern</td><td>Unique, random, time-bound</td></tr>
</tbody>
</table>
</div>
<h3 id="why-not-just-use-one-id"><a class="header" href="#why-not-just-use-one-id">Why Not Just Use One ID?</a></h3>
<p>Using only one ID would create problems:</p>
<p><strong>If only fingerprint_id:</strong></p>
<pre><code>Problem: Can't distinguish between separate occurrences

Jan 10  latency_spike (fingerprint_abc) - Incident A
Jan 15  latency_spike (fingerprint_abc) - Is this Incident A continuing?
                                          Or a new incident?
                                          We can't tell!
</code></pre>
<p><strong>If only incident_id:</strong></p>
<pre><code>Problem: Can't tell if issues are related

Jan 10  incident_123 (latency spike)
Jan 15  incident_456 (latency spike) - Are these the same type of issue?
                                        Different issue types?
                                        We can't tell!
</code></pre>
<p><strong>With both IDs:</strong></p>
<pre><code>Jan 10  fingerprint_abc / incident_123 (latency spike)
Jan 15  fingerprint_abc / incident_456 (latency spike)

Now we know:
- Same TYPE of anomaly (same fingerprint_abc)
- Different OCCURRENCES (different incident IDs)
- We can track patterns AND individual events
</code></pre>
<h2 id="fingerprint-id-pattern-identity"><a class="header" href="#fingerprint-id-pattern-identity">Fingerprint ID: Pattern Identity</a></h2>
<h3 id="what-is-it"><a class="header" href="#what-is-it">What Is It?</a></h3>
<p>A <strong>deterministic hash</strong> based on the anomaly’s content - what it is, not when it happened.</p>
<h3 id="how-its-generated"><a class="header" href="#how-its-generated">How It’s Generated</a></h3>
<pre><code class="language-python"># The fingerprint is generated from these components:
content = f"{service_name}_{model_name}_{anomaly_name}"
fingerprint_id = f"anomaly_{sha256(content)[:12]}"
</code></pre>
<p><strong>Example:</strong></p>
<pre><code>Components:
  service_name: booking
  model_name: business_hours
  anomaly_name: latency_spike_recent

Concatenated: "booking_business_hours_latency_spike_recent"
SHA256 hash:  d18f6ae2bf62a7c9...
Fingerprint:  anomaly_d18f6ae2bf62

Same inputs ALWAYS produce the same fingerprint!
</code></pre>
<h3 id="properties"><a class="header" href="#properties">Properties</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Property</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><strong>Deterministic</strong></td><td>Same pattern always gets same ID</td><td><code>booking_business_hours_latency_spike_recent</code> → always <code>anomaly_d18f6ae2bf62</code></td></tr>
<tr><td><strong>Content-based</strong></td><td>Based on what, not when</td><td>Doesn’t include timestamp</td></tr>
<tr><td><strong>Stable</strong></td><td>Doesn’t change across time</td><td>Same ID today, tomorrow, next year</td></tr>
<tr><td><strong>Reproducible</strong></td><td>Can be regenerated</td><td>Given the same inputs, always same output</td></tr>
</tbody>
</table>
</div>
<h3 id="what-changes-the-fingerprint"><a class="header" href="#what-changes-the-fingerprint">What Changes the Fingerprint?</a></h3>
<p>The fingerprint changes when the pattern type changes:</p>
<pre><code>SAME fingerprint (anomaly_abc):
─────────────────────────────
• booking + business_hours + latency_spike_recent (Monday)
• booking + business_hours + latency_spike_recent (Tuesday)
• booking + business_hours + latency_spike_recent (Different severity)

DIFFERENT fingerprint (anomaly_xyz):
───────────────────────────────────
• booking + business_hours + traffic_surge_failing (different anomaly type)
• booking + evening_hours + latency_spike_recent (different time period)
• search + business_hours + latency_spike_recent (different service)
</code></pre>
<h2 id="incident-id-occurrence-identity"><a class="header" href="#incident-id-occurrence-identity">Incident ID: Occurrence Identity</a></h2>
<h3 id="what-is-it-1"><a class="header" href="#what-is-it-1">What Is It?</a></h3>
<p>A <strong>unique identifier</strong> for each specific occurrence of an anomaly pattern.</p>
<h3 id="how-its-generated-1"><a class="header" href="#how-its-generated-1">How It’s Generated</a></h3>
<pre><code class="language-python"># The incident ID is generated randomly when a new incident is created:
incident_id = f"incident_{uuid4().hex[:16]}"
</code></pre>
<p><strong>Example:</strong></p>
<pre><code>Each new incident gets a new random ID:
  incident_1dcbafc91480
  incident_7a23b9f4c8e1
  incident_95d2e61a8b43

Even for the same pattern type, each occurrence has a unique ID!
</code></pre>
<h3 id="properties-1"><a class="header" href="#properties-1">Properties</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Property</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><strong>Unique</strong></td><td>Each occurrence gets its own ID</td><td>No two incidents share an ID</td></tr>
<tr><td><strong>UUID-based</strong></td><td>Random generation</td><td>Not predictable from inputs</td></tr>
<tr><td><strong>Transient</strong></td><td>New ID when pattern reappears</td><td>After resolution, next occurrence = new ID</td></tr>
<tr><td><strong>Time-bound</strong></td><td>Associated with specific time period</td><td>Tracks one continuous occurrence</td></tr>
</tbody>
</table>
</div>
<h3 id="when-is-a-new-incident-id-created"><a class="header" href="#when-is-a-new-incident-id-created">When Is a New Incident ID Created?</a></h3>
<pre><code>New incident_id created when:
────────────────────────────
• Anomaly detected for first time (no active incident)
• Anomaly reappears after being resolved
• Anomaly reappears after staleness threshold (&gt;30 min gap)

Same incident_id continues when:
─────────────────────────────────
• Anomaly detected while incident is OPEN
• Anomaly detected while incident is RECOVERING
• Anomaly detected within staleness threshold
</code></pre>
<h2 id="the-relationship-between-ids"><a class="header" href="#the-relationship-between-ids">The Relationship Between IDs</a></h2>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│                 Fingerprint: anomaly_d18f6ae2bf62                   │
│                 (Pattern: booking latency spike)                    │
├─────────────────────────────────────────────────────────────────────┤
│                              │                                      │
│                              │                                      │
│    ┌─────────────────────────┼─────────────────────────┐           │
│    │                         │                         │           │
│    ▼                         ▼                         ▼           │
│ ┌─────────────────┐   ┌─────────────────┐   ┌─────────────────┐    │
│ │   Incident A    │   │   Incident B    │   │   Incident C    │    │
│ │ incident_abc123 │   │ incident_def456 │   │ incident_ghi789 │    │
│ ├─────────────────┤   ├─────────────────┤   ├─────────────────┤    │
│ │ Jan 10, 10:00   │   │ Jan 15, 14:00   │   │ Jan 22, 09:00   │    │
│ │ Duration: 45min │   │ Duration: 2hr   │   │ Duration: 30min │    │
│ │ Status: CLOSED  │   │ Status: CLOSED  │   │ Status: OPEN    │    │
│ │ Occurrences: 8  │   │ Occurrences: 24 │   │ Occurrences: 5  │    │
│ └─────────────────┘   └─────────────────┘   └─────────────────┘    │
│                                                                     │
│ All three incidents represent the SAME type of anomaly              │
│ (same fingerprint) but are SEPARATE occurrences (different IDs)     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="what-each-id-enables"><a class="header" href="#what-each-id-enables">What Each ID Enables</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Capability</th><th>Fingerprint ID</th><th>Incident ID</th></tr>
</thead>
<tbody>
<tr><td>Track same issue across time</td><td>✅</td><td>❌</td></tr>
<tr><td>Identify individual occurrences</td><td>❌</td><td>✅</td></tr>
<tr><td>Correlate related events</td><td>✅</td><td>❌</td></tr>
<tr><td>Calculate MTTR per incident</td><td>❌</td><td>✅</td></tr>
<tr><td>Find recurring patterns</td><td>✅</td><td>❌</td></tr>
<tr><td>Link alert to specific event</td><td>❌</td><td>✅</td></tr>
</tbody>
</table>
</div>
<h2 id="how-fingerprinting-works"><a class="header" href="#how-fingerprinting-works">How Fingerprinting Works</a></h2>
<h3 id="the-complete-flow"><a class="header" href="#the-complete-flow">The Complete Flow</a></h3>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│                     Fingerprinting Process                          │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│   Step 1: Detection Result                                          │
│   ─────────────────────────                                         │
│   anomaly: latency_spike_recent                                     │
│   service: booking                                                  │
│   model: business_hours                                             │
│                                                                     │
│                          │                                          │
│                          ▼                                          │
│                                                                     │
│   Step 2: Generate Fingerprint ID                                   │
│   ──────────────────────────────                                    │
│   hash("booking_business_hours_latency_spike_recent")               │
│   → anomaly_d18f6ae2bf62                                            │
│                                                                     │
│                          │                                          │
│                          ▼                                          │
│                                                                     │
│   Step 3: Database Lookup                                           │
│   ───────────────────────                                           │
│   ┌─────────────────────────────────────────────────────────────┐   │
│   │  SELECT * FROM anomaly_incidents                            │   │
│   │  WHERE fingerprint_id = 'anomaly_d18f6ae2bf62'              │   │
│   │    AND status IN ('SUSPECTED', 'OPEN', 'RECOVERING')        │   │
│   └─────────────────────────────────────────────────────────────┘   │
│                                                                     │
│                          │                                          │
│                    ┌─────┴─────┐                                    │
│                    │           │                                    │
│               NOT FOUND      FOUND                                  │
│                    │           │                                    │
│                    ▼           ▼                                    │
│                                                                     │
│   Step 4a: Create New           Step 4b: Update Existing            │
│   ───────────────────           ───────────────────────             │
│   • Generate new incident_id    • Keep same incident_id             │
│   • Set status = SUSPECTED      • Check staleness                   │
│   • Initialize counters         • Increment occurrence_count        │
│   • Set first_seen = now        • Update last_updated               │
│                                 • Handle state transitions          │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="decision-tree"><a class="header" href="#decision-tree">Decision Tree</a></h3>
<pre><code>                    ┌───────────────────────┐
                    │   Anomaly Detected    │
                    └───────────┬───────────┘
                                │
                    ┌───────────▼───────────┐
                    │ Generate fingerprint  │
                    └───────────┬───────────┘
                                │
                    ┌───────────▼───────────┐
                    │ Active incident with  │
              ┌─────│ this fingerprint?     │─────┐
              │     └───────────────────────┘     │
              │NO                                 │YES
              │                                   │
              ▼                                   ▼
    ┌─────────────────────┐           ┌─────────────────────┐
    │   CREATE incident   │           │  Is it stale?       │
    │   fingerprint_action│           │  (gap &gt; 30 min)     │
    │   = CREATE          │           └──────────┬──────────┘
    │   incident_action   │                      │
    │   = CREATE          │             ┌────────┴────────┐
    └─────────────────────┘             │YES              │NO
                                        │                 │
                                        ▼                 ▼
                              ┌─────────────────┐  ┌─────────────────┐
                              │ Close stale     │  │ UPDATE incident │
                              │ Create new      │  │ fingerprint_    │
                              │                 │  │ action = UPDATE │
                              │ Two actions:    │  │ incident_action │
                              │ RESOLVE + CREATE│  │ = CONTINUE      │
                              └─────────────────┘  └─────────────────┘
</code></pre>
<h2 id="database-schema"><a class="header" href="#database-schema">Database Schema</a></h2>
<p>The fingerprinting system uses SQLite for persistence:</p>
<pre><code class="language-sql">CREATE TABLE anomaly_incidents (
    -- Identity
    fingerprint_id TEXT NOT NULL,          -- Pattern identifier
    incident_id TEXT PRIMARY KEY,          -- Occurrence identifier

    -- Context
    service_name TEXT NOT NULL,            -- Service (e.g., "booking")
    anomaly_name TEXT NOT NULL,            -- Pattern name
    detected_by_model TEXT,                -- Time period model

    -- State
    status TEXT NOT NULL,                  -- SUSPECTED, OPEN, RECOVERING, CLOSED
    severity TEXT NOT NULL,                -- low, medium, high, critical

    -- Timestamps
    first_seen TIMESTAMP NOT NULL,         -- When incident started
    last_updated TIMESTAMP NOT NULL,       -- Most recent detection
    resolved_at TIMESTAMP NULL,            -- When closed (null if active)

    -- Counters
    occurrence_count INTEGER NOT NULL,     -- Total detections
    consecutive_detections INTEGER NOT NULL, -- In a row (for confirmation)
    missed_cycles INTEGER NOT NULL,        -- Not detected (for grace period)

    -- Optional data
    current_value REAL,                    -- Current metric value
    threshold_value REAL,                  -- Threshold if applicable
    confidence_score REAL,                 -- ML confidence
    detection_method TEXT,                 -- How it was detected
    description TEXT,                      -- Human-readable description
    metadata TEXT                          -- JSON for extra data
);

-- Indexes for fast lookups
CREATE INDEX idx_fingerprint_status ON anomaly_incidents(fingerprint_id, status);
CREATE INDEX idx_service_timeline ON anomaly_incidents(service_name, first_seen DESC);
CREATE INDEX idx_active_incidents ON anomaly_incidents(status, last_updated DESC)
    WHERE status IN ('SUSPECTED', 'OPEN', 'RECOVERING');
</code></pre>
<h3 id="why-sqlite"><a class="header" href="#why-sqlite">Why SQLite?</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Advantage</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><strong>Simple</strong></td><td>No external dependencies</td></tr>
<tr><td><strong>Reliable</strong></td><td>ACID transactions built-in</td></tr>
<tr><td><strong>Fast</strong></td><td>Excellent for read-heavy workloads</td></tr>
<tr><td><strong>Portable</strong></td><td>Single file, easy to backup</td></tr>
<tr><td><strong>Lightweight</strong></td><td>Minimal resource usage</td></tr>
</tbody>
</table>
</div>
<p>For most deployments, SQLite handles the expected volume easily (&lt; 1000 active incidents).</p>
<h2 id="staleness-check-1"><a class="header" href="#staleness-check-1">Staleness Check</a></h2>
<h3 id="what-is-staleness"><a class="header" href="#what-is-staleness">What Is Staleness?</a></h3>
<p>When the time gap between detections exceeds a threshold, the system considers the incident “stale” - meaning it’s probably not the same ongoing issue, even if it’s the same pattern type.</p>
<h3 id="why-staleness-matters"><a class="header" href="#why-staleness-matters">Why Staleness Matters</a></h3>
<p>Without staleness check:</p>
<pre><code>10:00  Latency spike detected → incident_123 created
10:03  Latency spike detected → incident_123 continues
10:06  Latency spike resolved

(long gap - different issue)

15:00  Latency spike detected → incident_123 continues??? NO!
       This is likely a different issue, not the same one from 5 hours ago!
</code></pre>
<p>With staleness check:</p>
<pre><code>10:00  Latency spike detected → incident_123 created
10:03  Latency spike detected → incident_123 continues
10:06  Latency spike resolved

(long gap - different issue)

15:00  Latency spike detected
       Gap check: 15:00 - 10:06 = 4h 54min &gt; 30min threshold
       Action: Close incident_123 as "auto_stale", create incident_456
       Now we have two separate incidents (correct!)
</code></pre>
<h3 id="staleness-flow"><a class="header" href="#staleness-flow">Staleness Flow</a></h3>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│                     Staleness Check                                  │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│   Inputs:                                                           │
│   • last_updated: 10:00:00 (from database)                          │
│   • current_time: 11:15:00                                          │
│   • incident_separation_minutes: 30 (config)                        │
│                                                                     │
│   Calculation:                                                      │
│   ─────────────                                                     │
│   gap = current_time - last_updated                                 │
│   gap = 11:15 - 10:00 = 75 minutes                                  │
│                                                                     │
│   is_stale = gap &gt; incident_separation_minutes                      │
│   is_stale = 75 &gt; 30 = TRUE                                         │
│                                                                     │
│   Result:                                                           │
│   ────────                                                          │
│   ┌─────────────────────────────────────────────────────────────┐   │
│   │  Old incident closed:                                       │   │
│   │    incident_action: CLOSE                                   │   │
│   │    resolution_reason: "auto_stale"                          │   │
│   │                                                             │   │
│   │  New incident created:                                      │   │
│   │    incident_id: incident_new456                             │   │
│   │    incident_action: CREATE                                  │   │
│   │    status: SUSPECTED                                        │   │
│   └─────────────────────────────────────────────────────────────┘   │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="resolution-reasons-1"><a class="header" href="#resolution-reasons-1">Resolution Reasons</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Reason</th><th>Description</th><th>When It Happens</th></tr>
</thead>
<tbody>
<tr><td><code>resolved</code></td><td>Normal resolution</td><td>Anomaly cleared after grace period</td></tr>
<tr><td><code>auto_stale</code></td><td>Stale incident closed</td><td>Time gap exceeded threshold, pattern reappeared</td></tr>
<tr><td><code>suspected_expired</code></td><td>Never confirmed</td><td>SUSPECTED state expired without confirmation</td></tr>
</tbody>
</table>
</div>
<h2 id="per-anomaly-fingerprinting-fields"><a class="header" href="#per-anomaly-fingerprinting-fields">Per-Anomaly Fingerprinting Fields</a></h2>
<p>Each anomaly in the output includes comprehensive fingerprinting metadata:</p>
<pre><code class="language-json">{
  "anomalies": {
    "latency_spike_recent": {
      "type": "consolidated",
      "severity": "high",
      "description": "Latency spike: 450ms (normally 120ms)",

      "fingerprint_id": "anomaly_d18f6ae2bf62",
      "fingerprint_action": "UPDATE",

      "incident_id": "incident_31e9e23d4b2b",
      "incident_action": "CONTINUE",

      "status": "OPEN",
      "previous_status": "OPEN",

      "incident_duration_minutes": 15,
      "first_seen": "2025-12-17T13:45:00",
      "last_updated": "2025-12-17T14:00:00",

      "occurrence_count": 5,
      "consecutive_detections": 5,

      "is_confirmed": true,
      "newly_confirmed": false
    }
  }
}
</code></pre>
<h3 id="field-reference"><a class="header" href="#field-reference">Field Reference</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>fingerprint_id</code></td><td>string</td><td>Pattern identifier (deterministic hash)</td></tr>
<tr><td><code>fingerprint_action</code></td><td>string</td><td>Pattern action: CREATE, UPDATE, RESOLVE</td></tr>
<tr><td><code>incident_id</code></td><td>string</td><td>Occurrence identifier (random UUID)</td></tr>
<tr><td><code>incident_action</code></td><td>string</td><td>Incident action: CREATE, CONTINUE, CLOSE</td></tr>
<tr><td><code>status</code></td><td>string</td><td>Current status: SUSPECTED, OPEN, RECOVERING, CLOSED</td></tr>
<tr><td><code>previous_status</code></td><td>string</td><td>Status before this cycle</td></tr>
<tr><td><code>incident_duration_minutes</code></td><td>integer</td><td>Time since first_seen</td></tr>
<tr><td><code>first_seen</code></td><td>datetime</td><td>When this incident started</td></tr>
<tr><td><code>last_updated</code></td><td>datetime</td><td>Most recent detection time</td></tr>
<tr><td><code>occurrence_count</code></td><td>integer</td><td>Total times detected</td></tr>
<tr><td><code>consecutive_detections</code></td><td>integer</td><td>Detections in a row</td></tr>
<tr><td><code>is_confirmed</code></td><td>boolean</td><td>True if status is OPEN</td></tr>
<tr><td><code>newly_confirmed</code></td><td>boolean</td><td>True if just confirmed this cycle</td></tr>
</tbody>
</table>
</div>
<h2 id="action-types"><a class="header" href="#action-types">Action Types</a></h2>
<h3 id="fingerprint-actions"><a class="header" href="#fingerprint-actions">Fingerprint Actions</a></h3>
<p>The fingerprint action describes what happened to the pattern tracking:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Action</th><th>Description</th><th>When</th></tr>
</thead>
<tbody>
<tr><td><code>CREATE</code></td><td>New pattern encountered</td><td>First time this exact pattern is seen</td></tr>
<tr><td><code>UPDATE</code></td><td>Existing pattern updated</td><td>Pattern detected, already being tracked</td></tr>
<tr><td><code>RESOLVE</code></td><td>Pattern no longer active</td><td>Grace period exceeded, pattern cleared</td></tr>
</tbody>
</table>
</div>
<h3 id="incident-actions"><a class="header" href="#incident-actions">Incident Actions</a></h3>
<p>The incident action describes what happened to the specific occurrence:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Action</th><th>Description</th><th>When</th></tr>
</thead>
<tbody>
<tr><td><code>CREATE</code></td><td>New incident created</td><td>New occurrence of a pattern (including after stale)</td></tr>
<tr><td><code>CONTINUE</code></td><td>Incident continues</td><td>Same occurrence still active</td></tr>
<tr><td><code>CLOSE</code></td><td>Incident closed</td><td>Occurrence resolved (grace period exceeded)</td></tr>
</tbody>
</table>
</div>
<h3 id="combined-actions"><a class="header" href="#combined-actions">Combined Actions</a></h3>
<p>The combination tells the full story:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>fingerprint_action</th><th>incident_action</th><th>Meaning</th></tr>
</thead>
<tbody>
<tr><td>CREATE</td><td>CREATE</td><td>Brand new pattern, new incident</td></tr>
<tr><td>UPDATE</td><td>CONTINUE</td><td>Ongoing issue, same incident</td></tr>
<tr><td>UPDATE</td><td>CREATE</td><td>Same pattern reappeared (was stale or resolved)</td></tr>
<tr><td>RESOLVE</td><td>CLOSE</td><td>Pattern cleared, incident resolved</td></tr>
</tbody>
</table>
</div>
<h2 id="resolution-payload"><a class="header" href="#resolution-payload">Resolution Payload</a></h2>
<p>When an incident is resolved, the system sends detailed resolution information:</p>
<pre><code class="language-json">{
  "fingerprinting": {
    "resolved_incidents": [
      {
        "fingerprint_id": "anomaly_d18f6ae2bf62",
        "incident_id": "incident_31e9e23d4b2b",
        "anomaly_name": "latency_spike_recent",

        "fingerprint_action": "RESOLVE",
        "incident_action": "CLOSE",

        "final_severity": "high",
        "resolved_at": "2025-12-17T14:30:00",
        "total_occurrences": 8,
        "incident_duration_minutes": 45,
        "first_seen": "2025-12-17T13:45:00",
        "service_name": "booking",
        "last_detected_by_model": "business_hours",
        "resolution_reason": "resolved"
      }
    ]
  }
}
</code></pre>
<h3 id="resolution-fields"><a class="header" href="#resolution-fields">Resolution Fields</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>fingerprint_id</code></td><td>string</td><td>Pattern that was resolved</td></tr>
<tr><td><code>incident_id</code></td><td>string</td><td>Specific occurrence that closed</td></tr>
<tr><td><code>anomaly_name</code></td><td>string</td><td>Human-readable pattern name</td></tr>
<tr><td><code>final_severity</code></td><td>string</td><td>Severity at time of resolution</td></tr>
<tr><td><code>resolved_at</code></td><td>datetime</td><td>When the incident was closed</td></tr>
<tr><td><code>total_occurrences</code></td><td>integer</td><td>How many times detected during incident</td></tr>
<tr><td><code>incident_duration_minutes</code></td><td>integer</td><td>Total duration</td></tr>
<tr><td><code>first_seen</code></td><td>datetime</td><td>When it started</td></tr>
<tr><td><code>service_name</code></td><td>string</td><td>Affected service</td></tr>
<tr><td><code>resolution_reason</code></td><td>string</td><td>Why it was closed</td></tr>
</tbody>
</table>
</div>
<h2 id="database-cleanup"><a class="header" href="#database-cleanup">Database Cleanup</a></h2>
<p>To prevent unbounded database growth, closed incidents are automatically cleaned up:</p>
<pre><code class="language-json">{
  "fingerprinting": {
    "cleanup_max_age_hours": 72
  }
}
</code></pre>
<h3 id="cleanup-process"><a class="header" href="#cleanup-process">Cleanup Process</a></h3>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│                     Cleanup Process                                  │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│   Every cleanup run:                                                │
│   ──────────────────                                                │
│                                                                     │
│   1. Find CLOSED incidents older than cleanup_max_age_hours         │
│                                                                     │
│   DELETE FROM anomaly_incidents                                     │
│   WHERE status = 'CLOSED'                                           │
│     AND resolved_at &lt; (NOW - cleanup_max_age_hours)                 │
│                                                                     │
│   2. Leave active incidents untouched                               │
│                                                                     │
│   ┌─────────────────────────────────────────────────────────────┐   │
│   │  SUSPECTED incidents: KEPT (may still confirm)             │   │
│   │  OPEN incidents:      KEPT (active issue)                  │   │
│   │  RECOVERING incidents: KEPT (may resume or close)          │   │
│   │  CLOSED incidents:    DELETED if older than 72h            │   │
│   └─────────────────────────────────────────────────────────────┘   │
│                                                                     │
│   Result: Database stays bounded while preserving active state      │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="why-72-hours"><a class="header" href="#why-72-hours">Why 72 Hours?</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Shorter (24h)</th><th>Longer (168h/1 week)</th></tr>
</thead>
<tbody>
<tr><td>Less storage</td><td>More history</td></tr>
<tr><td>Faster queries</td><td>Slower queries</td></tr>
<tr><td>Less post-incident analysis</td><td>Better trend analysis</td></tr>
<tr><td>Risk of losing relevant history</td><td>More database bloat</td></tr>
</tbody>
</table>
</div>
<p>72 hours (3 days) is a balance that:</p>
<ul>
<li>Covers most incident review periods</li>
<li>Allows weekend incident analysis on Monday</li>
<li>Keeps database size manageable</li>
</ul>
<h2 id="configuration-reference"><a class="header" href="#configuration-reference">Configuration Reference</a></h2>
<p>All fingerprinting settings in <code>config.json</code>:</p>
<pre><code class="language-json">{
  "fingerprinting": {
    "db_path": "./anomaly_state.db",
    "confirmation_cycles": 2,
    "resolution_grace_cycles": 3,
    "incident_separation_minutes": 30,
    "cleanup_max_age_hours": 72
  }
}
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Setting</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>db_path</code></td><td><code>./anomaly_state.db</code></td><td>Path to SQLite database</td></tr>
<tr><td><code>confirmation_cycles</code></td><td>2</td><td>Detections required before alerting</td></tr>
<tr><td><code>resolution_grace_cycles</code></td><td>3</td><td>Non-detections before resolving</td></tr>
<tr><td><code>incident_separation_minutes</code></td><td>30</td><td>Gap threshold for staleness</td></tr>
<tr><td><code>cleanup_max_age_hours</code></td><td>72</td><td>Age threshold for cleanup</td></tr>
</tbody>
</table>
</div>
<h2 id="summary-1"><a class="header" href="#summary-1">Summary</a></h2>
<p>Fingerprinting provides the foundation for intelligent incident tracking:</p>
<ol>
<li><strong>Fingerprint ID</strong> enables pattern recognition across time</li>
<li><strong>Incident ID</strong> enables precise occurrence tracking</li>
<li><strong>Staleness check</strong> prevents confusion between separate issues</li>
<li><strong>Database persistence</strong> maintains state across restarts</li>
<li><strong>Automatic cleanup</strong> keeps the system healthy</li>
</ol>
<p>Together, these mechanisms enable Yaga2 to:</p>
<ul>
<li>Track recurring issues</li>
<li>Calculate accurate metrics (MTTR, frequency)</li>
<li>Prevent duplicate alerts</li>
<li>Provide complete incident lifecycle visibility</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="decision-matrix"><a class="header" href="#decision-matrix">Decision Matrix</a></h1>
<p>Quick reference for how different conditions map to alert decisions.</p>
<h2 id="end-to-end-decision-flow"><a class="header" href="#end-to-end-decision-flow">End-to-End Decision Flow</a></h2>
<pre><code>Metrics
   │
   ▼
┌──────────────────────────────────────────────────────────────────────────┐
│ 1. Detection: Is this behavior unusual?                                  │
│    Isolation Forest + Pattern Matching                                   │
│    Output: anomaly detected (yes/no), severity, pattern name             │
└────────────────────────────────────────────────────────────────────────┬─┘
                                                                         │
   ┌─────────────────────────────────────────────────────────────────────┘
   │
   ▼
┌──────────────────────────────────────────────────────────────────────────┐
│ 2. SLO Evaluation: Does it matter operationally?                         │
│    Latency, Errors, DB Latency, Request Rate vs thresholds              │
│    Output: slo_status (ok/warning/breached), adjusted severity           │
└────────────────────────────────────────────────────────────────────────┬─┘
                                                                         │
   ┌─────────────────────────────────────────────────────────────────────┘
   │
   ▼
┌──────────────────────────────────────────────────────────────────────────┐
│ 3. Incident Lifecycle: Should we alert now?                              │
│    Confirmation cycles, grace periods                                    │
│    Output: alert (yes/no), status (SUSPECTED/OPEN/RECOVERING/CLOSED)     │
└──────────────────────────────────────────────────────────────────────────┘
</code></pre>
<h2 id="detection-decision-matrix"><a class="header" href="#detection-decision-matrix">Detection Decision Matrix</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Traffic</th><th>Latency</th><th>Errors</th><th>Pattern</th><th>Severity</th></tr>
</thead>
<tbody>
<tr><td>High</td><td>Normal</td><td>Normal</td><td><code>traffic_surge_healthy</code></td><td>low</td></tr>
<tr><td>High</td><td>High</td><td>Normal</td><td><code>traffic_surge_degrading</code></td><td>high</td></tr>
<tr><td>High</td><td>High</td><td>High</td><td><code>traffic_surge_failing</code></td><td>critical</td></tr>
<tr><td>Very Low</td><td>Any</td><td>Any</td><td><code>traffic_cliff</code></td><td>critical</td></tr>
<tr><td>Normal</td><td>High</td><td>Normal</td><td><code>latency_spike_recent</code></td><td>high</td></tr>
<tr><td>Normal</td><td>High</td><td>Normal (deps healthy)</td><td><code>internal_latency_issue</code></td><td>high</td></tr>
<tr><td>Normal</td><td>Normal</td><td>High</td><td><code>error_rate_elevated</code></td><td>high</td></tr>
<tr><td>Normal</td><td>Normal</td><td>Very High</td><td><code>error_rate_critical</code></td><td>critical</td></tr>
<tr><td>Normal</td><td>Low</td><td>High</td><td><code>fast_failure</code></td><td>critical</td></tr>
<tr><td>Normal</td><td>Very Low</td><td>Very High</td><td><code>fast_rejection</code></td><td>critical</td></tr>
<tr><td>Normal</td><td>High (DB dominant)</td><td>Normal</td><td><code>database_bottleneck</code></td><td>high</td></tr>
<tr><td>Normal</td><td>Normal (DB high)</td><td>Normal</td><td><code>database_degradation</code></td><td>medium</td></tr>
</tbody>
</table>
</div>
<h2 id="slo-severity-adjustment-matrix"><a class="header" href="#slo-severity-adjustment-matrix">SLO Severity Adjustment Matrix</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ML Severity</th><th>Latency SLO</th><th>Error SLO</th><th>DB SLO</th><th>Final Severity</th></tr>
</thead>
<tbody>
<tr><td>critical</td><td>ok</td><td>ok</td><td>ok</td><td><strong>low</strong></td></tr>
<tr><td>critical</td><td>ok</td><td>ok</td><td>warning</td><td><strong>low</strong></td></tr>
<tr><td>critical</td><td>warning</td><td>ok</td><td>ok</td><td>high</td></tr>
<tr><td>critical</td><td>ok</td><td>warning</td><td>ok</td><td>high</td></tr>
<tr><td>critical</td><td>breached</td><td>ok</td><td>ok</td><td><strong>critical</strong></td></tr>
<tr><td>critical</td><td>ok</td><td>breached</td><td>ok</td><td><strong>critical</strong></td></tr>
<tr><td>high</td><td>ok</td><td>ok</td><td>ok</td><td><strong>low</strong></td></tr>
<tr><td>high</td><td>warning</td><td>ok</td><td>ok</td><td>high</td></tr>
<tr><td>medium</td><td>ok</td><td>ok</td><td>ok</td><td><strong>low</strong></td></tr>
<tr><td>any</td><td>breached</td><td>breached</td><td>any</td><td><strong>critical</strong></td></tr>
</tbody>
</table>
</div>
<p><strong>Key Rule</strong>: SLO status <code>ok</code> → Final severity <code>low</code> (regardless of ML severity)</p>
<h2 id="incident-state-decision-matrix"><a class="header" href="#incident-state-decision-matrix">Incident State Decision Matrix</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Current State</th><th>Anomaly Detected?</th><th>Consecutive</th><th>Action</th></tr>
</thead>
<tbody>
<tr><td>None</td><td>Yes</td><td>1</td><td>CREATE (SUSPECTED)</td></tr>
<tr><td>SUSPECTED</td><td>Yes</td><td>&lt; threshold</td><td>Stay SUSPECTED</td></tr>
<tr><td>SUSPECTED</td><td>Yes</td><td>≥ threshold</td><td>→ OPEN (alert)</td></tr>
<tr><td>SUSPECTED</td><td>No</td><td>&lt; grace</td><td>Stay SUSPECTED</td></tr>
<tr><td>SUSPECTED</td><td>No</td><td>≥ grace</td><td>→ CLOSED (silent)</td></tr>
<tr><td>OPEN</td><td>Yes</td><td>any</td><td>Continue OPEN</td></tr>
<tr><td>OPEN</td><td>No</td><td>1</td><td>→ RECOVERING</td></tr>
<tr><td>RECOVERING</td><td>Yes</td><td>any</td><td>→ OPEN (resume)</td></tr>
<tr><td>RECOVERING</td><td>No</td><td>&lt; grace</td><td>Stay RECOVERING</td></tr>
<tr><td>RECOVERING</td><td>No</td><td>≥ grace</td><td>→ CLOSED (resolve)</td></tr>
</tbody>
</table>
</div>
<h2 id="alert-decision-summary"><a class="header" href="#alert-decision-summary">Alert Decision Summary</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Condition</th><th>Alert Sent?</th><th>Reason</th></tr>
</thead>
<tbody>
<tr><td>First detection</td><td>No</td><td>SUSPECTED, waiting for confirmation</td></tr>
<tr><td>Second consecutive</td><td>Yes</td><td>Confirmed (OPEN)</td></tr>
<tr><td>Continuing OPEN</td><td>No</td><td>Already alerting</td></tr>
<tr><td>First non-detection</td><td>No</td><td>Grace period (RECOVERING)</td></tr>
<tr><td>Resolved</td><td>Resolution</td><td>CLOSED after grace period</td></tr>
<tr><td>Stale gap (&gt;30 min)</td><td>New alert</td><td>Old closed, new SUSPECTED</td></tr>
</tbody>
</table>
</div>
<h2 id="complete-example-scenarios"><a class="header" href="#complete-example-scenarios">Complete Example Scenarios</a></h2>
<h3 id="scenario-1-transient-spike-no-alert"><a class="header" href="#scenario-1-transient-spike-no-alert">Scenario 1: Transient Spike (No Alert)</a></h3>
<pre><code>10:00 - Latency 450ms detected
        → ML: latency_spike_recent (high)
        → SLO: 450ms &lt; 500ms acceptable → status: ok
        → Adjusted severity: low
        → Status: SUSPECTED (first detection)
        → Alert: ❌ NO

10:03 - Latency 120ms normal
        → No anomaly detected
        → Status: SUSPECTED (missed: 1)

10:06 - Latency 115ms normal
        → No anomaly detected
        → Status: SUSPECTED (missed: 2)

10:09 - Latency 118ms normal
        → Status: CLOSED (suspected_expired)
        → Alert: ❌ NO (never confirmed)
</code></pre>
<h3 id="scenario-2-real-incident-alert"><a class="header" href="#scenario-2-real-incident-alert">Scenario 2: Real Incident (Alert)</a></h3>
<pre><code>10:00 - Latency 850ms detected
        → ML: latency_spike_recent (critical)
        → SLO: 850ms &gt; 800ms warning → status: warning
        → Adjusted severity: high
        → Status: SUSPECTED
        → Alert: ❌ NO (not confirmed)

10:03 - Latency 900ms detected
        → Status: OPEN (confirmed!)
        → Alert: ✅ YES

10:06 - Latency 920ms detected
        → Status: OPEN (continue)
        → Alert: Already sent

...

10:30 - Latency 200ms normal
        → Status: RECOVERING (grace: 1)

10:33 - Latency 180ms normal
        → Status: RECOVERING (grace: 2)

10:36 - Latency 175ms normal
        → Status: CLOSED (resolved)
        → Resolution: ✅ YES
</code></pre>
<h3 id="scenario-3-slo-suppression-low-priority"><a class="header" href="#scenario-3-slo-suppression-low-priority">Scenario 3: SLO Suppression (Low Priority)</a></h3>
<pre><code>10:00 - Latency 280ms detected
        → ML: latency_spike_recent (high)
        → SLO: 280ms &lt; 300ms acceptable → status: ok
        → Adjusted severity: low (!!!)
        → Status: SUSPECTED
        → Alert: ❌ NO

10:03 - Latency 285ms detected
        → Status: OPEN (confirmed)
        → Severity: low
        → Alert: ✅ YES (low priority)
</code></pre>
<h2 id="quick-reference-when-to-alert"><a class="header" href="#quick-reference-when-to-alert">Quick Reference: When to Alert</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Must be True</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>Anomaly detected</td><td>ML flagged unusual behavior</td></tr>
<tr><td>Pattern matched</td><td>Known operational scenario</td></tr>
<tr><td>2+ consecutive</td><td>Confirmed, not transient</td></tr>
<tr><td>SLO evaluated</td><td>Operational impact assessed</td></tr>
</tbody>
</table>
</div>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Final Severity</th><th>Action</th></tr>
</thead>
<tbody>
<tr><td>critical</td><td>PagerDuty, immediate action</td></tr>
<tr><td>high</td><td>Alert, investigate soon</td></tr>
<tr><td>low</td><td>Log only, informational</td></tr>
<tr><td>none</td><td>No action</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="configuration-reference-1"><a class="header" href="#configuration-reference-1">Configuration Reference</a></h1>
<p>Complete configuration reference for <code>config.json</code>.</p>
<h2 id="configuration-file-location"><a class="header" href="#configuration-file-location">Configuration File Location</a></h2>
<p>The system searches for configuration in this order:</p>
<ol>
<li><code>CONFIG_FILE</code> environment variable</li>
<li><code>./config.json</code> (current directory)</li>
<li><code>./config/config.json</code></li>
<li><code>~/.smartbox/config.json</code></li>
<li><code>/etc/smartbox/config.json</code></li>
</ol>
<h2 id="core-sections"><a class="header" href="#core-sections">Core Sections</a></h2>
<h3 id="victoriametrics"><a class="header" href="#victoriametrics">VictoriaMetrics</a></h3>
<pre><code class="language-json">{
  "victoria_metrics": {
    "endpoint": "https://otel-metrics.production.smartbox.com",
    "timeout_seconds": 10,
    "max_retries": 3,
    "pool_connections": 20,
    "circuit_breaker_threshold": 5,
    "circuit_breaker_timeout_seconds": 300
  }
}
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>endpoint</code></td><td>required</td><td>VictoriaMetrics server URL</td></tr>
<tr><td><code>timeout_seconds</code></td><td>10</td><td>Request timeout</td></tr>
<tr><td><code>max_retries</code></td><td>3</td><td>Retry attempts</td></tr>
<tr><td><code>circuit_breaker_threshold</code></td><td>5</td><td>Failures before circuit opens</td></tr>
</tbody>
</table>
</div>
<h3 id="slo-configuration"><a class="header" href="#slo-configuration">SLO Configuration</a></h3>
<pre><code class="language-json">{
  "slos": {
    "enabled": true,
    "allow_downgrade_to_informational": true,
    "require_slo_breach_for_critical": true,
    "defaults": {
      "latency_acceptable_ms": 500,
      "latency_warning_ms": 800,
      "latency_critical_ms": 1000,
      "error_rate_acceptable": 0.005,
      "error_rate_warning": 0.01,
      "error_rate_critical": 0.02,
      "error_rate_floor": 0,
      "database_latency_floor_ms": 5.0,
      "database_latency_ratios": {
        "info": 1.5,
        "warning": 2.0,
        "high": 3.0,
        "critical": 5.0
      },
      "busy_period_factor": 1.5
    },
    "services": {
      "booking": {
        "latency_acceptable_ms": 300,
        "latency_critical_ms": 500,
        "error_rate_acceptable": 0.002,
        "error_rate_floor": 0.002
      }
    },
    "busy_periods": [
      {
        "start": "2024-12-20T00:00:00",
        "end": "2025-01-05T23:59:59"
      }
    ]
  }
}
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>enabled</code></td><td>true</td><td>Enable SLO evaluation</td></tr>
<tr><td><code>allow_downgrade_to_informational</code></td><td>true</td><td>Allow severity reduction when SLO ok</td></tr>
<tr><td><code>require_slo_breach_for_critical</code></td><td>true</td><td>Only critical if SLO breached</td></tr>
<tr><td><code>latency_acceptable_ms</code></td><td>500</td><td>Latency SLO acceptable threshold</td></tr>
<tr><td><code>latency_critical_ms</code></td><td>1000</td><td>Latency SLO critical threshold</td></tr>
<tr><td><code>error_rate_acceptable</code></td><td>0.005</td><td>Error rate acceptable (0.5%)</td></tr>
<tr><td><code>error_rate_floor</code></td><td>0</td><td>Error rate suppression floor</td></tr>
<tr><td><code>database_latency_floor_ms</code></td><td>5.0</td><td>DB latency noise floor</td></tr>
</tbody>
</table>
</div>
<h3 id="fingerprinting-1"><a class="header" href="#fingerprinting-1">Fingerprinting</a></h3>
<pre><code class="language-json">{
  "fingerprinting": {
    "db_path": "./anomaly_state.db",
    "cleanup_max_age_hours": 72,
    "incident_separation_minutes": 30,
    "confirmation_cycles": 2,
    "resolution_grace_cycles": 3
  }
}
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>db_path</code></td><td>./anomaly_state.db</td><td>SQLite database path</td></tr>
<tr><td><code>cleanup_max_age_hours</code></td><td>72</td><td>Hours before cleanup</td></tr>
<tr><td><code>incident_separation_minutes</code></td><td>30</td><td>Gap triggering new incident</td></tr>
<tr><td><code>confirmation_cycles</code></td><td>2</td><td>Cycles to confirm (send alert)</td></tr>
<tr><td><code>resolution_grace_cycles</code></td><td>3</td><td>Cycles before closing</td></tr>
</tbody>
</table>
</div>
<h3 id="services"><a class="header" href="#services">Services</a></h3>
<pre><code class="language-json">{
  "services": {
    "critical": ["booking", "search", "mobile-api"],
    "standard": ["friday", "gambit", "titan"],
    "micro": ["fa5"],
    "admin": ["m2-fr-adm", "m2-it-adm"],
    "core": ["m2-bb", "m2-fr"]
  }
}
</code></pre>
<p>Service categories affect default contamination rates:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Category</th><th>Contamination</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>critical</td><td>0.03</td><td>Revenue-critical services</td></tr>
<tr><td>standard</td><td>0.05</td><td>Normal production</td></tr>
<tr><td>core</td><td>0.04</td><td>Platform infrastructure</td></tr>
<tr><td>admin</td><td>0.06</td><td>Administrative tools</td></tr>
<tr><td>micro</td><td>0.08</td><td>Low-traffic services</td></tr>
</tbody>
</table>
</div>
<h3 id="dependencies"><a class="header" href="#dependencies">Dependencies</a></h3>
<pre><code class="language-json">{
  "dependencies": {
    "graph": {
      "booking": ["search", "vms", "r2d2"],
      "vms": ["titan"],
      "search": ["catalog", "r2d2"]
    },
    "cascade_detection": {
      "enabled": true,
      "max_depth": 5
    }
  }
}
</code></pre>
<h3 id="model-configuration"><a class="header" href="#model-configuration">Model Configuration</a></h3>
<pre><code class="language-json">{
  "model": {
    "models_directory": "./smartbox_models/",
    "min_training_samples": 500,
    "min_multivariate_samples": 1000,
    "default_contamination": 0.05,
    "default_n_estimators": 200,
    "contamination_by_service": {
      "booking": 0.02,
      "search": 0.04
    }
  }
}
</code></pre>
<h3 id="time-periods"><a class="header" href="#time-periods">Time Periods</a></h3>
<pre><code class="language-json">{
  "time_periods": {
    "business_hours": {"start": 8, "end": 18, "weekdays_only": true},
    "evening_hours": {"start": 18, "end": 22, "weekdays_only": true},
    "night_hours": {"start": 22, "end": 6, "weekdays_only": true},
    "weekend_day": {"start": 8, "end": 22, "weekends_only": true},
    "weekend_night": {"start": 22, "end": 8, "weekends_only": true}
  }
}
</code></pre>
<h3 id="inference"><a class="header" href="#inference">Inference</a></h3>
<pre><code class="language-json">{
  "inference": {
    "alerts_directory": "./alerts/",
    "max_workers": 3,
    "inter_service_delay_seconds": 0.2,
    "check_drift": false
  }
}
</code></pre>
<h2 id="environment-variables"><a class="header" href="#environment-variables">Environment Variables</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Variable</th><th>Config Path</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>CONFIG_FILE</code></td><td>-</td><td>Path to config file</td></tr>
<tr><td><code>VM_ENDPOINT</code></td><td>victoria_metrics.endpoint</td><td>VictoriaMetrics URL</td></tr>
<tr><td><code>FINGERPRINT_DB</code></td><td>fingerprinting.db_path</td><td>SQLite path</td></tr>
<tr><td><code>OBSERVABILITY_URL</code></td><td>observability_api.base_url</td><td>API server URL</td></tr>
<tr><td><code>OBSERVABILITY_ENABLED</code></td><td>observability_api.enabled</td><td>Enable API</td></tr>
</tbody>
</table>
</div>
<h2 id="docker-configuration"><a class="header" href="#docker-configuration">Docker Configuration</a></h2>
<pre><code class="language-yaml">environment:
  - TZ=UTC
  - CONFIG_PATH=/app/config.json
  - TRAIN_SCHEDULE=0 2 * * *
  - INFERENCE_SCHEDULE=*/10 * * * *
volumes:
  - ./smartbox_models:/app/smartbox_models
  - ./data:/app/data
  - ./config.json:/app/config.json
</code></pre>
<h2 id="adding-new-services"><a class="header" href="#adding-new-services">Adding New Services</a></h2>
<ol>
<li>Add to appropriate category in <code>services</code> section</li>
<li>Optionally add per-service SLO thresholds</li>
<li>Optionally add contamination override</li>
<li>Run training: <code>docker compose run --rm yaga train</code></li>
<li>Verify: <code>docker compose run --rm yaga inference --verbose</code></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="api-payload-reference"><a class="header" href="#api-payload-reference">API Payload Reference</a></h1>
<p>Reference for the JSON payload structure sent by the inference engine.</p>
<h2 id="alert-types"><a class="header" href="#alert-types">Alert Types</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Type</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>anomaly_detected</code></td><td>Anomalies detected for service</td></tr>
<tr><td><code>no_anomaly</code></td><td>Service is healthy</td></tr>
<tr><td><code>metrics_unavailable</code></td><td>Metrics collection failed</td></tr>
</tbody>
</table>
</div>
<h2 id="top-level-structure"><a class="header" href="#top-level-structure">Top-Level Structure</a></h2>
<pre><code class="language-json">{
  "alert_type": "anomaly_detected",
  "service_name": "booking",
  "timestamp": "2024-01-15T10:30:00",
  "time_period": "business_hours",
  "model_name": "business_hours",
  "model_type": "time_aware_5period",

  "anomaly_count": 1,
  "overall_severity": "high",

  "anomalies": { ... },
  "current_metrics": { ... },
  "slo_evaluation": { ... },
  "exception_context": { ... },
  "service_graph_context": { ... },
  "fingerprinting": { ... }
}
</code></pre>
<h2 id="current-metrics"><a class="header" href="#current-metrics">Current Metrics</a></h2>
<pre><code class="language-json">{
  "current_metrics": {
    "request_rate": 52.7,
    "application_latency": 110.5,
    "dependency_latency": 1.4,
    "database_latency": 0.8,
    "error_rate": 0.0001
  }
}
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Metric</th><th>Unit</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>request_rate</td><td>req/s</td><td>Requests per second</td></tr>
<tr><td>application_latency</td><td>ms</td><td>Server processing time</td></tr>
<tr><td>dependency_latency</td><td>ms</td><td>External dependency latency</td></tr>
<tr><td>database_latency</td><td>ms</td><td>Database query time</td></tr>
<tr><td>error_rate</td><td>ratio</td><td>Error rate (0.05 = 5%)</td></tr>
</tbody>
</table>
</div>
<h2 id="anomaly-object"><a class="header" href="#anomaly-object">Anomaly Object</a></h2>
<pre><code class="language-json">{
  "latency_spike_recent": {
    "type": "consolidated",
    "root_metric": "application_latency",
    "severity": "high",
    "confidence": 0.80,
    "score": -0.5,
    "signal_count": 2,

    "description": "Latency degradation: 636ms (92nd percentile)",
    "interpretation": "Latency recently increased...",
    "pattern_name": "latency_spike_recent",

    "detection_signals": [ ... ],
    "recommended_actions": [ ... ],
    "comparison_data": { ... },

    "fingerprint_id": "anomaly_d18f6ae2bf62",
    "incident_id": "incident_31e9e23d4b2b",
    "status": "OPEN",
    "occurrence_count": 5,
    "is_confirmed": true
  }
}
</code></pre>
<h2 id="detection-signals"><a class="header" href="#detection-signals">Detection Signals</a></h2>
<pre><code class="language-json">{
  "detection_signals": [
    {
      "method": "isolation_forest",
      "type": "ml_isolation",
      "severity": "low",
      "score": -0.01,
      "direction": "high",
      "percentile": 91.7
    },
    {
      "method": "named_pattern_matching",
      "type": "multivariate_pattern",
      "severity": "high",
      "score": -0.5,
      "pattern": "latency_spike_recent"
    }
  ]
}
</code></pre>
<h2 id="slo-evaluation"><a class="header" href="#slo-evaluation">SLO Evaluation</a></h2>
<pre><code class="language-json">{
  "slo_evaluation": {
    "original_severity": "critical",
    "adjusted_severity": "low",
    "severity_changed": true,
    "slo_status": "ok",
    "slo_proximity": 0.56,
    "operational_impact": "informational",

    "latency_evaluation": {
      "status": "ok",
      "value": 280.0,
      "threshold_acceptable": 300,
      "proximity": 0.93
    },
    "error_rate_evaluation": {
      "status": "ok",
      "value": 0.001,
      "value_percent": "0.10%",
      "within_acceptable": true
    },
    "database_latency_evaluation": {
      "status": "warning",
      "value_ms": 25.0,
      "baseline_mean_ms": 10.0,
      "ratio": 2.5
    },
    "request_rate_evaluation": {
      "status": "ok",
      "type": "normal",
      "value_rps": 52.7,
      "baseline_mean_rps": 50.0,
      "ratio": 1.05
    },

    "explanation": "Severity adjusted from critical to low..."
  }
}
</code></pre>
<h2 id="cascade-analysis"><a class="header" href="#cascade-analysis">Cascade Analysis</a></h2>
<pre><code class="language-json">{
  "cascade_analysis": {
    "is_cascade": true,
    "root_cause_service": "titan",
    "affected_chain": ["titan", "vms"],
    "cascade_type": "upstream_cascade",
    "confidence": 0.85,
    "propagation_path": [
      {"service": "titan", "has_anomaly": true, "anomaly_type": "database_bottleneck"},
      {"service": "vms", "has_anomaly": true, "anomaly_type": "downstream_cascade"}
    ]
  }
}
</code></pre>
<h2 id="fingerprinting-2"><a class="header" href="#fingerprinting-2">Fingerprinting</a></h2>
<pre><code class="language-json">{
  "fingerprinting": {
    "service_name": "booking",
    "model_name": "business_hours",
    "timestamp": "2025-12-17T13:56:06",
    "overall_action": "UPDATE",
    "total_active_incidents": 1,
    "total_alerting_incidents": 1,

    "action_summary": {
      "incident_creates": 0,
      "incident_continues": 1,
      "incident_closes": 0,
      "newly_confirmed": 0
    },
    "status_summary": {
      "suspected": 0,
      "confirmed": 1,
      "recovering": 0
    },
    "resolved_incidents": [],
    "newly_confirmed_incidents": []
  }
}
</code></pre>
<h2 id="exception-context"><a class="header" href="#exception-context">Exception Context</a></h2>
<p>Present when error SLO breached:</p>
<pre><code class="language-json">{
  "exception_context": {
    "service_name": "search",
    "timestamp": "2024-01-15T10:30:00",
    "total_exception_rate": 0.35,
    "exception_count": 3,
    "top_exceptions": [
      {
        "type": "Smartbox\\Search\\R2D2\\Exception\\R2D2Exception",
        "short_name": "R2D2Exception",
        "rate": 0.217,
        "percentage": 62.0
      }
    ],
    "query_successful": true
  }
}
</code></pre>
<h2 id="service-graph-context"><a class="header" href="#service-graph-context">Service Graph Context</a></h2>
<p>Present when client latency SLO breached:</p>
<pre><code class="language-json">{
  "service_graph_context": {
    "service_name": "cmhub",
    "total_request_rate": 2.1,
    "routes": [
      {
        "server": "r2d2",
        "route": "roomavailabilitylistener",
        "request_rate": 0.117,
        "avg_latency_ms": 29.0,
        "percentage": 5.6
      }
    ],
    "top_route": { ... },
    "slowest_route": { ... },
    "summary": "Service graph for cmhub..."
  }
}
</code></pre>
<h2 id="resolution-payload-1"><a class="header" href="#resolution-payload-1">Resolution Payload</a></h2>
<pre><code class="language-json">{
  "resolved_incidents": [
    {
      "fingerprint_id": "anomaly_061598e9ca91",
      "incident_id": "incident_abc123def456",
      "anomaly_name": "database_degradation",
      "fingerprint_action": "RESOLVE",
      "incident_action": "CLOSE",
      "final_severity": "medium",
      "resolved_at": "2025-12-17T14:30:00",
      "total_occurrences": 5,
      "incident_duration_minutes": 45,
      "first_seen": "2025-12-17T13:45:00",
      "resolution_reason": "resolved"
    }
  ]
}
</code></pre>
<h2 id="severity-values"><a class="header" href="#severity-values">Severity Values</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Severity</th><th>Priority</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>critical</td><td>1</td><td>Immediate action required</td></tr>
<tr><td>high</td><td>2</td><td>Investigate promptly</td></tr>
<tr><td>medium</td><td>3</td><td>Monitor closely</td></tr>
<tr><td>low</td><td>4</td><td>Informational</td></tr>
<tr><td>none</td><td>5</td><td>No anomaly</td></tr>
</tbody>
</table>
</div>
<h2 id="named-patterns"><a class="header" href="#named-patterns">Named Patterns</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Pattern</th><th>Severity</th><th>Trigger Condition</th></tr>
</thead>
<tbody>
<tr><td><code>traffic_surge_healthy</code></td><td>low</td><td>High traffic, normal latency/errors</td></tr>
<tr><td><code>traffic_surge_degrading</code></td><td>high</td><td>High traffic, high latency</td></tr>
<tr><td><code>traffic_surge_failing</code></td><td>critical</td><td>High traffic, high latency, high errors</td></tr>
<tr><td><code>traffic_cliff</code></td><td>critical</td><td>Very low traffic</td></tr>
<tr><td><code>latency_spike_recent</code></td><td>high</td><td>Normal traffic, high latency</td></tr>
<tr><td><code>internal_latency_issue</code></td><td>high</td><td>High latency, healthy deps</td></tr>
<tr><td><code>error_rate_elevated</code></td><td>high</td><td>Elevated error rate</td></tr>
<tr><td><code>error_rate_critical</code></td><td>critical</td><td>Very high error rate</td></tr>
<tr><td><code>fast_failure</code></td><td>critical</td><td>Low latency, high errors</td></tr>
<tr><td><code>fast_rejection</code></td><td>critical</td><td>Very low latency, very high errors</td></tr>
<tr><td><code>database_bottleneck</code></td><td>high</td><td>High DB latency, DB dominant</td></tr>
<tr><td><code>database_degradation</code></td><td>medium</td><td>High DB latency, compensating</td></tr>
<tr><td><code>upstream_cascade</code></td><td>high</td><td>High latency + upstream anomaly</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h1>
<p>This guide helps you diagnose and resolve common issues with the anomaly detection system. Each section follows a systematic approach: identify symptoms, understand root causes, and apply targeted solutions.</p>
<h2 id="how-to-use-this-guide-1"><a class="header" href="#how-to-use-this-guide-1">How to Use This Guide</a></h2>
<p>When troubleshooting:</p>
<ol>
<li><strong>Identify the symptom</strong> - What behavior are you observing?</li>
<li><strong>Gather evidence</strong> - Collect relevant logs and configuration</li>
<li><strong>Understand the cause</strong> - Why is this happening?</li>
<li><strong>Apply the fix</strong> - Make targeted changes</li>
<li><strong>Verify the solution</strong> - Confirm the issue is resolved</li>
</ol>
<pre><code>┌─────────────────────────────────────────────────────────────────┐
│                   TROUBLESHOOTING WORKFLOW                      │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│    Symptom                                                      │
│       │                                                         │
│       ▼                                                         │
│    Which category?                                              │
│       │                                                         │
│       ├──▶ Detection Issues (no alerts, false positives)        │
│       ├──▶ Incident Lifecycle (confirmation, resolution)        │
│       ├──▶ Metrics Issues (unavailable, validation)             │
│       ├──▶ SLO Issues (evaluation, thresholds)                  │
│       ├──▶ Training Issues (failures, drift)                    │
│       └──▶ Container Issues (restarts, resources)               │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
</code></pre>
<hr>
<h2 id="detection-issues"><a class="header" href="#detection-issues">Detection Issues</a></h2>
<h3 id="no-anomalies-detected-when-expected"><a class="header" href="#no-anomalies-detected-when-expected">No Anomalies Detected (When Expected)</a></h3>
<p><strong>Symptoms</strong>:</p>
<ul>
<li>Real incidents are occurring but the system generates no alerts</li>
<li>Dashboards show problems but Yaga2 reports <code>alert_type: "no_anomaly"</code></li>
<li>Users report issues but your alerting is silent</li>
</ul>
<p><strong>Why This Happens</strong>:</p>
<p>The detection pipeline has multiple stages where anomalies can be “filtered out”:</p>
<pre><code>Metrics Collection → ML Detection → Pattern Matching → SLO Evaluation → Alerting
        │                 │                │                 │
        │                 │                │                 │
     No data?         No model?      No pattern?      SLO suppressed?
     (fail here)      (fail here)    (no match)       (low severity)
</code></pre>
<p><strong>Diagnostic Decision Tree</strong>:</p>
<pre><code>No alerts generated?
        │
        ├─▶ Are metrics being collected?
        │       │
        │       ├─ NO ──▶ Check VictoriaMetrics connectivity
        │       │         (See: metrics_unavailable section)
        │       │
        │       └─ YES ─▶ Do models exist for this service?
        │                       │
        │                       ├─ NO ──▶ Train models
        │                       │
        │                       └─ YES ─▶ Is the service in config?
        │                                       │
        │                                       ├─ NO ──▶ Add to config
        │                                       │
        │                                       └─ YES ─▶ Check contamination
        │                                                 and SLO settings
</code></pre>
<p><strong>Step 1: Verify models exist</strong>:</p>
<pre><code class="language-bash"># Check if models directory exists for your service
ls -la smartbox_models/&lt;service-name&gt;/

# Expected output shows time period subdirectories:
# drwxr-xr-x  business_hours/
# drwxr-xr-x  evening_hours/
# drwxr-xr-x  night_hours/
# drwxr-xr-x  weekend_day/
# drwxr-xr-x  weekend_night/
</code></pre>
<p>If no models exist, train them:</p>
<pre><code class="language-bash">docker compose run --rm yaga train
</code></pre>
<p><strong>Step 2: Check inference logs</strong>:</p>
<pre><code class="language-bash"># View recent inference activity
docker compose exec yaga tail -50 /app/logs/inference.log

# Look for your service
docker compose exec yaga grep "&lt;service-name&gt;" /app/logs/inference.log | tail -20
</code></pre>
<p><strong>Step 3: Run verbose inference</strong>:</p>
<pre><code class="language-bash"># This shows detailed detection output
docker compose run --rm yaga inference --verbose
</code></pre>
<p>Look for output like:</p>
<pre><code>Service: booking
  Time period: business_hours
  Metrics collected: ✓
  ML detection: No anomalies (all scores normal)
  Pattern matching: No patterns matched
  Result: alert_type = no_anomaly
</code></pre>
<p><strong>Common Causes and Solutions</strong>:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Cause</th><th>How to Identify</th><th>Solution</th></tr>
</thead>
<tbody>
<tr><td>No trained model</td><td><code>ls smartbox_models/&lt;service&gt;/</code> is empty</td><td>Run <code>docker compose run --rm yaga train</code></td></tr>
<tr><td>Service not in config</td><td>Service not listed in <code>config.json</code></td><td>Add to appropriate <code>services</code> category</td></tr>
<tr><td>Contamination too high</td><td>Model trained with high contamination (e.g., 0.10)</td><td>Lower to 0.03-0.05 and retrain</td></tr>
<tr><td>Model stale</td><td>Model trained on old data; current behavior differs</td><td>Retrain with recent data</td></tr>
<tr><td>Wrong time period</td><td>Checking business_hours during night</td><td>Verify timezone configuration</td></tr>
<tr><td>SLO suppression</td><td>Anomaly detected but SLO says “ok”</td><td>Check SLO thresholds (may be too lenient)</td></tr>
</tbody>
</table>
</div>
<p><strong>Example Investigation</strong>:</p>
<pre><code>Scenario: booking service had a 5-minute outage but no alert was generated

Step 1: Check models
$ ls smartbox_models/booking/
business_hours/  evening_hours/  night_hours/  weekend_day/  weekend_night/
✓ Models exist

Step 2: Check if service is in config
$ grep -A5 '"services"' config.json
"services": {
  "critical": ["booking", "search"],
  ...
}
✓ Service is configured

Step 3: Run verbose inference during the issue
$ docker compose run --rm yaga inference --verbose
...
booking: ML detected anomaly (score: -0.45, severity: high)
booking: SLO evaluation: latency 250ms &lt; 500ms acceptable → status: ok
booking: Severity adjusted: high → low (SLO status ok)
...

Diagnosis: SLO threshold too lenient (500ms acceptable)
Fix: Lower latency_acceptable_ms to 200ms
</code></pre>
<hr>
<h3 id="too-many-false-positives"><a class="header" href="#too-many-false-positives">Too Many False Positives</a></h3>
<p><strong>Symptoms</strong>:</p>
<ul>
<li>Alerts fire during normal operation</li>
<li>Team ignores alerts due to noise (alert fatigue)</li>
<li>Anomalies detected don’t correlate with real problems</li>
</ul>
<p><strong>Understanding False Positives</strong>:</p>
<p>False positives occur when the ML model flags behavior as “anomalous” when it’s actually normal. This happens because:</p>
<pre><code>┌─────────────────────────────────────────────────────────────────┐
│                    WHY FALSE POSITIVES HAPPEN                   │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. Contamination Too Low                                       │
│     ┌──────────────────────────────────┐                       │
│     │ Training data distribution       │                       │
│     │                                  │                       │
│     │         ●●●●●●                   │                       │
│     │        ●●●●●●●●                  │                       │
│     │       ●●●●●●●●●●   ← Model       │                       │
│     │        ●●●●●●●●      learns      │                       │
│     │         ●●●●●●       "normal"    │                       │
│     │           ▲                      │                       │
│     │     If contamination=0.02,      │                       │
│     │     only top 2% flagged         │                       │
│     └──────────────────────────────────┘                       │
│     → Normal variance at edges gets flagged                    │
│                                                                 │
│  2. Seasonality Not Captured                                    │
│     Model trained during quiet period, now in busy season       │
│                                                                 │
│  3. Recent Behavior Change                                      │
│     Deployment changed baseline, model is stale                 │
│                                                                 │
│  4. Time Period Mismatch                                        │
│     Night traffic patterns flagged by business_hours model      │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
</code></pre>
<p><strong>Solution 1: Increase contamination rate</strong></p>
<p>The contamination rate tells the model what percentage of training data to consider “anomalous”. Higher values = less sensitive detection.</p>
<pre><code class="language-json">{
  "model": {
    "contamination_by_service": {
      "noisy-service": 0.08
    }
  }
}
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Contamination</th><th>Sensitivity</th><th>Best For</th></tr>
</thead>
<tbody>
<tr><td>0.02-0.03</td><td>Very high</td><td>Critical services where every alert matters</td></tr>
<tr><td>0.05</td><td>Balanced</td><td>Most production services</td></tr>
<tr><td>0.08-0.10</td><td>Lower</td><td>Noisy services, variable traffic</td></tr>
</tbody>
</table>
</div>
<p><strong>Solution 2: Adjust SLO thresholds</strong></p>
<p>SLO evaluation can suppress anomalies that don’t have operational impact:</p>
<pre><code class="language-json">{
  "slos": {
    "services": {
      "noisy-service": {
        "latency_acceptable_ms": 600,
        "error_rate_floor": 0.005
      }
    }
  }
}
</code></pre>
<p>This tells the system: “Even if ML detects an anomaly, don’t alert unless latency exceeds 600ms or error rate exceeds 0.5%.”</p>
<p><strong>Solution 3: Retrain models</strong></p>
<p>After changing configuration, always retrain:</p>
<pre><code class="language-bash">docker compose run --rm yaga train
</code></pre>
<p><strong>Measuring Improvement</strong>:</p>
<p>Track your false positive rate before and after changes:</p>
<pre><code>Before tuning:
  Total alerts (7 days): 145
  Actionable: 12 (8%)
  False positives: 133 (92%)

After increasing contamination 0.03 → 0.06:
  Total alerts (7 days): 34
  Actionable: 11 (32%)
  False positives: 23 (68%)

After adjusting SLO thresholds:
  Total alerts (7 days): 18
  Actionable: 10 (56%)
  False positives: 8 (44%)
</code></pre>
<hr>
<h3 id="alerts-always-low-severity"><a class="header" href="#alerts-always-low-severity">Alerts Always Low Severity</a></h3>
<p><strong>Symptom</strong>:</p>
<ul>
<li>All alerts show <code>severity: low</code></li>
<li>Never see <code>high</code> or <code>critical</code> alerts</li>
<li>Real incidents don’t escalate properly</li>
</ul>
<p><strong>Understanding Severity Flow</strong>:</p>
<pre><code>ML Detection (original severity)
        │
        ▼
SLO Evaluation
        │
        ├─ SLO breached ──▶ Keep/Escalate to CRITICAL
        │
        ├─ SLO warning ───▶ Keep HIGH
        │
        └─ SLO ok ────────▶ Downgrade to LOW
</code></pre>
<p>When SLO status is <code>ok</code> (all metrics within acceptable thresholds), severity is <strong>always</strong> downgraded to <code>low</code>. This is intentional—if metrics are operationally acceptable, the alert shouldn’t be high priority.</p>
<p><strong>The Problem</strong>:</p>
<p>If your SLO thresholds are too lenient, anomalies will never breach them:</p>
<pre><code>Example: Latency threshold too high

Current latency: 450ms (genuinely slow for this service)
SLO acceptable: 500ms (too lenient)
SLO status: "ok" (450 &lt; 500)
Result: severity = low (even though users are impacted)
</code></pre>
<p><strong>Solution: Tighten SLO thresholds</strong></p>
<p>Review and lower your acceptable thresholds:</p>
<pre><code class="language-json">{
  "slos": {
    "services": {
      "booking": {
        "latency_acceptable_ms": 200,
        "latency_warning_ms": 300,
        "latency_critical_ms": 400,
        "error_rate_acceptable": 0.002,
        "error_rate_warning": 0.005,
        "error_rate_critical": 0.01
      }
    }
  }
}
</code></pre>
<p><strong>Before vs After</strong>:</p>
<pre><code>BEFORE (lenient thresholds):
  Latency acceptable: 500ms
  Current latency: 350ms
  SLO status: ok (350 &lt; 500)
  ML severity: high → Adjusted: low

AFTER (tightened thresholds):
  Latency acceptable: 200ms
  Current latency: 350ms
  SLO status: warning (350 &gt; 200, &lt; 400)
  ML severity: high → Adjusted: high (kept)
</code></pre>
<p><strong>Finding the Right Thresholds</strong>:</p>
<ol>
<li>
<p>Look at your historical latency distribution:</p>
<pre><code class="language-bash"># In VictoriaMetrics, query p50, p90, p99 for your service
</code></pre>
</li>
<li>
<p>Set thresholds based on percentiles:</p>
<ul>
<li><code>acceptable</code>: Around p75 (allows normal variance)</li>
<li><code>warning</code>: Around p90 (getting concerning)</li>
<li><code>critical</code>: Around p99 (definitely a problem)</li>
</ul>
</li>
</ol>
<hr>
<h2 id="incident-lifecycle-issues"><a class="header" href="#incident-lifecycle-issues">Incident Lifecycle Issues</a></h2>
<h3 id="alerts-not-being-sent"><a class="header" href="#alerts-not-being-sent">Alerts Not Being Sent</a></h3>
<p><strong>Symptom</strong>:</p>
<ul>
<li>Verbose output shows anomaly detected</li>
<li>Web API never receives the alert</li>
<li>Dashboard shows no incidents</li>
</ul>
<p><strong>Understanding Alert Flow</strong>:</p>
<pre><code>Anomaly Detected
        │
        ▼
Is incident confirmed?
        │
        ├─ NO (SUSPECTED) ──▶ NOT sent to web API
        │                      └─ Wait for next cycle
        │
        └─ YES (OPEN) ───────▶ Sent to web API
                                └─ Dashboard shows alert
</code></pre>
<p><strong>Diagnostic Steps</strong>:</p>
<p><strong>Step 1: Check if anomaly is in SUSPECTED state</strong></p>
<pre><code class="language-bash">docker compose run --rm yaga inference --verbose
</code></pre>
<p>Look for:</p>
<pre><code>fingerprint_action: CREATE
status: SUSPECTED
is_confirmed: false
cycles_to_confirm: 1
</code></pre>
<p>If <code>status: SUSPECTED</code>, the anomaly hasn’t been confirmed yet. Wait for the next detection cycle.</p>
<p><strong>Step 2: Check web API connectivity</strong></p>
<pre><code class="language-bash"># Test if web API is reachable
docker compose exec yaga curl http://observability-api:8000/health

# Expected: {"status": "healthy"}
</code></pre>
<p>If this fails, check:</p>
<ul>
<li>Is the observability API running?</li>
<li>Is the network configured correctly?</li>
<li>Are there firewall rules blocking traffic?</li>
</ul>
<p><strong>Step 3: Check if API is enabled</strong></p>
<pre><code class="language-json">{
  "observability_api": {
    "enabled": true,
    "base_url": "http://observability-api:8000"
  }
}
</code></pre>
<p>If <code>enabled: false</code>, alerts won’t be sent.</p>
<p><strong>Step 4: Check for API errors in logs</strong></p>
<pre><code class="language-bash">docker compose exec yaga grep -i "api" /app/logs/inference.log | tail -20

# Look for:
# - "API call failed"
# - "Connection refused"
# - "Timeout"
</code></pre>
<hr>
<h3 id="orphaned-incidents-in-web-api"><a class="header" href="#orphaned-incidents-in-web-api">Orphaned Incidents in Web API</a></h3>
<p><strong>Symptom</strong>:</p>
<ul>
<li>Web API shows OPEN incidents that never resolve</li>
<li>Incidents stuck in dashboard for days/weeks</li>
<li><code>consecutive_detections = 1</code> on stuck incidents</li>
</ul>
<p><strong>Why This Happens</strong>:</p>
<p>Before v1.3.2, SUSPECTED incidents were sent to the web API. If they expired without confirmation, no resolution was sent:</p>
<pre><code>OLD BEHAVIOR (pre-v1.3.2):
─────────────────────────
10:00  Anomaly detected → SUSPECTED incident created
10:00  Alert sent to web API → Web API creates OPEN incident
10:03  Anomaly not detected → SUSPECTED expires
10:03  No resolution sent → Web API incident stays OPEN forever!

NEW BEHAVIOR (v1.3.2+):
───────────────────────
10:00  Anomaly detected → SUSPECTED incident created
       (NOT sent to web API - waiting for confirmation)
10:03  Anomaly not detected → SUSPECTED expires silently
       Web API never knew about it → No orphan created
</code></pre>
<p><strong>Identifying Orphaned Incidents</strong>:</p>
<p>Query your web API database:</p>
<pre><code class="language-sql">-- Find orphaned incidents (never confirmed but sent)
SELECT incident_id, service_name, created_at, consecutive_detections
FROM incidents
WHERE status = 'OPEN'
  AND consecutive_detections = 1
  AND created_at &lt; NOW() - INTERVAL '24 hours';
</code></pre>
<p><strong>Solutions</strong>:</p>
<ol>
<li><strong>Manual cleanup</strong>: Close orphaned incidents in the web API</li>
<li><strong>Wait for upgrade</strong>: v1.3.2+ won’t create new orphans</li>
<li><strong>Automated cleanup</strong>: Add a job to close incidents older than X days with <code>consecutive_detections = 1</code></li>
</ol>
<hr>
<h3 id="incidents-restarting-unexpectedly"><a class="header" href="#incidents-restarting-unexpectedly">Incidents Restarting Unexpectedly</a></h3>
<p><strong>Symptom</strong>:</p>
<ul>
<li>Same anomaly pattern creates multiple incident IDs</li>
<li>Incident history shows many short incidents instead of one long one</li>
<li>Resolution reason shows <code>auto_stale</code></li>
</ul>
<p><strong>Understanding Staleness</strong>:</p>
<p>The system considers an incident “stale” if the time gap since last detection exceeds <code>incident_separation_minutes</code> (default: 30):</p>
<pre><code>Timeline with 45-minute gap:
──────────────────────────────────────────────────────────────
10:00  Anomaly detected → incident_abc created (SUSPECTED)
10:03  Detected again → incident_abc confirmed (OPEN)
10:06  Detected again → incident_abc continues
...
10:30  Last detection
       (service recovers but then has a second issue)
11:15  Anomaly detected → 45 min gap &gt; 30 min threshold
                        → incident_abc auto-closed (stale)
                        → incident_xyz created (new incident)
</code></pre>
<p><strong>When This Is Correct vs. Problematic</strong>:</p>
<pre><code>CORRECT (two separate issues):
──────────────────────────────
10:00-10:30: Database slow due to query
11:15-12:00: Database slow due to disk I/O

These ARE two different incidents - staleness helps track them separately.


PROBLEMATIC (one issue, intermittent symptoms):
───────────────────────────────────────────────
10:00-10:30: Network flaky (detected)
10:30-11:15: Network stable (not detected)
11:15-11:45: Network flaky again (same issue)

This is ONE issue but creates multiple incidents.
</code></pre>
<p><strong>Solution: Increase separation threshold</strong></p>
<pre><code class="language-json">{
  "fingerprinting": {
    "incident_separation_minutes": 60
  }
}
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Threshold</th><th>Use Case</th></tr>
</thead>
<tbody>
<tr><td>15 min</td><td>Fast-resolving issues, want precise tracking</td></tr>
<tr><td>30 min</td><td>Default, good balance</td></tr>
<tr><td>60 min</td><td>Intermittent issues, prefer fewer incidents</td></tr>
<tr><td>120 min</td><td>Very intermittent, consolidate aggressively</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="metrics-issues"><a class="header" href="#metrics-issues">Metrics Issues</a></h2>
<h3 id="metrics_unavailable-alerts"><a class="header" href="#metrics_unavailable-alerts">metrics_unavailable Alerts</a></h3>
<p><strong>Symptom</strong>:</p>
<ul>
<li><code>alert_type: "metrics_unavailable"</code> instead of detection results</li>
<li>Detection skipped for some/all services</li>
<li>Circuit breaker messages in logs</li>
</ul>
<p><strong>Understanding This Alert</strong>:</p>
<p>The system returns <code>metrics_unavailable</code> when it cannot collect metrics reliably. This prevents false alerts—if we can’t read metrics, we shouldn’t guess.</p>
<pre><code>┌─────────────────────────────────────────────────────────────────┐
│                  METRICS UNAVAILABLE SCENARIOS                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Scenario 1: VictoriaMetrics Down                              │
│  ─────────────────────────────────                             │
│  All metrics fail → All services return metrics_unavailable    │
│                                                                 │
│  Scenario 2: Critical Metric Failed                            │
│  ─────────────────────────────────                             │
│  request_rate failed → Detection skipped (prevents false       │
│  "traffic cliff" from 0.0 value)                               │
│                                                                 │
│  Scenario 3: Partial Failure (Non-Critical)                    │
│  ──────────────────────────────────────────                    │
│  database_latency failed, others OK → Detection runs with      │
│  warning, partial_metrics_failure in output                    │
│                                                                 │
│  Scenario 4: Circuit Breaker Open                              │
│  ────────────────────────────────                              │
│  Too many failures → Circuit breaker prevents further calls    │
│  Wait for timeout (5 min) or fix underlying issue              │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
</code></pre>
<p><strong>Diagnostic Steps</strong>:</p>
<p><strong>Step 1: Test VictoriaMetrics connectivity</strong></p>
<pre><code class="language-bash"># From inside the container
docker compose exec yaga curl -s "http://vm:8428/api/v1/status/buildinfo"

# Expected: JSON with version info
# If this fails, VM is unreachable
</code></pre>
<p><strong>Step 2: Check for circuit breaker</strong></p>
<pre><code class="language-bash">docker compose exec yaga tail -50 /app/logs/inference.log | grep -i "circuit"

# Look for:
# "Circuit breaker OPEN - too many failures"
# "Circuit breaker will reset in X seconds"
</code></pre>
<p><strong>Step 3: Check which metrics failed</strong></p>
<p>Look at the output:</p>
<pre><code class="language-json">{
  "alert_type": "metrics_unavailable",
  "failed_metrics": ["request_rate", "application_latency"],
  "collection_errors": {
    "request_rate": "Connection timeout after 10s",
    "application_latency": "Connection timeout after 10s"
  }
}
</code></pre>
<p><strong>Solutions</strong>:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Problem</th><th>Solution</th></tr>
</thead>
<tbody>
<tr><td>VM down</td><td>Restart VictoriaMetrics, check VM health</td></tr>
<tr><td>Network issue</td><td>Check firewall, DNS, routing</td></tr>
<tr><td>Circuit breaker open</td><td>Wait 5 minutes or fix underlying issue</td></tr>
<tr><td>Timeout</td><td>Increase <code>timeout_seconds</code> in config</td></tr>
</tbody>
</table>
</div>
<p><strong>Circuit Breaker Behavior</strong>:</p>
<pre><code>Normal Operation:
  Requests succeed → Circuit CLOSED

5 consecutive failures:
  Circuit OPENS → All requests fail fast (no retry)

After 5 minutes (configurable):
  Circuit HALF-OPEN → One request allowed

  If succeeds → Circuit CLOSES
  If fails → Circuit stays OPEN for another timeout
</code></pre>
<hr>
<h3 id="validation-warnings"><a class="header" href="#validation-warnings">Validation Warnings</a></h3>
<p><strong>Symptom</strong>:</p>
<ul>
<li><code>validation_warnings</code> array in output</li>
<li>Messages like “capping at 1.0” or “using 0.0”</li>
</ul>
<p><strong>Understanding Validation</strong>:</p>
<p>Before metrics are processed, they’re validated at the inference boundary:</p>
<pre><code>Raw Metric Value
        │
        ▼
   Validation
        │
        ├─ NaN/Inf? ──────────▶ Replace with 0.0
        ├─ Negative rate? ────▶ Cap at 0.0
        ├─ Error rate &gt; 1.0? ─▶ Cap at 1.0
        ├─ Latency &gt; 5 min? ──▶ Cap at 300,000ms
        └─ Request rate &gt; 1M? ▶ Cap at 1,000,000
        │
        ▼
   Sanitized Value (used for detection)
   + Warning (logged for investigation)
</code></pre>
<p><strong>Example Warnings</strong>:</p>
<pre><code class="language-json">{
  "validation_warnings": [
    "error_rate: value 1.5 &gt; 1.0, capping at 1.0",
    "application_latency: negative value -50, using 0.0",
    "request_rate: NaN detected, using 0.0"
  ]
}
</code></pre>
<p><strong>Why This Happens</strong>:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Warning</th><th>Likely Cause</th><th>Investigation</th></tr>
</thead>
<tbody>
<tr><td>Error rate &gt; 1.0</td><td>Metric miscalculation upstream</td><td>Check error rate query definition</td></tr>
<tr><td>Negative latency</td><td>Clock skew or calculation bug</td><td>Check latency metric source</td></tr>
<tr><td>NaN/Inf</td><td>Division by zero, no data</td><td>Check if service was down</td></tr>
<tr><td>Extreme values</td><td>Metric spike or bug</td><td>Verify in VictoriaMetrics UI</td></tr>
</tbody>
</table>
</div>
<p><strong>Action Required</strong>:</p>
<p>Validation warnings are <strong>informational</strong>. Detection still runs with sanitized values. However, frequent warnings indicate upstream data quality issues that should be investigated.</p>
<pre><code class="language-bash"># Track warning frequency
docker compose exec yaga grep "validation_warnings" /app/logs/inference.log | wc -l
</code></pre>
<hr>
<h2 id="slo-issues"><a class="header" href="#slo-issues">SLO Issues</a></h2>
<h3 id="slo-evaluation-failed"><a class="header" href="#slo-evaluation-failed">SLO Evaluation Failed</a></h3>
<p><strong>Symptom</strong>:</p>
<ul>
<li>Warning in logs about SLO evaluation</li>
<li>Anomalies not being severity-adjusted</li>
<li>Missing <code>slo_evaluation</code> in output</li>
</ul>
<p><strong>Common Causes</strong>:</p>
<p><strong>1. SLO not enabled</strong></p>
<pre><code class="language-json">{
  "slos": {
    "enabled": false
  }
}
</code></pre>
<p>Set to <code>true</code> to enable SLO evaluation.</p>
<p><strong>2. Service not configured</strong></p>
<p>If a service isn’t in the SLO config, it uses defaults:</p>
<pre><code class="language-json">{
  "slos": {
    "services": {
      "booking": {
        "latency_acceptable_ms": 300
      }
    }
  }
}
</code></pre>
<p>Services not listed use values from <code>slos.defaults</code>.</p>
<p><strong>3. Missing training statistics</strong></p>
<p>SLO evaluation for database latency requires training baseline. If the model doesn’t have statistics, evaluation is skipped.</p>
<pre><code class="language-bash"># Check if training statistics exist
docker compose exec yaga cat smartbox_models/&lt;service&gt;/business_hours/metadata.json | python -c "import sys,json; d=json.load(sys.stdin); print(d.get('statistics', {}))"
</code></pre>
<hr>
<h3 id="database-latency-always-ok"><a class="header" href="#database-latency-always-ok">Database Latency Always OK</a></h3>
<p><strong>Symptom</strong>:</p>
<ul>
<li>ML detects database latency anomaly</li>
<li>But <code>database_latency_evaluation.status: "ok"</code></li>
<li>Even when database is clearly slow</li>
</ul>
<p><strong>Understanding the Noise Floor</strong>:</p>
<p>Database latency uses a hybrid approach:</p>
<pre><code>Current DB Latency
        │
        ▼
   Below noise floor?
        │
        ├─ YES (e.g., 2ms &lt; 5ms floor)
        │       │
        │       └─▶ status: "ok" (noise filtered)
        │
        └─ NO (above floor)
                │
                └─▶ Calculate ratio to baseline
                        │
                        └─▶ Ratio-based status
</code></pre>
<p><strong>Why This Design</strong>:</p>
<p>For services with very fast databases (sub-millisecond), small absolute changes are operationally meaningless:</p>
<pre><code>Without noise floor:
  Baseline: 0.3ms
  Current: 0.6ms
  Ratio: 2.0x → status: "warning"

  But 0.6ms is still FAST! This alert is noise.

With noise floor (5ms):
  Current: 0.6ms &lt; 5ms floor
  status: "ok" (filtered)

  Operationally correct - 0.6ms is fine.
</code></pre>
<p><strong>Adjusting the Floor</strong>:</p>
<p>For services with fast databases where you want to detect small changes:</p>
<pre><code class="language-json">{
  "slos": {
    "services": {
      "fast-db-service": {
        "database_latency_floor_ms": 1.0
      }
    }
  }
}
</code></pre>
<p>For services with slow databases where 5ms is normal:</p>
<pre><code class="language-json">{
  "slos": {
    "services": {
      "slow-db-service": {
        "database_latency_floor_ms": 10.0
      }
    }
  }
}
</code></pre>
<p><strong>Checking Current Configuration</strong>:</p>
<pre><code class="language-json">{
  "database_latency_evaluation": {
    "value_ms": 2.0,
    "baseline_mean_ms": 1.5,
    "floor_ms": 5.0,
    "below_floor": true,
    "ratio": 0.0,
    "status": "ok"
  }
}
</code></pre>
<p>If <code>below_floor: true</code>, the value is being filtered. Lower the floor if needed.</p>
<hr>
<h2 id="training-issues"><a class="header" href="#training-issues">Training Issues</a></h2>
<h3 id="training-fails"><a class="header" href="#training-fails">Training Fails</a></h3>
<p><strong>Symptoms</strong>:</p>
<ul>
<li>Models not updating (old timestamps)</li>
<li>Training command exits with error</li>
<li>No model files created</li>
</ul>
<p><strong>Diagnostic Checklist</strong>:</p>
<pre><code>Training Failed?
        │
        ├─▶ 1. Can we reach VictoriaMetrics?
        │       │
        │       └─ No → Fix network/VM connectivity
        │
        ├─▶ 2. Is there enough historical data?
        │       │
        │       └─ &lt; 30 days → Wait for data accumulation
        │
        ├─▶ 3. Is there enough disk space?
        │       │
        │       └─ Low → Clean old models/logs
        │
        ├─▶ 4. Is there enough memory?
        │       │
        │       └─ OOM → Increase container memory
        │
        └─▶ 5. Is the config valid?
                │
                └─ Invalid JSON → Fix syntax
</code></pre>
<p><strong>Step 1: Check training logs</strong></p>
<pre><code class="language-bash">docker compose exec yaga cat /app/logs/train.log

# Look for:
# - "Training failed for service X"
# - "Not enough data points"
# - "Connection refused"
# - "MemoryError"
</code></pre>
<p><strong>Step 2: Check VictoriaMetrics</strong></p>
<pre><code class="language-bash">docker compose exec yaga curl -s "http://vm:8428/api/v1/status/buildinfo"

# If this fails, VM is unreachable
</code></pre>
<p><strong>Step 3: Check disk space</strong></p>
<pre><code class="language-bash">df -h

# Models typically need 10-50MB per service
# Ensure at least 1GB free
</code></pre>
<p><strong>Step 4: Run training manually</strong></p>
<pre><code class="language-bash">docker compose run --rm yaga train 2&gt;&amp;1 | tee train_output.log

# This shows real-time output for diagnosis
</code></pre>
<p><strong>Common Causes and Solutions</strong>:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Cause</th><th>Symptoms</th><th>Solution</th></tr>
</thead>
<tbody>
<tr><td>VM unreachable</td><td>“Connection refused”</td><td>Check network, VM status</td></tr>
<tr><td>Not enough data</td><td>“Insufficient samples”</td><td>Wait for 30 days of data</td></tr>
<tr><td>Disk full</td><td>“No space left”</td><td>Clean old models: <code>rm -rf smartbox_models/old-service/</code></td></tr>
<tr><td>Memory exhausted</td><td>“MemoryError” or OOM killed</td><td>Increase memory limit in docker-compose</td></tr>
<tr><td>Invalid config</td><td>“JSON decode error”</td><td>Validate config.json syntax</td></tr>
</tbody>
</table>
</div>
<p><strong>Minimum Data Requirements</strong>:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Model Type</th><th>Minimum Samples</th><th>Typical Duration</th></tr>
</thead>
<tbody>
<tr><td>Univariate (per-metric)</td><td>500</td><td>~2 days at 5-min intervals</td></tr>
<tr><td>Multivariate (combined)</td><td>1000</td><td>~4 days at 5-min intervals</td></tr>
<tr><td>Full time-period coverage</td><td>8640</td><td>30 days (captures weekly patterns)</td></tr>
</tbody>
</table>
</div>
<hr>
<h3 id="model-drift-detected"><a class="header" href="#model-drift-detected">Model Drift Detected</a></h3>
<p><strong>Symptom</strong>:</p>
<ul>
<li><code>drift_warning</code> in inference output</li>
<li>Confidence scores reduced</li>
<li>Recommendation to retrain</li>
</ul>
<p><strong>Understanding Drift</strong>:</p>
<p>Model drift occurs when production data differs significantly from training data:</p>
<pre><code>Training Data (30 days ago)          Current Data
─────────────────────────            ────────────
Mean latency: 100ms                  Mean latency: 200ms

Distribution:                        Distribution:
    ●●●●●                               ●●●●●
   ●●●●●●●                            ●●●●●●●●●
  ●●●●●●●●●                            ●●●●●●●●●●●●
 ●●●●●●●●●●●                              ●●●●●●●●●●●
    100ms                                    200ms

Model expects ~100ms                 Seeing ~200ms → DRIFT
</code></pre>
<p><strong>Drift Score Interpretation</strong>:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Score</th><th>Status</th><th>Action</th></tr>
</thead>
<tbody>
<tr><td>&lt; 3</td><td>Normal</td><td>No action needed</td></tr>
<tr><td>3-5</td><td>Moderate drift</td><td>Monitor, consider retraining</td></tr>
<tr><td>&gt; 5</td><td>Severe drift</td><td>Retrain soon</td></tr>
</tbody>
</table>
</div>
<p><strong>Output Example</strong>:</p>
<pre><code class="language-json">{
  "drift_warning": {
    "type": "model_drift",
    "overall_drift_score": 4.2,
    "affected_metrics": ["application_latency", "request_rate"],
    "recommendation": "WARNING: Moderate drift detected. Consider retraining.",
    "confidence_penalty_applied": 0.15
  }
}
</code></pre>
<p><strong>Why Drift Happens</strong>:</p>
<ol>
<li><strong>Seasonal changes</strong>: Holiday traffic patterns vs. normal</li>
<li><strong>Deployments</strong>: New code changed latency characteristics</li>
<li><strong>Infrastructure</strong>: Migrated to faster/slower hardware</li>
<li><strong>Business changes</strong>: New features changed usage patterns</li>
</ol>
<p><strong>Solution: Retrain</strong></p>
<pre><code class="language-bash">docker compose run --rm yaga train
</code></pre>
<p>This replaces old models with ones trained on recent data.</p>
<p><strong>Preventing Drift Issues</strong>:</p>
<ul>
<li>Schedule regular retraining (daily or weekly)</li>
<li>Retrain after significant deployments</li>
<li>Monitor drift scores in dashboards</li>
</ul>
<hr>
<h2 id="container-issues"><a class="header" href="#container-issues">Container Issues</a></h2>
<h3 id="container-keeps-restarting"><a class="header" href="#container-keeps-restarting">Container Keeps Restarting</a></h3>
<p><strong>Symptoms</strong>:</p>
<ul>
<li><code>docker compose ps</code> shows restart count increasing</li>
<li>Container never stays “Up” for long</li>
<li>Application logs show repeated startup/shutdown</li>
</ul>
<p><strong>Diagnostic Steps</strong>:</p>
<p><strong>Step 1: Check exit code</strong></p>
<pre><code class="language-bash">docker compose ps -a

# Look for:
# smartbox-anomaly  Exit 1   (error exit)
# smartbox-anomaly  Exit 137 (OOM killed)
# smartbox-anomaly  Exit 0   (clean exit, shouldn't restart)
</code></pre>
<p><strong>Step 2: Check recent logs</strong></p>
<pre><code class="language-bash">docker compose logs --tail 100

# Look for error messages near the end
</code></pre>
<p><strong>Step 3: Check container events</strong></p>
<pre><code class="language-bash">docker events --filter container=smartbox-anomaly --since 1h
</code></pre>
<p><strong>Common Exit Codes</strong>:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Exit Code</th><th>Meaning</th><th>Solution</th></tr>
</thead>
<tbody>
<tr><td>0</td><td>Clean exit</td><td>Check restart policy, should be <code>unless-stopped</code></td></tr>
<tr><td>1</td><td>Application error</td><td>Check logs for error message</td></tr>
<tr><td>137</td><td>OOM killed</td><td>Increase memory limit</td></tr>
<tr><td>139</td><td>Segmentation fault</td><td>Report bug, check for corruption</td></tr>
</tbody>
</table>
</div>
<p><strong>Solution by Cause</strong>:</p>
<p><strong>Config file missing</strong>:</p>
<pre><code class="language-bash"># Check if config exists
ls -la config.json

# If missing, create from template
cp config.example.json config.json
</code></pre>
<p><strong>Invalid JSON in config</strong>:</p>
<pre><code class="language-bash"># Validate JSON syntax
python -m json.tool config.json

# If error, fix the syntax issue
</code></pre>
<p><strong>OOM killed</strong>:</p>
<pre><code class="language-yaml"># In docker-compose.yml, increase memory:
deploy:
  resources:
    limits:
      memory: 4G
</code></pre>
<hr>
<h3 id="database-locked"><a class="header" href="#database-locked">Database Locked</a></h3>
<p><strong>Symptom</strong>:</p>
<ul>
<li><code>database is locked</code> error in logs</li>
<li>SQLite error messages</li>
<li>Fingerprinting failures</li>
</ul>
<p><strong>Understanding the Issue</strong>:</p>
<p>SQLite doesn’t handle concurrent writes well. This happens when:</p>
<pre><code>Process A: Running inference
                │
                ├─▶ UPDATE anomaly_incidents SET ...
                │
Process B: Also running inference (concurrent)
                │
                └─▶ UPDATE anomaly_incidents SET ...
                         │
                         └─▶ ERROR: database is locked
</code></pre>
<p><strong>How This Can Happen</strong>:</p>
<ol>
<li><strong>Manual inference + scheduled inference</strong>: Running <code>docker compose run --rm yaga inference</code> while the scheduled inference is also running</li>
<li><strong>Multiple containers</strong>: Two containers sharing the same volume</li>
<li><strong>Long-running queries</strong>: One process holds lock while another waits</li>
</ol>
<p><strong>Solutions</strong>:</p>
<p><strong>1. Only run one inference at a time</strong>:</p>
<pre><code class="language-bash"># Check if inference is already running
docker compose ps | grep yaga

# If running, wait for it to finish
</code></pre>
<p><strong>2. Use proper container orchestration</strong>:</p>
<p>Ensure only one inference container runs at a time via scheduling:</p>
<pre><code class="language-yaml"># In docker-compose.yml
command: ["scheduler"]  # Uses cron, prevents overlap
</code></pre>
<p><strong>3. Check for stuck processes</strong>:</p>
<pre><code class="language-bash"># Look for zombie inference processes
docker compose exec yaga ps aux | grep python

# If stuck, restart the container
docker compose restart
</code></pre>
<p><strong>Corrupt Database Recovery</strong>:</p>
<p>If the database becomes corrupted:</p>
<pre><code class="language-bash"># Backup current state
cp data/anomaly_state.db data/anomaly_state.db.backup

# Remove corrupted database
rm data/anomaly_state.db

# Restart - new database will be created
docker compose restart
</code></pre>
<p>Note: This loses incident history. All incidents will start fresh (new incident IDs).</p>
<hr>
<h2 id="debugging"><a class="header" href="#debugging">Debugging</a></h2>
<h3 id="enable-verbose-logging"><a class="header" href="#enable-verbose-logging">Enable Verbose Logging</a></h3>
<p>For detailed diagnostic output:</p>
<p><strong>Option 1: Command-line flag</strong></p>
<pre><code class="language-bash">docker compose run --rm yaga inference --verbose
</code></pre>
<p>This shows:</p>
<ul>
<li>Metrics being collected</li>
<li>ML detection results</li>
<li>Pattern matching output</li>
<li>SLO evaluation details</li>
<li>Fingerprinting actions</li>
</ul>
<p><strong>Option 2: Config file</strong></p>
<pre><code class="language-json">{
  "logging": {
    "level": "DEBUG"
  }
}
</code></pre>
<p>Then rebuild and restart:</p>
<pre><code class="language-bash">docker compose build
docker compose up -d
</code></pre>
<p><strong>Log Level Guide</strong>:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Level</th><th>Use Case</th></tr>
</thead>
<tbody>
<tr><td>INFO</td><td>Normal operation (default)</td></tr>
<tr><td>DEBUG</td><td>Troubleshooting, detailed output</td></tr>
<tr><td>WARNING</td><td>Only problems and warnings</td></tr>
<tr><td>ERROR</td><td>Only errors</td></tr>
</tbody>
</table>
</div>
<hr>
<h3 id="inspect-model-details"><a class="header" href="#inspect-model-details">Inspect Model Details</a></h3>
<p><strong>View model metadata</strong>:</p>
<pre><code class="language-bash"># Check training info
docker compose exec yaga cat /app/smartbox_models/&lt;service&gt;/business_hours/metadata.json | python -m json.tool
</code></pre>
<p>Expected output:</p>
<pre><code class="language-json">{
  "service_name": "booking",
  "period": "business_hours",
  "trained_at": "2024-01-15T02:00:00Z",
  "training_samples": 8640,
  "contamination": 0.02,
  "n_estimators": 250,
  "statistics": {
    "application_latency": {
      "mean": 110.3,
      "std": 45.2,
      "p50": 105.0,
      "p95": 180.0,
      "p99": 250.0
    }
  },
  "calibrated_thresholds": {
    "critical": -0.58,
    "high": -0.32,
    "medium": -0.15,
    "low": -0.08
  }
}
</code></pre>
<p><strong>Check model age</strong>:</p>
<pre><code class="language-bash"># Find oldest and newest models
find smartbox_models -name "metadata.json" -exec sh -c 'echo {} &amp;&amp; grep trained_at {}' \; | sort
</code></pre>
<hr>
<h3 id="check-fingerprint-database"><a class="header" href="#check-fingerprint-database">Check Fingerprint Database</a></h3>
<p><strong>List active incidents</strong>:</p>
<pre><code class="language-bash">docker compose exec yaga sqlite3 /app/data/anomaly_state.db \
  "SELECT fingerprint_id, status, severity, occurrence_count,
          datetime(first_seen), datetime(last_updated)
   FROM anomaly_incidents
   WHERE status != 'CLOSED'
   ORDER BY last_updated DESC;"
</code></pre>
<p><strong>Count incidents by status</strong>:</p>
<pre><code class="language-bash">docker compose exec yaga sqlite3 /app/data/anomaly_state.db \
  "SELECT status, COUNT(*) FROM anomaly_incidents GROUP BY status;"
</code></pre>
<p><strong>Find long-running incidents</strong>:</p>
<pre><code class="language-bash">docker compose exec yaga sqlite3 /app/data/anomaly_state.db \
  "SELECT fingerprint_id, service_name,
          ROUND((julianday('now') - julianday(first_seen)) * 24 * 60) as duration_minutes
   FROM anomaly_incidents
   WHERE status = 'OPEN'
   ORDER BY duration_minutes DESC
   LIMIT 10;"
</code></pre>
<p><strong>Database schema reference</strong>:</p>
<pre><code class="language-sql">-- Main table: anomaly_incidents
fingerprint_id TEXT       -- Pattern identifier (hash)
incident_id TEXT PRIMARY  -- Unique occurrence ID
service_name TEXT         -- Service name
anomaly_name TEXT         -- Anomaly type
status TEXT               -- SUSPECTED, OPEN, RECOVERING, CLOSED
severity TEXT             -- low, medium, high, critical
first_seen TIMESTAMP      -- When first detected
last_updated TIMESTAMP    -- Last detection time
resolved_at TIMESTAMP     -- When closed (NULL if open)
occurrence_count INTEGER  -- Times detected
consecutive_detections INTEGER -- For confirmation
missed_cycles INTEGER     -- For grace period
</code></pre>
<hr>
<h2 id="quick-reference-common-commands"><a class="header" href="#quick-reference-common-commands">Quick Reference: Common Commands</a></h2>
<pre><code class="language-bash"># Health check
docker compose ps
docker compose exec yaga python -c "import smartbox_anomaly; print('OK')"

# View logs
docker compose logs --tail 100
docker compose exec yaga tail -f /app/logs/inference.log

# Manual operations
docker compose run --rm yaga inference --verbose
docker compose run --rm yaga train

# Check models
ls -la smartbox_models/
docker compose exec yaga cat smartbox_models/&lt;service&gt;/business_hours/metadata.json

# Check database
docker compose exec yaga sqlite3 /app/data/anomaly_state.db ".tables"
docker compose exec yaga sqlite3 /app/data/anomaly_state.db "SELECT * FROM anomaly_incidents WHERE status='OPEN';"

# Network connectivity
docker compose exec yaga curl -s http://vm:8428/api/v1/status/buildinfo
docker compose exec yaga curl http://observability-api:8000/health

# Container management
docker compose restart
docker compose build
docker compose up -d
</code></pre>
<hr>
<h2 id="getting-help"><a class="header" href="#getting-help">Getting Help</a></h2>
<p>If you can’t resolve an issue:</p>
<ol>
<li>
<p><strong>Collect diagnostic information</strong>:</p>
<pre><code class="language-bash">docker compose logs --tail 200 &gt; diagnostic.log
docker compose run --rm yaga inference --verbose &gt;&gt; diagnostic.log 2&gt;&amp;1
docker compose exec yaga cat config.json &gt;&gt; diagnostic.log
</code></pre>
</li>
<li>
<p><strong>Check documentation</strong>:</p>
<ul>
<li><a href="slo/README.html">Configuration Guide</a> - SLO and threshold settings</li>
<li><a href="#detection-pipeline">Detection Pipeline</a> - How detection works</li>
<li><a href="incidents/README.html">Incident Lifecycle</a> - State machine details</li>
</ul>
</li>
<li>
<p><strong>File an issue</strong> with:</p>
<ul>
<li>Symptom description</li>
<li>Expected vs. actual behavior</li>
<li>Diagnostic logs</li>
<li>Configuration (sanitized)</li>
<li>Steps to reproduce</li>
</ul>
</li>
</ol>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr-ef4e11c1.min.js"></script>
        <script src="mark-09e88c2c.min.js"></script>
        <script src="searcher-c2a407aa.js"></script>

        <script src="clipboard-1626706a.min.js"></script>
        <script src="highlight-abc7f01d.js"></script>
        <script src="book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
